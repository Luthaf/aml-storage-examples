{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cfd345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ae5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "from torch_builder import TensorBuilder\n",
    "import ase.io\n",
    "from itertools import product\n",
    "from torch_cg import ClebschGordanReal\n",
    "from torch_hamiltonians import fix_pyscf_l1, dense_to_blocks, blocks_to_dense, couple_blocks, decouple_blocks, hamiltonian_features\n",
    "import matplotlib.pyplot as plt\n",
    "from rascal.representations import SphericalExpansion\n",
    "import copy\n",
    "import chemiscope\n",
    "from ase.units import Hartree\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e301ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0,'/Users//new-hamiltonian/equistore-examples')\n",
    "from utils.librascal import  RascalSphericalExpansion, RascalPairExpansion\n",
    "from utils.acdc_mini import acdc_standardize_keys, cg_increment, cg_combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db09b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = ase.io.read(\"../data/hamiltonian/water-hamiltonian/water_coords_1000.xyz\",\":100\")\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cd929d-ea1c-4410-9fe6-7fb633c1ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e65738-0c11-4c93-932f-0daad032263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "but = ipywidgets.Button(description=\"ciao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca187cb0-698f-4967-ae8e-47fa609ce33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030f2c21e0f94a9988e97f6804cc1a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='ciao', style=ButtonStyle())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab04f50-5371-4071-82d8-c5ae41599daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michele/local/lib/python3.8/site-packages/chemiscope/structures.py:305: UserWarning: value '[ 8.210879  0.280605  0.        0.280605  9.429281 -0.        0.\n",
      " -0.        8.821933]' of type '<class 'numpy.ndarray'>' for the 'alpha' property from ASE is not convertible to float or string, this property will be ignored.\n",
      "  warnings.warn(\n",
      "/home/michele/local/lib/python3.8/site-packages/chemiscope/structures.py:305: UserWarning: value '[0.598465 0.56854  0.      ]' of type '<class 'numpy.ndarray'>' for the 'mu' property from ASE is not convertible to float or string, this property will be ignored.\n",
      "  warnings.warn(\n",
      "/home/michele/local/lib/python3.8/site-packages/chemiscope/structures.py:305: UserWarning: value '[ -8.941737  -1.676889  -0.        -1.676889  -2.743205   0.\n",
      "  -0.         0.        -5.531125  -1.676889  -2.743205   0.\n",
      "  -2.743205 -13.352259   0.         0.         0.        -3.4944\n",
      "  -0.         0.        -5.531125   0.         0.        -3.4944\n",
      "  -5.531125  -3.4944     0.      ]' of type '<class 'numpy.ndarray'>' for the 'beta' property from ASE is not convertible to float or string, this property will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6de5fa056d4ed1839513cebcdee9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StructureWidget(value='{\"meta\": {\"name\": \" \"}, \"structures\": [{\"size\": 3, \"names\": [\"O\", \"H\", \"H\"], \"x\": [50.0â€¦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemiscope.show(frames, mode=\"structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "292e1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jorbs = json.load(open('data/water-hamiltonian/water_orbs.json', \"r\"))\n",
    "jorbs = json.loads(json.load(open('../data/hamiltonian/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab6a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "hams = np.load(\"../data/hamiltonian/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames)]\n",
    "# NO NEED TO CORRECT L1 ORDER FOR SAPH ORTHOGONALIZED MATRICES...\n",
    "#for i, f in enumerate(frames):\n",
    "#    hams[i] = fix_pyscf_l1(hams[i], f, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "754797bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17e9fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss functions\n",
    "def mse_full(fock, pred_blocks, frame, orbs):\n",
    "    predicted = blocks_to_dense(decouple_blocks(pred_blocks), frame, orbs)\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = (torch.linalg.norm(fock[i]-predicted[i]))**2/len(fock[i])\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2\n",
    "\n",
    "def mse_eigvals(fock, pred_blocks, frame, orbs):\n",
    "    predicted = blocks_to_dense(decouple_blocks(pred_blocks), frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196167ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feats(feat, all_blocks=True): \n",
    "    all_norm = 0\n",
    "    for block_idx, block in feat: \n",
    "        block_norm = np.linalg.norm(block.values)\n",
    "#         print(block_idx, block_norm)\n",
    "        all_norm += block_norm**2\n",
    "    normalized_blocks=[]\n",
    "    for block_idx, block in feat: \n",
    "        newblock = TensorBlock(\n",
    "                        values=block.values/np.sqrt(all_norm ),\n",
    "                        samples=block.samples,\n",
    "                        components=block.components,\n",
    "                        properties= block.properties)\n",
    "                    \n",
    "        normalized_blocks.append(newblock) \n",
    "        \n",
    "    norm_feat = TensorMap(feat.keys, normalized_blocks)\n",
    "    return norm_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad712",
   "metadata": {},
   "source": [
    "## Manipulate Hamiltonian into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "461e38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = dense_to_blocks(hams, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8cfe3",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "13064b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 2.5,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 8,\n",
    "    \"max_angular\": 4,\n",
    "    \"gaussian_sigma_constant\" : 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\":  False,\n",
    "}\n",
    "\n",
    "spex = RascalSphericalExpansion(rascal_hypers)\n",
    "rhoi = spex.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cc39e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = RascalPairExpansion(rascal_hypers)\n",
    "gij = pairs.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "abf74c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho1i = acdc_standardize_keys(rhoi)\n",
    "rho1i.keys_to_properties(['species_neighbor'])\n",
    "gij =  acdc_standardize_keys(gij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6a666711",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho2i = cg_increment(rho1i, rho1i, lcut=3, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4565f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rho3i = cg_increment(rho2i, rho1i, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bdcb6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho1ij = cg_increment(rho1i, gij, lcut=3, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0c5d9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rho2ij = cg_increment(rho2i, gij, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b29eb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_feats = hamiltonian_features(rho2i, rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6739d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ham_feats = normalize_feats(ham_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8c8bc",
   "metadata": {},
   "source": [
    "# Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cc7c1c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, coupled_blocks, features, weights=None, intercepts=None):\n",
    "        super().__init__()\n",
    "        self.coupled_blocks = coupled_blocks\n",
    "        self.features = features\n",
    "        self.weights = {}\n",
    "        if weights==None:\n",
    "            for idx_fock, block_fock in self.coupled_blocks:\n",
    "                block_type, ai, ni, li, aj, nj, lj, L = idx_fock\n",
    "                parity= (-1)**(li+lj+L)\n",
    "                size = self.features.block(block_type=block_type, spherical_harmonics_l=L,inversion_sigma=parity, \n",
    "                                       species_center=ai, species_neighbor=aj).values.shape[2]\n",
    "                #self.weights[idx_fock] = torch.nn.Parameter(torch.zeros(size, dtype=torch.float64))\n",
    "                self.weights[idx_fock] = torch.nn.Parameter(torch.randn(size, dtype=torch.float64))\n",
    "            \n",
    "        else: \n",
    "            self.weights = weights\n",
    "        \n",
    "        self.intercepts = {}\n",
    "        if intercepts is None:\n",
    "            for idx_fock, block_fock in self.coupled_blocks:\n",
    "                block_type, ai, ni, li, aj, nj, lj, L = idx_fock\n",
    "                parity= (-1)**(li+lj+L)\n",
    "                if L==0 and parity==1 and block_type==0:\n",
    "                    self.intercepts[idx_fock] = torch.nn.Parameter(torch.randn(1, dtype=torch.float64))\n",
    "                else:\n",
    "                    self.intercepts[idx_fock] = 0\n",
    "        else:\n",
    "            self.intercepts = intercepts\n",
    "         \n",
    "    def forward(self, features):\n",
    "        k = []\n",
    "        pred_blocks = []\n",
    "        for (idx, wts) in self.weights.items():\n",
    "            #print(wts)\n",
    "            block_type, ai, ni, li, aj, nj, lj, L = idx\n",
    "            k.append(list(idx))\n",
    "            parity= (-1)**(li+lj+L)\n",
    "            X = features.block(block_type=block_type, spherical_harmonics_l=L,inversion_sigma=parity, \n",
    "                                   species_center=ai, species_neighbor=aj)\n",
    "            X_new = torch.from_numpy(X.values.reshape(-1, X.values.shape[2]))\n",
    "            #print(idx, wts.shape, X.values.shape, X_new.shape)\n",
    "            Y = X_new @ wts + self.intercepts[idx]\n",
    "            \n",
    "            newblock = TensorBlock(\n",
    "                        values=Y.reshape((-1, 2 * L + 1, 1)),\n",
    "                        samples=X.samples,\n",
    "                        components=[Labels(\n",
    "                            [\"mu\"], np.asarray(range(-L, L + 1), dtype=np.int32).reshape(-1, 1)\n",
    "                        )],\n",
    "                        properties= Labels([\"values\"], np.asarray([[0]], dtype=np.int32))\n",
    "                    )\n",
    "            pred_blocks.append(newblock) \n",
    "        \n",
    "        keys = Labels(('block_type', 'a_i', 'n_i', 'l_i', 'a_j', 'n_j', 'l_j', 'L'), np.asarray(k, dtype=np.int32))\n",
    "        pred_fock = TensorMap(keys, pred_blocks)\n",
    "        return(pred_fock)\n",
    "        ### add direct eigenvalue prediction here as well\n",
    "    \n",
    "    def parameters(self):\n",
    "        for idx, wts in self.weights.items():\n",
    "            yield wts\n",
    "        for idx, wts in self.intercepts.items():\n",
    "            if type(wts) is not int:\n",
    "                yield wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3bf5cc",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# For single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5dbeb35",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i_frame = frames[0]\n",
    "i_ham = hams[0] \n",
    "\n",
    "i_feats = ham_feats.slice(Labels(names=[\"structure\"], values=np.asarray(range(1), dtype=np.int32).reshape(-1,1)) )\n",
    "i_focks = fock_bc.slice(Labels(names=[\"structure\"], values=np.asarray(range(1), dtype=np.int32).reshape(-1,1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "044d1f5c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "norm_i_feats = normalize_feats(i_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf945f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Testing the model with weights from equistore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "133da935",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights = np.load('model_weights.npy', allow_pickle=True)[()]\n",
    "intercepts = np.load('model_intercepts.npy', allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6e2e675",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model contains 8 parameters\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 256 is different from 5184)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-697224629414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"the model contains {len(list(model.parameters()))} parameters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7c4f7f2600b7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#print(idx, wts.shape, X.values.shape, X_new.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mwts\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercepts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_new\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mwts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 256 is different from 5184)"
     ]
    }
   ],
   "source": [
    "model = LinearModel(i_focks, i_feats, weights=weights, intercepts=intercepts)\n",
    "print(f\"the model contains {len(list(model.parameters()))} parameters\")\n",
    "\n",
    "pred = model(i_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117810f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss = mse_full([torch.from_numpy(i_ham.astype(np.float64))], pred, [i_frame], orbs)\n",
    "print(torch.sqrt(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7f25f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training with loss on full Hamiltonian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456cbe7",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LinearModel(i_focks, norm_i_feats)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15000, gamma=0.01)\n",
    "\n",
    "all_losses = []\n",
    "for epoch in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(norm_i_feats)\n",
    "    loss = mse_full([torch.from_numpy(i_ham.astype(np.float64))], pred, [i_frame], orbs)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "#     scheduler.step()\n",
    "    \n",
    "    all_losses.append(loss.item())\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bba86",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.loglog(all_losses)\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"$MSE_{full}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf654b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred = model(norm_i_feats)\n",
    "mse_full([torch.from_numpy(i_ham.astype(np.float64))], pred, [i_frame], orbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebedc2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training with loss on eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac120e84",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = LinearModel(i_focks, i_feats)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.01)\n",
    "\n",
    "eigval_losses = []\n",
    "for epoch in range(30000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(i_feats[0])\n",
    "    eigval_loss = mse_eigvals([torch.from_numpy(i_ham.astype(np.float64))], pred, [i_frame], orbs)\n",
    "    eigval_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    eigval_losses.append(eigval_loss.item())\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(epoch, eigval_loss.item())\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfda8d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.loglog(eigval_losses)\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"$MSE_{\\epsilon}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d4008",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_coupled = model(ham_feats)\n",
    "rmse_full = torch.sqrt(mse_full([torch.from_numpy(i_ham.astype(np.float64))], pred, [i_frame], orbs))\n",
    "print(rmse_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef93952",
   "metadata": {},
   "source": [
    "# Train and test on multiple frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36370cdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "77c7f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(frames)\n",
    "ntrain = int(N*0.8)\n",
    "train_frames = frames[:ntrain]\n",
    "train_hams = hams[:ntrain]\n",
    "train_feats = norm_ham_feats.slice(Labels(names=[\"structure\"], values=np.asarray(range(ntrain), dtype=np.int32).reshape(-1,1)) )\n",
    "train_focks = fock_bc.slice(Labels(names=[\"structure\"], values=np.asarray(range(ntrain), dtype=np.int32).reshape(-1,1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0c58935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frames = frames[ntrain:N]\n",
    "test_hams = hams[ntrain:N]\n",
    "test_feats = norm_ham_feats.slice(Labels(names=[\"structure\"], values=np.asarray(range(ntrain,N), dtype=np.int32).reshape(-1,1)) )\n",
    "test_focks = fock_bc.slice(Labels(names=[\"structure\"], values=np.asarray(range(ntrain,N), dtype=np.int32).reshape(-1,1)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351030a-9807-4792-a95e-1222cc09e90f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train on Hamiltonian target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3f58021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 326.60009765625\n",
      "50 0.5482138991355896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6492e5b655fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_hams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LinearModel(train_focks, train_feats)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.8)\n",
    "\n",
    "all_losses = []\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_feats)\n",
    "    loss = mse_full(torch.from_numpy(train_hams.astype(np.float64)), pred, train_frames, orbs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    all_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(epoch, loss.item()) \n",
    "#     if loss.item <1e-15: \n",
    "#         break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed648e-d95d-4e6b-a228-4d355c775a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(train_focks, train_feats)\n",
    "optimizer = torch.optim.LBFGS(\n",
    "        model.parameters(),\n",
    "        lr=0.8,  line_search_fn=\"strong_wolfe\",\n",
    "        history_size=256,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f06d8b7e-0004-4938-a879-dca0d01355ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3f6232ef734a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "for epoch in range(300):\n",
    "    def single_step():\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(train_feats)\n",
    "        loss = mse_full(torch.from_numpy(train_hams.astype(np.float64)), pred, train_frames, orbs)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    loss = optimizer.step(single_step)\n",
    "    \n",
    "    all_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(epoch, loss.item()) \n",
    "#     if loss.item <1e-15: \n",
    "#         break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(np.sqrt(all_losses))\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"$RMSE_{full}$\")\n",
    "print(np.sqrt(all_losses[-1]), \"eV rmse on TRAIN H prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cf306",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = model(norm_ham_feats.slice(Labels(names=[\"structure\"], values=np.asarray(range(N), dtype=np.int32).reshape(-1,1)) ))\n",
    "all_loss = mse_full(torch.from_numpy(hams[:N].astype(np.float64)), pred_all, frames[:N], orbs)\n",
    "print(torch.sqrt(all_loss), \"eV rmse on ALL H prediction\")\n",
    "# test_loss_eigvals = mse_eigvals(torch.from_numpy(test_hams.astype(np.float64)), pred_test, test_frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TEMPORARY FIX to reindex just the test frames \n",
    "pred_test = model(test_feats)\n",
    "bvalues = []\n",
    "for i, b in pred_test:\n",
    "    newblock = TensorBlock(\n",
    "                        values=b.values,\n",
    "                        samples=pred.block(i).samples[:len(b.values)],\n",
    "                        components=b.components,\n",
    "                        properties= b.properties)\n",
    "                    \n",
    "    bvalues.append(newblock) \n",
    "        \n",
    "reindexed_pred_test  = TensorMap(test_focks.keys, bvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340609fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindexed_pred_test.block(0).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = mse_full(torch.from_numpy(test_hams.astype(np.float64)), reindexed_pred_test, test_frames, orbs)\n",
    "test_loss_eigvals = mse_eigvals(torch.from_numpy(test_hams.astype(np.float64)), reindexed_pred_test, test_frames, orbs)\n",
    "print(torch.sqrt(test_loss),  \"eV rmse on TEST H prediction\")\n",
    "print(torch.sqrt(test_loss_eigvals),  \"eV rmse on TEST eigen prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d86a47",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training on eigenvalue loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b848afac-b445-4aab-9749-196bc10af61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(train_focks, train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7ad2b9cb-37c2-4836-b106-06c2ad9bc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.5)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.1)\n",
    "optimizer = torch.optim.LBFGS(\n",
    "        model.parameters(),\n",
    "        lr=0.1,  line_search_fn=\"strong_wolfe\",\n",
    "        history_size=256, tolerance_grad=1e-12, tolerance_change=1e-12\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7971f15-729c-49e1-a192-8e40f8632272",
   "metadata": {},
   "source": [
    "manual gradient check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1967bc0a-4409-4dd1-a47b-4889015aac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_loss = 0\n",
    "full_loss = 0\n",
    "def single_step():\n",
    "    global eva_loss, full_loss\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_feats)\n",
    "    eva_loss = mse_eigvals(torch.from_numpy(train_hams.astype(np.float64)), pred, train_frames, orbs)\n",
    "    full_loss = mse_full(torch.from_numpy(train_hams.astype(np.float64)), pred, train_frames, orbs)\n",
    "    loss_combined = 1e0*eva_loss + 1e-4*full_loss\n",
    "    loss_combined.backward()\n",
    "    return loss_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "82631bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.856097496969554e-06 0.0023511937761550045 0.01510000256027047\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 4.9708047879814025e-06\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 1.1284339893058139e-05\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 4.509364647334141e-06\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 3.468839575788039e-06\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 1.4027248645832418e-06\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 9.147812118975273e-06\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 1.0373503071430188e-05\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 5.633418086214473e-07\n",
      "1 5.550913180762047e-06 0.002239991010547779 0.02730472513799641\n",
      "2 5.092114528821015e-06 0.0021852577389186237 0.03241019362751727\n",
      "3 4.880393450601051e-06 0.0021580257133496524 0.03400242230656714\n",
      "4 4.77269145174969e-06 0.002100450855553198 0.037777729854792885\n",
      "5 4.554609483892332e-06 0.0020642800603616223 0.04206037543065555\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 1.4579176818002023e-06\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 9.826346988086403e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 7.467255843833045e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 1.0390476557712937e-06\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.0707650132428543e-06\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.0681749481815732e-06\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 1.9356246404836933e-06\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 5.851197299657806e-07\n",
      "6 4.438159685743351e-06 0.002043420322980944 0.04382008838649629\n",
      "7 4.3675866309915794e-06 0.0020360702915529897 0.04439983206929134\n",
      "8 4.342716740922804e-06 0.00202532461019666 0.04414777223374289\n",
      "9 4.2968423559884964e-06 0.0020150389122776196 0.044018610161198825\n",
      "10 4.2541456220453325e-06 0.0020071969752459163 0.043538216772492144\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 1.5952050352323824e-06\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 1.715312756160906e-06\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 4.796638501285896e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 8.283339777636211e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 4.947230292268102e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 6.659553448494768e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.4480901249657252e-06\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 5.41442412457589e-07\n",
      "11 4.218397329409206e-06 0.001992031820585492 0.044726801704886215\n",
      "12 4.168239453299972e-06 0.001983035353864056 0.04583485537570546\n",
      "13 4.142512611405924e-06 0.001976553235046714 0.04689230714487997\n",
      "14 4.1266515379106075e-06 0.001969137319560997 0.04677673329965641\n",
      "15 4.096308061106587e-06 0.0019609595980271187 0.04808610203182423\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 1.3830392437106387e-06\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.5270693650908606e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 3.3436007082836696e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 9.310750316329369e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 5.594328268698239e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 5.297197164528542e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 3.3868095597527986e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 3.056780552153481e-07\n",
      "16 4.076589865956179e-06 0.0019604423013961335 0.04808591204711713\n",
      "17 4.074559510843676e-06 0.0019577800855907664 0.04628841516719948\n",
      "18 4.047164601404892e-06 0.0019417043442305749 0.050254331717193915\n",
      "19 4.022765546038063e-06 0.001932185618799396 0.052048436988449936\n",
      "20 4.004245244789269e-06 0.0019321933739085155 0.05203478648379786\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.3838818901849216e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.395110803989839e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.627188510975054e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 8.386363390603355e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.4747249895736067e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.611123743497874e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.80842455045248e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3822372514925595e-07\n",
      "21 4.0041331346174155e-06 0.001932197336107926 0.05202289221325633\n",
      "22 4.0040246770857745e-06 0.001932193328106838 0.05201404176853005\n",
      "23 4.003917111290418e-06 0.0019321849684131717 0.05200685731979654\n",
      "24 4.003810072989977e-06 0.0019321745196702286 0.052000491841156835\n",
      "25 4.0037034896351e-06 0.0019321630965710168 0.05199452970137459\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.439112161909789e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.429714076339357e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.6818792219039225e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 8.136169871301958e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3944276711675784e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.5279455731102217e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.8389708315876293e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.39058922414389e-07\n",
      "26 4.003597343637613e-06 0.0019321512297012127 0.051988773281696744\n",
      "27 4.003491629169475e-06 0.0019321391722006522 0.05198312825667015\n",
      "28 4.003386343087164e-06 0.0019321270457600282 0.05197754918577897\n",
      "29 4.0032814828933816e-06 0.001932114909224406 0.051972013987659445\n",
      "30 4.003177046240581e-06 0.0019321027911096003 0.05196651185271445\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.438609609554094e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.440982517501194e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.674156605069668e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 8.07809778444512e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3822058669084505e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.5247296405386133e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.838001484593554e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3886788218780527e-07\n",
      "31 4.003073030827339e-06 0.0019320907052090008 0.051961037444464134\n",
      "32 4.002969434385515e-06 0.0019320786581312145 0.05195558809595879\n",
      "33 4.002866254645808e-06 0.0019320666529681128 0.051950162449444504\n",
      "34 4.002763489363774e-06 0.0019320546910899735 0.05194475978812058\n",
      "35 4.0026611363073276e-06 0.0019320427730200207 0.05193937971020436\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.4364721885568455e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.450815071264982e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.6655550285667267e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 8.024127305308844e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3719086139267904e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.5236547177887994e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.837284575244919e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3866926622300497e-07\n",
      "36 4.00255919324697e-06 0.0019320308988657754 0.05193402197106653\n",
      "37 4.002457657981218e-06 0.0019320190685368187 0.05192868639996115\n",
      "38 4.002356528312428e-06 0.0019320072818351348 0.051923372867281485\n",
      "39 4.0022558020554605e-06 0.0019319955385223148 0.05191808125973969\n",
      "40 4.002155477039423e-06 0.001931983838333831 0.051912811474093726\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.4343229150545373e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.46065891527362e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.6570599312083146e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.971057924001706e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3619627381018798e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.523062859829997e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.836640180340872e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3847350414529013e-07\n",
      "41 4.002055551097602e-06 0.0019319721809912616 0.05190756341390325\n",
      "42 4.001956022080971e-06 0.001931960566218954 0.05190233698323321\n",
      "43 4.001856887857171e-06 0.0019319489937347748 0.0518971320891608\n",
      "44 4.001758146300789e-06 0.001931937463262824 0.05189194863730405\n",
      "45 4.001659795296055e-06 0.0019319259745201086 0.051886786536295706\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.4322070410301317e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.470555776066916e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.648691870487824e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.918802815509529e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3523140052063178e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.52288606208563e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.8360574675427983e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.382808070899767e-07\n",
      "46 4.0015618327317835e-06 0.0019319145272289418 0.05188164569518348\n",
      "47 4.001464256522281e-06 0.0019319031211136022 0.05187652602338072\n",
      "48 4.001367064593927e-06 0.0019318917559040612 0.05187142742868724\n",
      "49 4.001270254879034e-06 0.001931880431322124 0.051866349822413095\n",
      "50 4.001173825315449e-06 0.0019318691471001596 0.051861293113901016\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.430124765023179e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.480503019942883e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.6404493439794755e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.867347867481518e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3429535372984785e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.52310318021958e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.8355331584050066e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3809112421044773e-07\n",
      "51 4.0010777738620935e-06 0.0019318579029673248 0.051856257214385144\n",
      "52 4.000982098485757e-06 0.0019318466986561747 0.051851242034756224\n",
      "53 4.000886797163447e-06 0.0019318355339012362 0.0518462474865074\n",
      "54 4.000791867886691e-06 0.0019318244084369095 0.05184127348191018\n",
      "55 4.0006973086552355e-06 0.001931813322001639 0.05183631993370672\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.4280749718192326e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.490496847631248e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.6323303664035507e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.816681063090067e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3338739362806492e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.523694999417448e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.83506433029272e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3790439866632178e-07\n",
      "56 4.000603117489968e-06 0.001931802274336942 0.05183138675287931\n",
      "57 4.000509292406037e-06 0.001931791265181007 0.05182647385307774\n",
      "58 4.0004158314340106e-06 0.0019317802942735065 0.051821581148978\n",
      "59 4.000322732621447e-06 0.0019317693613615133 0.05181670855361896\n",
      "60 4.000229994028137e-06 0.001931758466189224 0.05181185598189964\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.426056590153776e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.500533507517248e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.624332887499949e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.766790642554885e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3250679315804864e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.524642958765336e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.834648158197526e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.377205743318953e-07\n",
      "61 4.000137613722654e-06 0.0019317476085054695 0.05180702334792564\n",
      "62 4.000045589783852e-06 0.0019317367880601918 0.051802210565277264\n",
      "63 3.999953920290039e-06 0.0019317260045985875 0.05179741755103176\n",
      "64 3.999862603338016e-06 0.0019317152578741397 0.05179264422052678\n",
      "65 3.999771637039161e-06 0.0019317045476402088 0.05178789049026022\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.424068573619144e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.5106093811195794e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.616454910333158e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.717664992251205e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3165283876971336e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.52592907520274e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.834281922082102e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3753959634405352e-07\n",
      "66 3.999681019516982e-06 0.0019316938736529286 0.051783156276414125\n",
      "67 3.999590748903009e-06 0.001931683235667668 0.05177844149575913\n",
      "68 3.9995008233324855e-06 0.001931672633442538 0.051773746065440156\n",
      "69 3.999411240955697e-06 0.0019316620667360668 0.05176906990330573\n",
      "70 3.999321999932388e-06 0.0019316515353087489 0.05176441292754222\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.422109905924517e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.520720977790042e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.6086944735342533e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.6692926442158e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3082482965460278e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.527535939729898e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.83396299885351e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.373614110068949e-07\n",
      "71 3.999233098433957e-06 0.001931641038923555 0.05175977505628574\n",
      "72 3.9991445346414e-06 0.0019316305773445866 0.05175515620832178\n",
      "73 3.99905630674736e-06 0.0019316201503367914 0.05175055630292541\n",
      "74 3.998968412953354e-06 0.0019316097576679606 0.051745975259161706\n",
      "75 3.998880851470255e-06 0.0019315993991032999 0.051741412998010966\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.420179589795565e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.5308649405307845e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.6010496787606624e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.621662267882516e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.3002207873337188e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.529446705925999e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.833688865391014e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3718596588527518e-07\n",
      "76 3.998793620519303e-06 0.0019315890744160476 0.05173686943841535\n",
      "77 3.9987067183322075e-06 0.0019315787833766548 0.05173234450094491\n",
      "78 3.998620143147282e-06 0.0019315685257557992 0.051727838106832444\n",
      "79 3.998533893211098e-06 0.00193155830132902 0.051723350176841806\n",
      "80 3.9984479667846696e-06 0.001931548109869457 0.05171888063316153\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.418276721664443e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.541037969702666e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.593518552277781e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.57476273343749e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.2924390729449402e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.5316451257959836e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.8334570772842986e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3701320924111219e-07\n",
      "81 3.998362362134993e-06 0.0019315379511546996 0.051714429397934504\n",
      "82 3.99827707754629e-06 0.0019315278249629737 0.05170999639400681\n",
      "83 3.998192111313016e-06 0.0019315177310776249 0.05170558154175367\n",
      "84 3.99810746172435e-06 0.0019315076692739365 0.05170118476625909\n",
      "85 3.99802312708752e-06 0.001931497639336337 0.05169680599071427\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.4164003311424786e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.5512369726101123e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.586099321891927e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.528582976496616e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.284896561291512e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.534115450863061e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.833265300215498e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3684309109315722e-07\n",
      "86 3.997939105725997e-06 0.0019314876410497998 0.05169244513781243\n",
      "87 3.997855395960695e-06 0.0019314776741989068 0.05168810213220033\n",
      "88 3.9977719961316955e-06 0.0019314677385678582 0.05168377689855929\n",
      "89 3.997688904578442e-06 0.0019314578339458784 0.05167946936134089\n",
      "90 3.9976061196578815e-06 0.0019314479601225086 0.05167517944562566\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.4145495727161135e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.5614588752472667e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.578790085683512e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.483112157115883e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.277586722565934e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.5368425328865554e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.833111267920368e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.3667556190181792e-07\n",
      "91 3.997523639735161e-06 0.0019314381168873556 0.051670907077261956\n",
      "92 3.997441463184078e-06 0.001931428304031876 0.051666652181986975\n",
      "93 3.997359588384891e-06 0.0019314185213522618 0.05166241468518218\n",
      "94 3.99727801373293e-06 0.0019314087686411912 0.05165819451275293\n",
      "95 3.997196737615823e-06 0.0019313990456922851 0.05165399159260959\n",
      "gradients\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) 2.412723557902867e-07\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) 3.5717007788243197e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) 2.5715890999229674e-07\n",
      "(0, 8, 2, 1, 8, 2, 1, 2) 7.438339528579363e-07\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) 2.27050320447996e-07\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) 3.5398117195937065e-07\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) 2.8329928131759155e-07\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 2.365105737086279e-07\n",
      "96 3.997115758446008e-06 0.0019313893523023316 0.051649805851556464\n",
      "97 3.997035074637167e-06 0.0019313796882712356 0.05164563721682505\n",
      "98 3.996954684619885e-06 0.0019313700533984623 0.051641485616032864\n",
      "99 3.996874586827472e-06 0.0019313604474850993 0.05163735097684626\n"
     ]
    }
   ],
   "source": [
    "all_eigval_losses = []\n",
    "all_losses = []\n",
    "combined_loss = []\n",
    "for epoch in range(100):  \n",
    "    loss_combined = optimizer.step(single_step)\n",
    "    #scheduler.step()\n",
    "    \n",
    "    all_eigval_losses.append(eva_loss.item())\n",
    "    all_losses.append(full_loss.item())\n",
    "    combined_loss.append(loss_combined.item())\n",
    "    \n",
    "    if epoch % 1== 0:\n",
    "        print(epoch, loss_combined.item(), np.sqrt(eva_loss.item()), np.sqrt(full_loss.item()))\n",
    "        if epoch % 5== 0:\n",
    "            print(\"gradients\") \n",
    "            for k, w in model.weights.items():  \n",
    "                print(k, np.linalg.norm(w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c3f90eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '$RMSE_{\\\\epsilon}$')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(np.sqrt(all_eigval_losses))\n",
    "plt.loglog(np.sqrt(all_losses))\n",
    "plt.loglog(np.sqrt(combined_loss), 'g.')\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"$RMSE_{\\epsilon}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2b447789",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model(test_feats)\n",
    "bvalues = []\n",
    "for i, b in pred_test:\n",
    "    newblock = TensorBlock(\n",
    "                        values=b.values,\n",
    "                        samples=pred.block(i).samples[:len(b.values)],\n",
    "                        components=b.components,\n",
    "                        properties= b.properties)\n",
    "                    \n",
    "    bvalues.append(newblock) \n",
    "        \n",
    "reindexed_pred_test  = TensorMap(test_focks.keys, bvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "921b4e03-8551-462d-8b11-bb137720ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = blocks_to_dense(decouple_blocks(reindexed_pred_test), test_frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "167ef78c-b384-4e73-84b5-1d25921c8610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3969, -0.6683, -0.6192, -0.5151,  0.1617,  0.2686])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.eigvalsh(torch.from_numpy(test_hams[13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c051b84d-7300-4f06-925d-3eaa0024814f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5c78601cd0>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(dense[13].detach().numpy() - test_hams[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "00cc7fd9-c4aa-42f2-aee9-a28a1b15e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3969, -0.6683, -0.6192, -0.5151,  0.1617,  0.2686])\n",
      "tensor([-1.3904, -0.6671, -0.6154, -0.5051,  0.1875,  0.2485],\n",
      "       grad_fn=<LinalgEighBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.linalg.eigvalsh(torch.from_numpy(test_hams[13])))\n",
    "print(torch.linalg.eigvalsh(dense[13]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f37ccee3-f17c-4657-8777-b8663a94cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0143, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0097, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0157, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0226, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0081, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0518, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0046, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3062, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0147, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0062, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0014, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0020, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(torch.mean(torch.abs((torch.linalg.eigvalsh(dense[i]) - torch.linalg.eigvalsh(torch.from_numpy(test_hams[i]))) * Hartree )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9ec19b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1193, grad_fn=<SqrtBackward0>) eV rmse on TEST H prediction\n",
      "tensor(0.0363, grad_fn=<SqrtBackward0>) eV rmse on TEST eigen prediction\n"
     ]
    }
   ],
   "source": [
    "test_loss = mse_full(torch.from_numpy(test_hams.astype(np.float64)), reindexed_pred_test, test_frames, orbs)\n",
    "test_loss_eigvals = mse_eigvals(torch.from_numpy(test_hams.astype(np.float64)), reindexed_pred_test, test_frames, orbs)\n",
    "print(torch.sqrt(test_loss),  \"eV rmse on TEST H prediction\")\n",
    "print(torch.sqrt(test_loss_eigvals),  \"eV rmse on TEST eigen prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c21a5e97-792d-4627-8960-c263348a8b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ba6abbf88e4179be60390e8538002e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StructureWidget(value='{\"meta\": {\"name\": \" \"}, \"structures\": [{\"size\": 3, \"names\": [\"O\", \"H\", \"H\"], \"x\": [50.0â€¦"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemiscope.show(test_frames, mode=\"structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f922057-0171-44d8-aa4b-9261347e17dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f890709b27744342a0cd8da3a0a56b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StructureWidget(value='{\"meta\": {\"name\": \" \"}, \"structures\": [{\"size\": 3, \"names\": [\"O\", \"H\", \"H\"], \"x\": [50.0â€¦"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemiscope.show(train_frames, mode=\"structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4429de6",
   "metadata": {},
   "source": [
    "## Train first on MSE_full and then retrain on MSE_eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ade88c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LinearModel(train_focks, train_feats)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.8)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.1)\n",
    "\n",
    "all_losses = []\n",
    "for epoch in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(train_feats)\n",
    "    loss = mse_full(torch.from_numpy(train_hams.astype(np.float64)), pred, train_frames, orbs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step()\n",
    "    \n",
    "    all_losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc02e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eigval_losses = []\n",
    "all_ham_losses = []\n",
    "combined_loss = []\n",
    "\n",
    "for epoch in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    re_pred = model(train_feats)\n",
    "    loss1 = mse_full(torch.from_numpy(train_hams.astype(np.float64)), re_pred, train_frames, orbs)\n",
    "    loss2 = mse_eigvals(torch.from_numpy(train_hams.astype(np.float64)), re_pred, train_frames, orbs)\n",
    "    loss_combined = loss1 + loss2\n",
    "    loss_combined.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step()\n",
    "    \n",
    "    all_eigval_losses.append(loss2.item())\n",
    "    all_ham_losses.append(loss1.item())\n",
    "    combined_loss.append(loss_combined.item())\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(\"Epoch:\", epoch, \"combined:\", loss_combined.item(), \"MSE_full:\", loss1.item(), \"MSE_eva:\", loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff11c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model(test_feats)\n",
    "bvalues = []\n",
    "for i, b in pred_test:\n",
    "    newblock = TensorBlock(\n",
    "                        values=b.values,\n",
    "                        samples=pred.block(i).samples[:len(b.values)],\n",
    "                        components=b.components,\n",
    "                        properties= b.properties)\n",
    "                    \n",
    "    bvalues.append(newblock) \n",
    "        \n",
    "reindexed_pred_test  = TensorMap(test_focks.keys, bvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = mse_full(torch.from_numpy(test_hams.astype(np.float64)), reindexed_pred_test, test_frames, orbs)\n",
    "test_loss_eigvals = mse_eigvals(torch.from_numpy(test_hams.astype(np.float64)), reindexed_pred_test, test_frames, orbs)\n",
    "print(torch.sqrt(test_loss),  \"eV rmse on TEST H prediction\")\n",
    "print(torch.sqrt(test_loss_eigvals),  \"eV rmse on TEST eigen prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "507b0143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec16b199bbde4615b77fa881ba432ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StructureWidget(value='{\"meta\": {\"name\": \" \"}, \"structures\": [{\"size\": 3, \"names\": [\"O\", \"H\", \"H\"], \"x\": [50.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chemiscope.show(test_frames, mode=\"structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b115819-d0fa-4f56-be21-97439c82acf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "9b1dab49005db54eeaf19407b770a132388229704ecb91b436b854691f0d31ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
