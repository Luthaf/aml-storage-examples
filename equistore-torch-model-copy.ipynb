{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5c7c0",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Manipulate-Hamiltonian-into-blocks\" data-toc-modified-id=\"Manipulate-Hamiltonian-into-blocks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Manipulate Hamiltonian into blocks</a></span></li><li><span><a href=\"#Feature-computation\" data-toc-modified-id=\"Feature-computation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature computation</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Restructuring-blocks-of-the-Hamiltonian\" data-toc-modified-id=\"Restructuring-blocks-of-the-Hamiltonian-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Restructuring blocks of the Hamiltonian</a></span></li><li><span><a href=\"#OLD-AND-EXISTING\" data-toc-modified-id=\"OLD-AND-EXISTING-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>OLD AND EXISTING</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3096d21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:37:43.352724Z",
     "start_time": "2022-12-20T16:37:41.651991Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2bb576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:40:10.349581Z",
     "start_time": "2022-12-20T16:39:45.235775Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from ase.io import read\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "# from rascal.representations import SphericalExpansion\n",
    "import copy\n",
    "from ase.units import Hartree\n",
    "\n",
    "from torch_hamiltonian_utils.torch_cg import ClebschGordanReal\n",
    "from torch_hamiltonian_utils.torch_hamiltonians import fix_pyscf_l1, dense_to_blocks, blocks_to_dense, couple_blocks, decouple_blocks, hamiltonian_features\n",
    "from torch_hamiltonian_utils.torch_builder import TensorBuilder\n",
    "\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "from equistore_utils.librascal import  RascalSphericalExpansion, RascalPairExpansion\n",
    "from equistore_utils.acdc_mini import acdc_standardize_keys, cg_increment, cg_combine\n",
    "from equistore_utils.model_hamiltonian import get_feat_keys \n",
    "\n",
    "import importlib\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b40dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from equistore_utils.model_hamiltonian import get_feat_keys_from_uncoupled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dadd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames1 = read(\"data/water-hamiltonian/water_coords_1000.xyz\",\":2\")\n",
    "frames2 =  read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "frames = frames1 #+ frames2\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "    \n",
    "jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "# jorbs = json.load(open('data/ethanol-hamiltonian/orbs_saph_ethanol.json', \"r\"))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1, \"C\":6}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]\n",
    "focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames1)]\n",
    "focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "focks = focks1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ba6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "##generating jagged matrix\n",
    "\n",
    "# f=[]\n",
    "# for x in focks1:\n",
    "#     f.append(x)\n",
    "# for x in focks2:\n",
    "#     f.append(x) \n",
    "# focks = np.asanyarray(f, dtype=object)\n",
    "# for f in focks: \n",
    "#     print(f.shape)\n",
    "# dense_to_blocks(focks, frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc18008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:40:11.217769Z",
     "start_time": "2022-12-20T16:40:11.168433Z"
    }
   },
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad712",
   "metadata": {},
   "source": [
    "## Manipulate Hamiltonian into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "461e38a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:40:11.311086Z",
     "start_time": "2022-12-20T16:40:11.222371Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "8a026cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Atoms(symbols='C2OH6', pbc=False, cell=[100.0, 100.0, 100.0]),\n",
       " Atoms(symbols='C2OH6', pbc=False, cell=[100.0, 100.0, 100.0])]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "7c03608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blockidx [3, 4, 5, 18]\n",
      "keys [(1, 6, 2, 0, 6, 2, 0) (1, 6, 2, 0, 6, 2, 1) (1, 6, 2, 1, 6, 2, 1)\n",
      " (1, 1, 1, 0, 1, 1, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"blockidx\", blocks.blocks_matching(block_type=1))\n",
    "print(\"keys\", blocks.keys[blocks.blocks_matching(block_type=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "9a637a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels([(2,  1, 0, 8, 8, 0), (2, -1, 1, 8, 8, 0), (2,  1, 2, 8, 8, 0)],\n",
       "       dtype=[('order_nu', '<i4'), ('inversion_sigma', '<i4'), ('spherical_harmonics_l', '<i4'), ('species_center', '<i4'), ('species_neighbor', '<i4'), ('block_type', '<i4')])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feat_keys_from_uncoupled(blocks.keys[2],order_nu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "977342b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T19:25:14.739883Z",
     "start_time": "2022-12-20T19:25:14.731416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8332]],\n",
       "\n",
       "        [[-0.7726]]])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_bc.block(0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8cfe3",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "13064b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:01.430128Z",
     "start_time": "2022-12-20T16:41:01.421540Z"
    }
   },
   "outputs": [],
   "source": [
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 4.0,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 4,\n",
    "    \"max_angular\": 2,\n",
    "    \"gaussian_sigma_constant\" : 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\":  False,\n",
    "}\n",
    "\n",
    "spex = RascalSphericalExpansion(rascal_hypers)\n",
    "rhoi = spex.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "cc39e55f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:01.478465Z",
     "start_time": "2022-12-20T16:41:01.432194Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = RascalPairExpansion(rascal_hypers)\n",
    "gij = pairs.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "abf74c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:01.585374Z",
     "start_time": "2022-12-20T16:41:01.480000Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1i = acdc_standardize_keys(rhoi)\n",
    "rho1i.keys_to_properties(['species_neighbor'])\n",
    "gij =  acdc_standardize_keys(gij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "6a666711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:01.735877Z",
     "start_time": "2022-12-20T16:41:01.588767Z"
    }
   },
   "outputs": [],
   "source": [
    "rho2i = cg_increment(rho1i, rho1i, lcut=3, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "4565f04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:01.856829Z",
     "start_time": "2022-12-20T16:41:01.738918Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho3i = cg_increment(rho2i, rho1i, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "bdcb6035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:01.972456Z",
     "start_time": "2022-12-20T16:41:01.858545Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1ij = cg_increment(rho1i, gij, lcut=3, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "0c5d9d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:01.990275Z",
     "start_time": "2022-12-20T16:41:01.974322Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho2ij = cg_increment(rho2i, gij, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "b29eb700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:02.109928Z",
     "start_time": "2022-12-20T16:41:01.992242Z"
    }
   },
   "outputs": [],
   "source": [
    "features = hamiltonian_features(rho2i, rho1ij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704bdf2",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4022d0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (2, 1, 1, 1, 1, 0)\n",
      "2 (2, 1, 2, 1, 1, 0)\n",
      "6 (2, -1, 1, 1, 1, 0)\n",
      "7 (2, -1, 2, 1, 1, 0)\n",
      "8 (2, 1, 3, 1, 1, 0)\n",
      "10 (2, -1, 2, 8, 8, 0)\n",
      "11 (2, 1, 3, 8, 8, 0)\n",
      "12 (2, -1, 3, 1, 1, 0)\n",
      "13 (2, -1, 3, 8, 8, 0)\n",
      "17 (2, 1, 1, 1, 1, 1)\n",
      "18 (2, 1, 1, 1, 1, -1)\n",
      "20 (2, 1, 2, 1, 1, 1)\n",
      "21 (2, 1, 2, 1, 1, -1)\n",
      "22 (2, 1, 2, 1, 8, 2)\n",
      "23 (2, -1, 1, 1, 1, 1)\n",
      "24 (2, -1, 1, 1, 1, -1)\n",
      "25 (2, -1, 1, 1, 8, 2)\n",
      "26 (2, -1, 2, 1, 1, 1)\n",
      "27 (2, -1, 2, 1, 1, -1)\n",
      "28 (2, 1, 3, 1, 1, 1)\n",
      "29 (2, 1, 3, 1, 1, -1)\n",
      "30 (2, -1, 2, 1, 8, 2)\n",
      "31 (2, 1, 3, 1, 8, 2)\n",
      "32 (2, -1, 3, 1, 1, 1)\n",
      "33 (2, -1, 3, 1, 1, -1)\n",
      "34 (2, -1, 3, 1, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "idx_used = [0,3,4,5,9,14,15,16,19]\n",
    "for i in range(len(features.keys)):\n",
    "    if i in idx_used:\n",
    "        continue\n",
    "    print(i,features.keys[i])\n",
    "#checked keys of features not used - makes sense - either they are too high L or use inv_sigma thats not right \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "ee997771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:48:20.612484Z",
     "start_time": "2022-12-20T16:48:20.605818Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2839764941.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [442], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    for all keys:\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class HamModel(torch.nn.ModuleDict):\n",
    "    def __init__(self, frames, feature, target, regularization=None, seed=None, layer_size=None):\n",
    "        super().__init__()\n",
    "        self.frames= frames\n",
    "        self.features = features \n",
    "        self.target = target\n",
    "        self.models={}\n",
    "        for key, block in target:        \n",
    "            blocktype, L, nu,sigma,species1, species2 = get_feat_key(key)\n",
    "            block_feat = self.features.block(block_type=blocktype, spherical_harmonics_l=L, inversion_sigma=sigma, \n",
    "                                       species_center=species1, species_neighbor=species2)\n",
    "            \n",
    "            self.models[(key)] = BlockModel(block, block_feat, layer_size) \n",
    "            self.models[(key)].initialize_model(seed)\n",
    "            \n",
    "            \n",
    "    def forward(self, target):\n",
    "        pred_blocks =[]\n",
    "        for key, block in target:\n",
    "            pred = self.models[str(key)].forward(block)\n",
    "            newblock = TensorBlock(\n",
    "                        values=pred.reshape((-1, 1, 1)),\n",
    "                        samples=block.samples,\n",
    "                        components=block.components,\n",
    "                        properties= Labels([\"dummy\"], np.asarray([[0]], dtype=np.int32))\n",
    "                    )\n",
    "            pred_blocks.append(newblock) \n",
    "        \n",
    "        keys = target.keys\n",
    "        pred_target = TensorMap(keys, pred_blocks)\n",
    "        return(pred_target)\n",
    "    \n",
    "    def train_individual(self, train_dataloader_dict, optimizer_type, n_epochs, loss, lr):\n",
    "        #Iterates through the keys of self.model, then for each key we will fit self.model[key] with data[key]\n",
    "        for key in self.models:\n",
    "            print (\"Now training model {} of {}\".format(current, total))\n",
    "            #model[key].fit()\n",
    "            pass\n",
    "    def train_collective(optimizer_type):\n",
    "        data\n",
    "        opt = LBFGS\n",
    "        for all keys:\n",
    "            for data[key]:\n",
    "                pred_block = model[key](data[key])\n",
    "                \n",
    "        total block = \n",
    "        loss()\n",
    "        reg_loss\n",
    "        backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c84f80b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T19:19:32.376629Z",
     "start_time": "2022-12-20T19:19:32.366777Z"
    }
   },
   "outputs": [],
   "source": [
    "testmodel = BlockModel([10,20,30], [5,3,1], \"cpu\", keys = ['a', 'b', 'c'])\n",
    "weight = list(testmodel.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b4b7773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T19:22:29.698175Z",
     "start_time": "2022-12-20T19:22:29.690405Z"
    }
   },
   "outputs": [],
   "source": [
    "a = {}\n",
    "a[(1,0,1,1)] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f59845d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T18:58:08.212183Z",
     "start_time": "2022-12-20T18:58:08.175185Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlockModel(torch.nn.Module): #Currently only 1 model per block\n",
    "    def __init__(self, inputSize, outputSize, device, keys, seed = None, hidden_layers = None):\n",
    "        super().__init__()\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.device = device\n",
    "        self.keys = keys\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.initialize_model(seed)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.model = torch.nn.ModuleDict()\n",
    "        for index, key in enumerate(self.keys):\n",
    "            self.model[key] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = False)\n",
    "        else:\n",
    "            if self.hidden_layers is None: \n",
    "                self.nn = torch.nn.Linear(self.inputSize, self.outputSize, bias = False)\n",
    "            \n",
    "            else:\n",
    "                print (\"DNN not supported yet\")            \n",
    "#                 self.layer1 = nn.Linear(10,10)\n",
    "#                 self.act1 = nn.TanH()\n",
    "#                 self.layer2 = nn.Linear(10,100)\n",
    "#                 self.act2 = nn.Sigmoid()\n",
    "#                 self.nn = torch.nn.Sequential(\n",
    "#                     linear_init_weights(inputSize, self.layer_size),\n",
    "#                     torch.nn.Tanh(),\n",
    "#                     linear_init_weights(self.layer_size, self.layer_size),\n",
    "#                     torch.nn.Tanh(),\n",
    "#                     linear_init_weights(self.layer_size, OutputSize),\n",
    "#                 )\n",
    "           \n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred_values = []\n",
    "        for i, key in enumerate(x.key):\n",
    "            pred_values.append(self.model[key](x[key]))\n",
    "        pred_values.hstack???\n",
    "        pred = self.nn(x)\n",
    "#         pred = self.layer1(pred)\n",
    "#         pred = self.act1(pred)\n",
    "#         pred = self.layer2(pred)\n",
    "#         pred = self.act2(pred)\n",
    "#         pred = self.layer1(pred)\n",
    "        return pred \n",
    "\n",
    "    def fit(traindata_loader, loss_function, optimizer_type, lr, reg, nepochs = 5000):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 200\n",
    "            tol = 1e-2\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "            optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 30\n",
    "            tol = 1e-2\n",
    "            history_step = 10\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_threshold, gamma = 0.5)\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(npochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            \n",
    "            for x_data, y_data in traindata_loader: \n",
    "                optimizer.zero_grad()\n",
    "                x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        _pred = self.forward(x_data)\n",
    "                        _pred_loss = loss_function(_pred, y_data)       \n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                 \n",
    "                        _reg_loss = torch.sum(torch.pow(self.nn.weight,2)) #Only works for 1 layer\n",
    "                        _reg_loss *= reg.item() #Might only work for 1 layer\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    opt.step(closure)\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data)\n",
    "                    reg_loss = torch.sum(torch.pow(self.nn.weight,2))#Only works for 1 layer\n",
    "                    new_loss = pred_loss + reg_loss\n",
    "                    new_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data in traindata_loader:\n",
    "                    pred = self.forward(self.feature.values)\n",
    "                    current_loss  += loss_function(pred, y_data) #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = torch.sum(torch.pow(self.nn.weight,2))#Only works for 1 layer\n",
    "                new_loss = pred_loss + reg_loss\n",
    "                \n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    scheduler.step()\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa140247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f12db97",
   "metadata": {},
   "source": [
    "## Restructuring blocks of the Hamiltonian "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80275d",
   "metadata": {},
   "source": [
    "We need to merge blocktype +1 and -1 together as components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26ea61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames1 = read(\"data/water-hamiltonian/water_coords_1000.xyz\",\":2\")\n",
    "frames2 =  read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "frames = frames2 #+ frames2\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "    \n",
    "# jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "jorbs = json.load(open('data/ethanol-hamiltonian/orbs_saph_ethanol.json', \"r\"))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1, \"C\":6}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]\n",
    "focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames1)]\n",
    "focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "focks = focks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "533a9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_hamiltonian_utils.torch_hamiltonians import _orbs_offsets, _components_idx, _components_idx_2d\n",
    "def couple_blocks(blocks, cg=None):\n",
    "    if cg is None:\n",
    "        lmax = max(blocks.keys[\"li\"]+blocks.keys[\"lj\"])\n",
    "        cg = ClebschGordanReal(lmax)\n",
    "\n",
    "    block_builder = TensorBuilder([\"block_type\", \"a_i\", \"n_i\", \"l_i\", \"a_j\", \"n_j\", \"l_j\", \"L\"], [\"structure\", \"center\", \"neighbor\"], [[\"M\"]], [\"value\"])\n",
    "    for idx, block in blocks:\n",
    "        block_type, ai, ni, li, aj, nj, lj = tuple(idx)\n",
    "        decoupled = torch.moveaxis(block.values,-1,-2).reshape((len(block.samples), len(block.properties), 2*li+1, 2*lj+1))\n",
    "        coupled = cg.couple(decoupled)[(li,lj)]\n",
    "        for L in coupled:\n",
    "            bsamples = list(np.array(block.samples.asarray(), dtype=int))\n",
    "            samples_keep = list(range(len(bsamples)))\n",
    "            block_idx = tuple(idx) + (L,)\n",
    "            # skip blocks that are zero because of symmetry\n",
    "            if ai==aj and ni==nj and li==lj:\n",
    "                parity = (-1)**(li+lj+L)\n",
    "                if (parity == -1 and block_type ==0):\n",
    "                    continue\n",
    "                elif block_type == 1:\n",
    "                    if parity==-1:\n",
    "                        #remove samples with A>0 \n",
    "                        sample_idx=np.where(block.samples[\"structure\"]>0)[0]\n",
    "                        coupled[L][sample_idx] = 0\n",
    "\n",
    "                    else:\n",
    "                        #remove samples with A<0\n",
    "                        sample_idx=np.where(block.samples[\"structure\"]<0)[0]\n",
    "                        coupled[L][sample_idx] = 0\n",
    "#                         for i in sorted(sample_idx, reverse=True):\n",
    "#                             bsamples.pop(i)\n",
    "#                             samples_keep.pop(i)\n",
    "                        \n",
    "            new_block = block_builder.add_block(keys=block_idx, properties=np.asarray([[0]], dtype=np.int32), \n",
    "                            components=[_components_idx(L).reshape(-1,1)] )\n",
    "                \n",
    "            new_block.add_samples(labels=np.asarray(bsamples,dtype=np.int32).reshape(len(bsamples),-1), \n",
    "                            data=torch.moveaxis(coupled[L][samples_keep], -1, -2 ) )\n",
    "\n",
    "    return block_builder.build()\n",
    "\n",
    "def decouple_blocks(blocks, cg=None):\n",
    "    if cg is None:\n",
    "        lmax = max(blocks.keys[\"L\"])\n",
    "        cg = ClebschGordanReal(lmax)\n",
    "\n",
    "    block_builder = TensorBuilder([\"block_type\", \"a_i\", \"n_i\", \"l_i\", \"a_j\", \"n_j\", \"l_j\"], [\"structure\", \"center\", \"neighbor\"], [[\"m1\"], [\"m2\"]], [\"value\"])\n",
    "    for idx, block in blocks:\n",
    "        block_type, ai, ni, li, aj, nj, lj, L = tuple(idx)\n",
    "        block_idx = (block_type, ai, ni, li, aj, nj, lj)\n",
    "        bsamples=[]\n",
    "        if block_idx in block_builder.blocks:\n",
    "            continue        \n",
    "        coupled = {}\n",
    "        for L in range(np.abs(li-lj), li+lj+1):\n",
    "            bidx = blocks.keys.position(block_idx+(L,))\n",
    "            if bidx is not None:\n",
    "#                 print(idx, blocks.block(bidx).values.shape, torch.moveaxis(blocks.block(bidx).values, -1, -2) .shape)\n",
    "                coupled[L] = torch.moveaxis(blocks.block(bidx).values, -1, -2) \n",
    "                bsamples.append(blocks.block(bidx).samples)\n",
    "        decoupled = cg.decouple( {(li,lj):coupled})\n",
    "        \n",
    "        new_block = block_builder.add_block(keys=block_idx, properties=np.asarray([[0]], dtype=np.int32), \n",
    "                            components=[_components_idx(li), _components_idx(lj)] )\n",
    "        new_block.add_samples(labels=block.samples.view(dtype=np.int32).reshape(block.samples.shape[0],-1),\n",
    "                            data=torch.moveaxis(decoupled, 1, -1))\n",
    "    return block_builder.build()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bc6eafd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8, 24]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_bc.blocks_matching(block_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "25d607d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels([(1, 3, 4), (1, 3, 5), (1, 3, 6), (1, 3, 7), (1, 3, 8), (1, 4, 5),\n",
       "        (1, 4, 6), (1, 4, 7), (1, 4, 8), (1, 5, 6), (1, 5, 7), (1, 5, 8),\n",
       "        (1, 6, 7), (1, 6, 8), (1, 7, 8), (2, 3, 4), (2, 3, 5), (2, 3, 6),\n",
       "        (2, 3, 7), (2, 3, 8), (2, 4, 5), (2, 4, 6), (2, 4, 7), (2, 4, 8),\n",
       "        (2, 5, 6), (2, 5, 7), (2, 5, 8), (2, 6, 7), (2, 6, 8), (2, 7, 8)],\n",
       "       dtype=[('structure', '<i4'), ('center', '<i4'), ('neighbor', '<i4')])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_bc.block(24).samples#values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "604f0c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(np.abs(1), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fc2c0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_hamiltonian_utils.torch_hamiltonians import _orbs_offsets, _components_idx, _components_idx_2d\n",
    "def couple_blocks(blocks, cg=None):\n",
    "    if cg is None:\n",
    "        lmax = max(blocks.keys[\"li\"]+blocks.keys[\"lj\"])\n",
    "        cg = ClebschGordanReal(lmax)\n",
    "\n",
    "    block_builder = TensorBuilder([\"block_type\", \"a_i\", \"n_i\", \"l_i\", \"a_j\", \"n_j\", \"l_j\", \"L\"], [\"structure\", \"center\", \"neighbor\"], [[\"M\"]], [\"value\"])\n",
    "    for idx, block in blocks:\n",
    "        block_type, ai, ni, li, aj, nj, lj = tuple(idx)\n",
    "        decoupled = torch.moveaxis(block.values,-1,-2).reshape((len(block.samples), len(block.properties), 2*li+1, 2*lj+1))\n",
    "        coupled = cg.couple(decoupled)[(li,lj)]\n",
    "        for L in coupled:\n",
    "            bsamples = list(np.array(block.samples.asarray(), dtype=int))\n",
    "            samples_keep = list(range(len(bsamples)))\n",
    "            block_idx = tuple(idx) + (L,)\n",
    "            # skip blocks that are zero because of symmetry\n",
    "            if ai==aj and ni==nj and li==lj:\n",
    "                parity = (-1)**(li+lj+L)\n",
    "                if (parity == -1 and block_type ==0):\n",
    "                    continue\n",
    "                elif block_type == 1:\n",
    "                    if parity==-1:\n",
    "                        #remove samples with A>0 \n",
    "                        sample_idx=np.where(block.samples[\"structure\"]>0)[0]\n",
    "                        for i in sorted(sample_idx, reverse=True):\n",
    "                            bsamples.pop(i)\n",
    "                            samples_keep.pop(i)\n",
    "\n",
    "                    else:\n",
    "                        #parity =1 and remove samples with A<0\n",
    "                        sample_idx=np.where(block.samples[\"structure\"]<0)[0]\n",
    "                        for i in sorted(sample_idx, reverse=True):\n",
    "                            bsamples.pop(i)\n",
    "                            samples_keep.pop(i)\n",
    "                        \n",
    "            new_block = block_builder.add_block(keys=block_idx, properties=np.asarray([[0]], dtype=np.int32), \n",
    "                            components=[_components_idx(L).reshape(-1,1)] )\n",
    "                \n",
    "            new_block.add_samples(labels=np.asarray(bsamples,dtype=np.int32).reshape(len(bsamples),-1), \n",
    "                            data=torch.moveaxis(coupled[L][samples_keep], -1, -2 ) )\n",
    "\n",
    "    return block_builder.build()\n",
    "\n",
    "def decouple_blocks(blocks, cg=None):\n",
    "    #append and add to samples carefully  in coupled[L] - otherwise decoupled will screw up \n",
    "    if cg is None:\n",
    "        lmax = max(blocks.keys[\"L\"])\n",
    "        cg = ClebschGordanReal(lmax)\n",
    "    block_builder = TensorBuilder([\"block_type\", \"a_i\", \"n_i\", \"l_i\", \"a_j\", \"n_j\", \"l_j\"], [\"structure\", \"center\", \"neighbor\"], [[\"m1\"], [\"m2\"]], [\"value\"])\n",
    "   \n",
    "    for idx, block in blocks:\n",
    "        block_type, ai, ni, li, aj, nj, lj, L = tuple(idx)\n",
    "        block_idx = (block_type, ai, ni, li, aj, nj, lj)\n",
    "        bsamples = block.samples\n",
    "        \n",
    "        if block_type==1 and ni==nj and li==lj and len(range(np.abs(li-lj), li+lj+1))>1: \n",
    "            s1 = blocks.block(blocks.keys.position(block_idx+(np.abs(li-lj),))).samples \n",
    "            s2 = s1.copy()\n",
    "            s2[\"structure\"]=s2[\"structure\"]*(-1)\n",
    "            if s1[\"structure\"][0]>0 :\n",
    "                even_sample = s1\n",
    "                odd_sample = s2\n",
    "            else: \n",
    "                even_sample = s2\n",
    "                odd_sample = s1\n",
    "            bsamples = np.zeros((2*len(s1),), dtype = s1.dtype)\n",
    "            bsamples[0::2] = even_sample\n",
    "            bsamples[1::2] = odd_sample\n",
    "            \n",
    "        if block_idx in block_builder.blocks:\n",
    "            continue        \n",
    "        coupled = {}\n",
    "        for L in range(np.abs(li-lj), li+lj+1):\n",
    "            bidx = blocks.keys.position(block_idx+(L,))\n",
    "            if bidx is not None:\n",
    "                coupled[L] = torch.moveaxis(blocks.block(bidx).values, -1, -2)\n",
    "                if block_type==1 and ni==nj and li==lj and len(range(np.abs(li-lj), li+lj+1))>1: \n",
    "                    coupled[L] = torch.zeros(len(bsamples), 1, 2*L+1)\n",
    "                    if blocks.block(bidx).samples[\"structure\"][0]>0:\n",
    "                        print(idx, coupled[L].shape, torch.moveaxis(blocks.block(bidx).values, -1, -2).shape, bsamples.shape)\n",
    "                        coupled[L][0::2] =  torch.moveaxis(blocks.block(bidx).values, -1, -2)\n",
    "                    else: \n",
    "                        coupled[L][1::2] =  torch.moveaxis(blocks.block(bidx).values, -1, -2)\n",
    "\n",
    "\n",
    "        decoupled = cg.decouple({(li,lj):coupled})\n",
    "        \n",
    "        new_block = block_builder.add_block(keys=block_idx, properties=np.asarray([[0]], dtype=np.int32), \n",
    "                            components=[_components_idx(li), _components_idx(lj)] )\n",
    "        new_block.add_samples(labels=bsamples.view(dtype=np.int32).reshape(bsamples.shape[0],-1),\n",
    "                            data=torch.moveaxis(decoupled, 1, -1))\n",
    "    return block_builder.build()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "93b79989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_hamiltonian_utils.torch_hamiltonians import _orbs_offsets, _components_idx, _components_idx_2d, _atom_blocks_idx\n",
    "def blocks_to_dense(blocks, frames, orbs):\n",
    "    \"\"\"\n",
    "    Converts a TensorMap containing matrix blocks in the uncoupled basis, `blocks` into dense matrices.\n",
    "    Needs `frames` and `orbs` to reconstruct matrices in the correct order. See `dense_to_blocks` to understant\n",
    "    the different types of blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    orbs_tot, orbs_offset =  _orbs_offsets(orbs)\n",
    "    \n",
    "    atom_blocks_idx = _atom_blocks_idx(frames, orbs_tot)\n",
    "    \n",
    "    # init storage for the dense hamiltonians\n",
    "    dense = []        \n",
    "    for f in frames:\n",
    "        norbs = 0\n",
    "        for ai in f.numbers:\n",
    "            norbs += orbs_tot[ai]\n",
    "        ham = torch.zeros(norbs, norbs)#, dtype=np.float64)\n",
    "        dense.append(ham)\n",
    "\n",
    "    # loops over block types\n",
    "    for idx, block in blocks:\n",
    "        cur_struct = 9999\n",
    "        block_type, ai, ni, li, aj, nj, lj = tuple(idx)\n",
    "\n",
    "        # offset of the orbital block within the pair block in the matrix\n",
    "        ki_offset = orbs_offset[(ai,ni,li)]\n",
    "        kj_offset = orbs_offset[(aj,nj,lj)]\n",
    "        \n",
    "        # loops over samples (structure, i, j)\n",
    "        \n",
    "        for (A,i,j), block_data in zip(block.samples, block.values): \n",
    "            if A<0:\n",
    "                    continue \n",
    "            if A != cur_struct:\n",
    "                cur_struct = A - 1\n",
    "                ham = dense[cur_struct]\n",
    "            # coordinates of the atom block in the matrix\n",
    "            ki_base, kj_base = atom_blocks_idx[(cur_struct ,i,j)]\n",
    "            islice = slice(ki_base+ki_offset, ki_base+ki_offset+2*li+1)\n",
    "            jslice = slice(kj_base+kj_offset, kj_base+kj_offset+2*lj+1)\n",
    "\n",
    "            # print(i, ni, li, ki_base, ki_offset)\n",
    "            if block_type == 0:\n",
    "                ham[islice, jslice] = block_data[:,:,0].reshape(2*li+1,2*lj+1)\n",
    "                if ki_offset != kj_offset:\n",
    "                    ham[jslice, islice] = block_data[:,:,0].reshape(2*li+1,2*lj+1).T\n",
    "            elif block_type == 2:\n",
    "                ham[islice, jslice] = block_data[:,:,0].reshape(2*li+1,2*lj+1)\n",
    "                ham[jslice, islice] = block_data[:,:,0].reshape(2*li+1,2*lj+1).T   \n",
    "                \n",
    "            elif block_type == 1:\n",
    "                block_idx_minus = np.where(np.logical_and(np.logical_and(block.samples[\"structure\"]==-A,  block.samples[\"center\"]==i), block.samples[\"neighbor\"]==j))\n",
    "#                 print(idx, block_idx_minus, block.values[block_idx_minus].shape, li, lj)\n",
    "#                 print(block.samples[block_idx_minus], A, i, j)\n",
    "                if block_idx_minus[0]:\n",
    "                    print(block_idx_minus)\n",
    "                    assert block.values[block_idx_minus][0,:,:,0].shape == block_data[:,:,0].shape\n",
    "                    block_data_minus = block.values[block_idx_minus][0,:,:,0].reshape(2*li+1,2*lj+1)  / np.sqrt(2)\n",
    "                else: \n",
    "                    block_data_minus = np.zeros_like(block_data)\n",
    "                block_data_plus = block_data[:,:,0].reshape(2*li+1,2*lj+1)  / np.sqrt(2)\n",
    "#                 print(cur_struct , block_data.shape)\n",
    "                ham[islice, jslice] +=  block_data_plus\n",
    "                ham[jslice, islice] +=  block_data_plus.T\n",
    "                ham[islice, jslice] += block_data_minus\n",
    "                ham[jslice, islice] += block_data_minus.T\n",
    "                if ki_offset != kj_offset:\n",
    "                    islice = slice(ki_base+kj_offset, ki_base+kj_offset+2*lj+1)\n",
    "                    jslice = slice(kj_base+ki_offset, kj_base+ki_offset+2*li+1)\n",
    "                    \n",
    "                    ham[islice, jslice] += block_data_plus.T\n",
    "                    ham[jslice, islice] += block_data_plus\n",
    "                    \n",
    "                    ham[islice, jslice] -= block_data_minus .T\n",
    "                    ham[jslice, islice] -= block_data_minus\n",
    "                \n",
    "                    \n",
    "    return dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "7f43f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 2, 1, 6, 2, 1, 0) torch.Size([4, 1, 1]) torch.Size([2, 1, 1]) (4,)\n",
      "(1, 6, 2, 1, 6, 2, 1, 0) torch.Size([4, 1, 5]) torch.Size([2, 1, 5]) (4,)\n",
      "(array([1]),)\n",
      "(array([3]),)\n",
      "(array([1]),)\n",
      "(array([3]),)\n",
      "(array([1]),)\n",
      "(array([3]),)\n",
      "(array([1]),)\n",
      "(array([3]),)\n",
      "(array([1]),)\n",
      "(array([3]),)\n",
      "(array([1]),)\n",
      "(array([3]),)\n",
      "(array([5]),)\n",
      "(array([7]),)\n",
      "(array([9]),)\n",
      "(array([11]),)\n",
      "(array([13]),)\n",
      "(array([15]),)\n",
      "(array([17]),)\n",
      "(array([19]),)\n",
      "(array([21]),)\n",
      "(array([23]),)\n",
      "(array([25]),)\n",
      "(array([27]),)\n",
      "(array([29]),)\n",
      "(array([31]),)\n",
      "(array([33]),)\n",
      "(array([35]),)\n",
      "(array([37]),)\n",
      "(array([39]),)\n",
      "(array([41]),)\n",
      "(array([43]),)\n",
      "(array([45]),)\n",
      "(array([47]),)\n",
      "(array([49]),)\n",
      "(array([51]),)\n",
      "(array([53]),)\n",
      "(array([55]),)\n",
      "(array([57]),)\n",
      "(array([59]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/81/mvs1jcdn4871nrddg_30v9b40000gn/T/ipykernel_92048/998866105.py:57: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if block_idx_minus[0]:\n"
     ]
    }
   ],
   "source": [
    "blocks = dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)\n",
    "fock_dc = decouple_blocks(fock_bc, cg)\n",
    "r = blocks_to_dense(fock_dc, frames, orbs)\n",
    "rp = blocks_to_dense(blocks, frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b8b19d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc= couple_blocks(fock_dc, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "90971a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3716912657634958e-07\n",
      "0 1.3384690121057745e-07\n",
      "1 1.4969581206791603e-07\n",
      "1 1.4666266862600315e-07\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(focks):\n",
    "    print(i, np.linalg.norm(r[i] - focks[i]))\n",
    "    print(i, np.linalg.norm(rp[i] - focks[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e09174c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fock_bc2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfock_bc2\u001b[49m\u001b[38;5;241m.\u001b[39mblock(block_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;66;03m#values\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fock_bc2' is not defined"
     ]
    }
   ],
   "source": [
    "fock_bc2.block(block_type=-1, L=1).samples#values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bd2f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 6, 2, 0, 6, 2, 0, 0) torch.Size([4, 1, 1]) torch.Size([4, 1, 1])\n",
      "(0, 6, 2, 0, 6, 2, 1, 1) torch.Size([4, 3, 1]) torch.Size([4, 1, 3])\n",
      "(0, 6, 2, 1, 6, 2, 1, 0) torch.Size([4, 1, 1]) torch.Size([4, 1, 1])\n",
      "(0, 6, 2, 1, 6, 2, 1, 0) torch.Size([4, 5, 1]) torch.Size([4, 1, 5])\n",
      "(1, 6, 2, 0, 6, 2, 0, 0) torch.Size([2, 1, 1]) torch.Size([2, 1, 1])\n",
      "(1, 6, 2, 0, 6, 2, 1, 1) torch.Size([2, 3, 1]) torch.Size([2, 1, 3])\n",
      "(-1, 6, 2, 0, 6, 2, 1, 1) torch.Size([2, 3, 1]) torch.Size([2, 1, 3])\n",
      "(1, 6, 2, 1, 6, 2, 1, 0) torch.Size([2, 1, 1]) torch.Size([2, 1, 1])\n",
      "(1, 6, 2, 1, 6, 2, 1, 0) torch.Size([2, 5, 1]) torch.Size([2, 1, 5])\n",
      "(-1, 6, 2, 1, 6, 2, 1, 1) torch.Size([2, 3, 1]) torch.Size([2, 1, 3])\n",
      "(2, 6, 2, 0, 8, 2, 0, 0) torch.Size([4, 1, 1]) torch.Size([4, 1, 1])\n",
      "(2, 6, 2, 0, 8, 2, 1, 1) torch.Size([4, 3, 1]) torch.Size([4, 1, 3])\n",
      "(2, 6, 2, 1, 8, 2, 0, 1) torch.Size([4, 3, 1]) torch.Size([4, 1, 3])\n",
      "(2, 6, 2, 1, 8, 2, 1, 0) torch.Size([4, 1, 1]) torch.Size([4, 1, 1])\n",
      "(2, 6, 2, 1, 8, 2, 1, 0) torch.Size([4, 3, 1]) torch.Size([4, 1, 3])\n",
      "(2, 6, 2, 1, 8, 2, 1, 0) torch.Size([4, 5, 1]) torch.Size([4, 1, 5])\n",
      "(0, 8, 2, 0, 8, 2, 0, 0) torch.Size([2, 1, 1]) torch.Size([2, 1, 1])\n",
      "(0, 8, 2, 0, 8, 2, 1, 1) torch.Size([2, 3, 1]) torch.Size([2, 1, 3])\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) torch.Size([2, 1, 1]) torch.Size([2, 1, 1])\n",
      "(0, 8, 2, 1, 8, 2, 1, 0) torch.Size([2, 5, 1]) torch.Size([2, 1, 5])\n",
      "(2, 1, 1, 0, 6, 2, 0, 0) torch.Size([24, 1, 1]) torch.Size([24, 1, 1])\n",
      "(2, 1, 1, 0, 6, 2, 1, 1) torch.Size([24, 3, 1]) torch.Size([24, 1, 3])\n",
      "(2, 1, 1, 0, 8, 2, 0, 0) torch.Size([12, 1, 1]) torch.Size([12, 1, 1])\n",
      "(2, 1, 1, 0, 8, 2, 1, 1) torch.Size([12, 3, 1]) torch.Size([12, 1, 3])\n",
      "(0, 1, 1, 0, 1, 1, 0, 0) torch.Size([12, 1, 1]) torch.Size([12, 1, 1])\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) torch.Size([30, 1, 1]) torch.Size([30, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch_hamiltonian_utils.torch_hamiltonians import old_dense_to_blocks, old_blocks_to_dense\n",
    "blocks2 = old_dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc2 = couple_blocks(blocks2, cg)\n",
    "fock_dc2 = decouple_blocks(fock_bc2, cg)\n",
    "r2= old_blocks_to_dense(fock_dc2, frames, orbs)\n",
    "rp2 = old_blocks_to_dense(blocks2, frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc8fa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, b in fock_dc2:\n",
    "    if np.linalg.norm(blocks2.block(k).values - b.values)>1e-7:\n",
    "        print(k, np.linalg.norm(blocks2.block(k).values - b.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7612017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3716912657634958e-07\n",
      "1 1.4969581206791603e-07\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(focks):\n",
    "    print(i, np.linalg.norm(r2[i] - focks[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3a5391b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 2, 0, 6, 2, 0, 0) 0.0\n",
      "(1, 6, 2, 0, 6, 2, 1, 1) 0.0\n",
      "[(-1, 6, 2, 0, 6, 2, 1, 1)] 0.0\n",
      "(1, 6, 2, 1, 6, 2, 1, 0) 0.0\n",
      "(1, 6, 2, 1, 6, 2, 1, 2) 0.0\n",
      "(1, 1, 1, 0, 1, 1, 0, 0) 0.0\n",
      "(1, 6, 2, 0, 6, 2, 0) 0.0\n",
      "(1, 6, 2, 0, 6, 2, 1) 0.0\n",
      "0.5378022878260719 0.5378022878260719\n",
      "[(-1, 6, 2, 0, 6, 2, 1)] 0.0\n",
      "(1, 6, 2, 1, 6, 2, 1) 0.0\n",
      "0.02060351348093143 0.0\n",
      "[(-1, 6, 2, 1, 6, 2, 1)] 0.02060351348093143\n",
      "(1, 1, 1, 0, 1, 1, 0) 0.0\n"
     ]
    }
   ],
   "source": [
    "for k, b in fock_bc:\n",
    "    n = len(fock_bc2.block(k).values)\n",
    "    if k[\"block_type\"]==1:\n",
    "        print(k,np.linalg.norm(fock_bc2.block(k).values - b.values[::2]))\n",
    "        mkey = Labels(k.dtype.names, np.asarray([(-1, k[1], k[2], k[3], k[4], k[5], k[6], k[7])], np.int32))\n",
    "        if fock_bc2.keys.position(mkey[0]):\n",
    "            print(mkey,np.linalg.norm(fock_bc2.block(mkey).values - b.values[1::2]) )\n",
    "\n",
    "for k, b in fock_dc:\n",
    "    n = len(fock_dc2.block(k).values)\n",
    "    if k[\"block_type\"]==1:\n",
    "        print(k,np.linalg.norm(fock_dc2.block(k).values - b.values[::2]))\n",
    "        mkey = Labels(k.dtype.names, np.asarray([(-1, k[1], k[2], k[3], k[4], k[5], k[6])], np.int32))\n",
    "        if fock_dc2.keys.position(mkey[0]):\n",
    "            print(np.linalg.norm(fock_dc2.block(mkey).values), np.linalg.norm(b.values[1::2]) )\n",
    "            print(mkey,np.linalg.norm(fock_dc2.block(mkey).values - b.values[1::2]) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "f96d6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "?decouple_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df6e62c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('block_type', 'a_i', 'n_i', 'l_i', 'a_j', 'n_j', 'l_j')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks.keys.dtype.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bca31",
   "metadata": {},
   "source": [
    "## OLD AND EXISTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bbc7bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------------\n",
    "#Soln 1 - have the same number of components - so add a redundant dimension of perm_sym across ALL blocks -this code\n",
    "#--------------------------------------------------\n",
    "from torch_hamiltonian_utils.torch_hamiltonians import _orbs_offsets, _components_idx, _components_idx_2d\n",
    "\n",
    "def dense_to_blocks(dense, frames, orbs):\n",
    "    \"\"\"\n",
    "    Converts a list of dense matrices `dense` corresponding to the single-particle Hamiltonians for the structures\n",
    "    described by `frames`, and using the orbitals described in the dictionary `orbs` into a TensorMap storage format.\n",
    "\n",
    "    The label convention is as follows: \n",
    "\n",
    "    The keys that label the blocks are [\"block_type\", \"a_i\", \"n_i\", \"l_i\", \"a_j\", \"n_j\", \"l_j\"].\n",
    "    block_type: 0 -> diagonal blocks, atom i=j\n",
    "                2 -> different species block, stores only when n_i,l_i and n_j,l_j are lexicographically sorted\n",
    "                1,-1 -> same specie, off-diagonal. store separately symmetric (1) and anti-symmetric (-1) term\n",
    "    a_{i,j}: chemical species (atomic number) of the two atoms\n",
    "    n_{i,j}: radial channel\n",
    "    l_{i,j}: angular momentum\n",
    "    \"\"\"\n",
    "\n",
    "    block_builder = TensorBuilder([\"block_type\", \"a_i\", \"n_i\", \"l_i\", \"a_j\", \"n_j\", \"l_j\"], [\"structure\", \"center\", \"neighbor\"], [[\"m1\"], [\"m2\"], [\"perm_sym\"]], [\"value\"])\n",
    "    orbs_tot, _ = _orbs_offsets(orbs)\n",
    "    for A in range(len(frames)):\n",
    "        frame = frames[A]\n",
    "        ham = dense[A]\n",
    "        ki_base = 0\n",
    "        for i, ai in enumerate(frame.numbers):\n",
    "            kj_base = 0\n",
    "            for j, aj in enumerate(frame.numbers):\n",
    "                if i==j:\n",
    "                    block_type = 0  # diagonal\n",
    "                elif ai==aj:\n",
    "                    block_type = 1  # same-species\n",
    "                    if i>j:\n",
    "                        kj_base+=orbs_tot[aj]\n",
    "                        continue\n",
    "                else:\n",
    "                    block_type = 2  # different species\n",
    "                    if ai>aj: # only sorted element types\n",
    "                        kj_base += orbs_tot[aj]\n",
    "                        continue\n",
    "                block_data = torch.from_numpy(ham[ki_base:ki_base+orbs_tot[ai], kj_base:kj_base+orbs_tot[aj]])\n",
    "                \n",
    "                if block_type == 1:\n",
    "                    block_data_plus = (block_data + block_data.T) *1/np.sqrt(2)\n",
    "                    block_data_minus = (block_data - block_data.T) *1/np.sqrt(2)\n",
    "                ki_offset = 0\n",
    "                for ni, li, mi in orbs[ai]:\n",
    "                    if mi != -li: # picks the beginning of each (n,l) block and skips the other orbitals\n",
    "                        continue\n",
    "                    kj_offset = 0\n",
    "                    for nj, lj, mj in orbs[aj]:\n",
    "                        if mj != -lj: # picks the beginning of each (n,l) block and skips the other orbitals\n",
    "                            continue                    \n",
    "                        if (ai==aj and (ni>nj or (ni==nj and li>lj))): \n",
    "                            kj_offset += 2*lj+1\n",
    "                            continue\n",
    "                        block_idx = (block_type, ai, ni, li, aj, nj, lj)\n",
    "                        if block_idx not in block_builder.blocks:\n",
    "                            block = block_builder.add_block(keys=block_idx, properties=np.asarray([[0]], dtype=np.int32),\n",
    "                                            components=[_components_idx(li), _components_idx(lj), np.array([+1,-1], np.int32).reshape(-1,1)] ) \n",
    "#                             if block_type %2 ==0:\n",
    "#                                 block = block_builder.add_block(keys=block_idx, properties=np.asarray([[0]], dtype=np.int32),\n",
    "#                                             components=[_components_idx(li), _components_idx(lj)] )\n",
    "                           \n",
    "                            \n",
    "#                             else:\n",
    "#                                 assert block_type == 1\n",
    "#                                 block = block_builder.add_block(keys=block_idx, properties=np.asarray([[0]], dtype=np.int32),\n",
    "#                                             components=[_components_idx(li), _components_idx(lj), np.array([+1,-1], np.int32).reshape(-1,1)] ) \n",
    "#------------------------------------------------\n",
    "# ideally one would like to have blocks with different number of components - as in the COMMENTED out code above\n",
    "# so the blocktype=1 will have a +-1 dimension on the components for permutation symmetry but not the other blockd\n",
    "# Currently this raises an EquistoreError as the TensorMap requires all blocks to have the same components \n",
    "\n",
    "#Soln 1 - have the same number of components - so add a redundant dimension of perm_sym across ALL blocks -this code\n",
    "#Soln 2 - append as component dimesnions for l1, l2 - the -1  \n",
    "#--------------------------------------------------\n",
    "\n",
    "                        else:\n",
    "                            block = block_builder.blocks[block_idx]\n",
    "                        \n",
    "                        islice = slice(ki_offset,ki_offset+2*li+1)\n",
    "                        jslice = slice(kj_offset,kj_offset+2*lj+1)\n",
    "                        \n",
    "                        if block_type == 1:\n",
    "                            bdata = torch.stack([block_data_plus, block_data_minus])\n",
    "#                             print(block_data.shape, block_data_plus[islice, jslice].shape, block_data[:2,islice, jslice].shape)\n",
    "                            block.add_samples(labels=[(A,i,j)],data=bdata[:, islice, jslice].swapaxes(0,-2).reshape((1,2*li+1,2*lj+1,2,1)) )\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            \n",
    "                            bdata = torch.stack([block_data, torch.empty_like(block_data)])\n",
    "                            print(bdata.shape, block_data.shape)\n",
    "#                             print(block_data.shape)\n",
    "                            block.add_samples(labels=[(A,i,j)], data=bdata[:,islice, jslice].swapaxes(0,-2).reshape((1,2*li+1,2*lj+1,2,1)) )\n",
    "                           \n",
    "                        \n",
    "                        kj_offset += 2*lj+1\n",
    "                    ki_offset += 2*li+1\n",
    "                kj_base+=orbs_tot[aj]\n",
    "\n",
    "            ki_base+=orbs_tot[ai]\n",
    "    return block_builder.build()\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b01d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-20T16:41:02.217312Z",
     "start_time": "2022-12-20T16:41:02.217300Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_init_weights(in_size, out_size):\n",
    "    m=torch.nn.Linear(in_size, out_size)\n",
    "    #m.weight.data.fill_(0)\n",
    "    m.bias.data.fill_(0)\n",
    "    return m \n",
    "\n",
    "class BlockModel(torch.nn.Module):\n",
    "    def __init__(self, frames, block, feat, optim, layer_size=None):\n",
    "        super().__init__()\n",
    "        self.frames = frames\n",
    "        self.nn = None\n",
    "        self.layer_size = layer_size\n",
    "        self.feature = feat\n",
    "        self.values = block\n",
    "        \n",
    "        \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        inputSize = self.feature.values.shape[-1] #feature dimension \n",
    "        outputSize = self.block.values.shape[0] #block sample size \n",
    "        \n",
    "        if self.layer_size is None: \n",
    "            self.nn = nn.Linear(inputSize, outputSize, bias = False)\n",
    "            \n",
    "        else:\n",
    "#             self.layer1 = nn.Linear(10,10)\n",
    "#             self.act1 = nn.TanH()\n",
    "#             self.layer2 = nn.Linear(10,100)\n",
    "#             self.act2 = nn.Sigmoid()\n",
    "            self.nn = torch.nn.Sequential(\n",
    "                linear_init_weights(inputSize, self.layer_size),\n",
    "                torch.nn.Tanh(),\n",
    "                linear_init_weights(self.layer_size, self.layer_size),\n",
    "                torch.nn.Tanh(),\n",
    "                linear_init_weights(self.layer_size, OutputSize),\n",
    "            )\n",
    "           \n",
    "        \n",
    "    def forward(self):\n",
    "        if self.nn is None:\n",
    "            raise Exception(\"call initialize_model first\")\n",
    "            \n",
    "        pred = self.nn(self.feature.values)\n",
    "#         pred = self.layer1(pred)\n",
    "#         pred = self.act1(pred)\n",
    "#         pred = self.layer2(pred)\n",
    "#         pred = self.act2(pred)\n",
    "#         pred = self.layer1(pred)\n",
    "        return pred \n",
    "\n",
    "    def fit(loss_function, optimizer_type, lr, reg, nepochs = 5000):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = self.reg.item())\n",
    "            threshold = 20000\n",
    "            scheduler_threshold = 20000\n",
    "            tol = 1e-3\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_threshold, gamma = 0.5)\n",
    "        best_state = copy.deepcopy(self.nn.state_dict())\n",
    "        \n",
    "        for epoch in tqdm(nepochs):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.nn(self.feature.values)\n",
    "            loss = loss_function(self.values, pred, self.frames, )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, coupled_blocks, features, weights=None, intercepts=None):\n",
    "        super().__init__()\n",
    "        self.coupled_blocks = coupled_blocks\n",
    "        self.features = features\n",
    "        self.weights = {}\n",
    "        if weights==None:\n",
    "            for idx_fock, block_fock in self.coupled_blocks:\n",
    "                block_type, ai, ni, li, aj, nj, lj, L = idx_fock\n",
    "                parity= (-1)**(li+lj+L)\n",
    "                size = self.features.block(block_type=block_type, spherical_harmonics_l=L,inversion_sigma=parity, \n",
    "                                       species_center=ai, species_neighbor=aj).values.shape[2]\n",
    "                #self.weights[idx_fock] = torch.nn.Parameter(torch.zeros(size, dtype=torch.float64))\n",
    "                self.weights[idx_fock] = torch.nn.Parameter(torch.randn(size, dtype=torch.float64))\n",
    "            \n",
    "        else: \n",
    "            self.weights = weights\n",
    "        \n",
    "        self.intercepts = {}\n",
    "        if intercepts is None:\n",
    "            for idx_fock, block_fock in self.coupled_blocks:\n",
    "                block_type, ai, ni, li, aj, nj, lj, L = idx_fock\n",
    "                parity= (-1)**(li+lj+L)\n",
    "                if L==0 and parity==1 and block_type==0:\n",
    "                    self.intercepts[idx_fock] = torch.nn.Parameter(torch.randn(1, dtype=torch.float64))\n",
    "                else:\n",
    "                    self.intercepts[idx_fock] = 0\n",
    "        else:\n",
    "            self.intercepts = intercepts\n",
    "         \n",
    "    def forward(self, features):\n",
    "        k = []\n",
    "        pred_blocks = []\n",
    "        for (idx, wts) in self.weights.items():\n",
    "            #print(wts)\n",
    "            block_type, ai, ni, li, aj, nj, lj, L = idx\n",
    "            k.append(list(idx))\n",
    "            parity= (-1)**(li+lj+L)\n",
    "            X = features.block(block_type=block_type, spherical_harmonics_l=L,inversion_sigma=parity, \n",
    "                                   species_center=ai, species_neighbor=aj)\n",
    "            X_new = torch.from_numpy(X.values.reshape(-1, X.values.shape[2]))\n",
    "            #print(idx, wts.shape, X.values.shape, X_new.shape)\n",
    "            Y = X_new @ wts + self.intercepts[idx]\n",
    "            \n",
    "            newblock = TensorBlock(\n",
    "                        values=Y.reshape((-1, 2 * L + 1, 1)),\n",
    "                        samples=X.samples,\n",
    "                        components=[Labels(\n",
    "                            [\"mu\"], np.asarray(range(-L, L + 1), dtype=np.int32).reshape(-1, 1)\n",
    "                        )],\n",
    "                        properties= Labels([\"values\"], np.asarray([[0]], dtype=np.int32))\n",
    "                    )\n",
    "            pred_blocks.append(newblock) \n",
    "        \n",
    "        keys = Labels(('block_type', 'a_i', 'n_i', 'l_i', 'a_j', 'n_j', 'l_j', 'L'), np.asarray(k, dtype=np.int32))\n",
    "        pred_fock = TensorMap(keys, pred_blocks)\n",
    "        return(pred_fock)\n",
    "        ### add direct eigenvalue prediction here as well\n",
    "    \n",
    "    def parameters(self):\n",
    "        for idx, wts in self.weights.items():\n",
    "            yield wts\n",
    "        for idx, wts in self.intercepts.items():\n",
    "            if type(wts) is not int:\n",
    "                yield wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "280ae8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "#Soln 2 - append as sample dimension - the -1   \n",
    "#--------------------------------------------------\n",
    "from torch_hamiltonian_utils.torch_hamiltonians import _orbs_offsets, _components_idx, _components_idx_2d,_atom_blocks_idx\n",
    "\n",
    "def blocks_to_dense(blocks, frames, orbs):\n",
    "    \"\"\"\n",
    "    Converts a TensorMap containing matrix blocks in the uncoupled basis, `blocks` into dense matrices.\n",
    "    Needs `frames` and `orbs` to reconstruct matrices in the correct order. See `dense_to_blocks` to understant\n",
    "    the different types of blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    orbs_tot, orbs_offset =  _orbs_offsets(orbs)\n",
    "    \n",
    "    atom_blocks_idx = _atom_blocks_idx(frames, orbs_tot)\n",
    "    \n",
    "    # init storage for the dense hamiltonians\n",
    "    dense = []        \n",
    "    for f in frames:\n",
    "        norbs = 0\n",
    "        for ai in f.numbers:\n",
    "            norbs += orbs_tot[ai]\n",
    "        ham = torch.zeros(norbs, norbs)#, dtype=np.float64)\n",
    "        dense.append(ham)\n",
    "\n",
    "    # loops over block types\n",
    "    for idx, block in blocks:\n",
    "        cur_struct = 9999\n",
    "        block_type, ai, ni, li, aj, nj, lj = tuple(idx)\n",
    "\n",
    "        # offset of the orbital block within the pair block in the matrix\n",
    "        ki_offset = orbs_offset[(ai,ni,li)]\n",
    "        kj_offset = orbs_offset[(aj,nj,lj)]\n",
    "        \n",
    "        # loops over samples (structure, i, j)\n",
    "        \n",
    "        for (A,i,j), block_data in zip(block.samples, block.values): \n",
    "            if A<0:\n",
    "                    continue \n",
    "            if A != cur_struct:\n",
    "                cur_struct = A - 1\n",
    "                ham = dense[cur_struct]\n",
    "            # coordinates of the atom block in the matrix\n",
    "            ki_base, kj_base = atom_blocks_idx[(cur_struct ,i,j)]\n",
    "            islice = slice(ki_base+ki_offset, ki_base+ki_offset+2*li+1)\n",
    "            jslice = slice(kj_base+kj_offset, kj_base+kj_offset+2*lj+1)\n",
    "\n",
    "            # print(i, ni, li, ki_base, ki_offset)\n",
    "            if block_type == 0:\n",
    "                ham[islice, jslice] = block_data[:,:,0].reshape(2*li+1,2*lj+1)\n",
    "                if ki_offset != kj_offset:\n",
    "                    ham[jslice, islice] = block_data[:,:,0].reshape(2*li+1,2*lj+1).T\n",
    "            elif block_type == 2:\n",
    "                ham[islice, jslice] = block_data[:,:,0].reshape(2*li+1,2*lj+1)\n",
    "                ham[jslice, islice] = block_data[:,:,0].reshape(2*li+1,2*lj+1).T   \n",
    "                \n",
    "            elif block_type == 1:\n",
    "\n",
    "                block_idx_minus = np.where(np.logical_and(block.samples[\"structure\"]==-A,  block.samples[\"center\"]==i, block.samples[\"neighbor\"]==j))\n",
    "                print(idx, block_idx_minus, block.values[block_idx_minus].shape, li, lj)\n",
    "#                 print(block.values[block_idx_minus][0,:,:,0].shape, block_data.shape), \n",
    "                block_data_minus = block.values[block_idx_minus][0,:,:,0].reshape(2*li+1,2*lj+1)  / np.sqrt(2)\n",
    "                block_data_plus = block_data[:,:,0].reshape(2*li+1,2*lj+1)  / np.sqrt(2)\n",
    "#                 print(cur_struct , block_data.shape)\n",
    "                ham[islice, jslice] +=  block_data_plus\n",
    "                ham[jslice, islice] +=  block_data_plus.T\n",
    "                ham[islice, jslice] += block_data_minus\n",
    "                ham[jslice, islice] += block_data_minus.T\n",
    "                if ki_offset != kj_offset:\n",
    "                    islice = slice(ki_base+kj_offset, ki_base+kj_offset+2*lj+1)\n",
    "                    jslice = slice(kj_base+ki_offset, kj_base+ki_offset+2*li+1)\n",
    "                    \n",
    "                    ham[islice, jslice] += block_data_plus.T\n",
    "                    ham[jslice, islice] += block_data_plus\n",
    "                    \n",
    "                    ham[islice, jslice] -= block_data_minus .T\n",
    "                    ham[jslice, islice] -= block_data_minus\n",
    "                \n",
    "                    \n",
    "    return dense\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b1f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe53c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "786px",
    "left": "27px",
    "top": "111.133px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
