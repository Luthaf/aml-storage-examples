{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5c7c0",
   "metadata": {
    "hide_input": true,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Manipulate-Hamiltonian-into-blocks\" data-toc-modified-id=\"Manipulate-Hamiltonian-into-blocks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Manipulate Hamiltonian into blocks</a></span></li><li><span><a href=\"#Feature-computation\" data-toc-modified-id=\"Feature-computation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature computation</a></span></li><li><span><a href=\"#Feature-Preprocessing\" data-toc-modified-id=\"Feature-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Preprocessing</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#DataLoader\" data-toc-modified-id=\"DataLoader-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DataLoader</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Tests</a></span><ul class=\"toc-item\"><li><span><a href=\"#set-up-wigner-d-rotations-matrices\" data-toc-modified-id=\"set-up-wigner-d-rotations-matrices-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>set up wigner-d rotations matrices</a></span></li><li><span><a href=\"#block-wise-rotations\" data-toc-modified-id=\"block-wise-rotations-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>block wise rotations</a></span></li><li><span><a href=\"#Rotate-matrix\" data-toc-modified-id=\"Rotate-matrix-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Rotate matrix</a></span></li><li><span><a href=\"#Eigenvalue-tests\" data-toc-modified-id=\"Eigenvalue-tests-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Eigenvalue tests</a></span></li><li><span><a href=\"#check-decoupling-of-blocks\" data-toc-modified-id=\"check-decoupling-of-blocks-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>check decoupling of blocks</a></span></li><li><span><a href=\"#check-feature-rotations\" data-toc-modified-id=\"check-feature-rotations-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>check feature rotations</a></span></li></ul></li><li><span><a href=\"#need-to-modify-samples-for-features-to-start-from-1\" data-toc-modified-id=\"need-to-modify-samples-for-features-to-start-from-1-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>need to modify samples for features to start from 1</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3096d21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:32:48.888548Z",
     "start_time": "2023-02-06T17:32:48.885877Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2bb576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.292787Z",
     "start_time": "2023-02-06T19:35:54.812257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ase.io\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "# from rascal.representations import SphericalExpansion\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from ase.units import Hartree\n",
    "\n",
    "from torch_hamiltonian_utils.torch_cg import ClebschGordanReal\n",
    "from torch_hamiltonian_utils.torch_hamiltonians import fix_pyscf_l1, lowdin_orthogonalize, dense_to_blocks, blocks_to_dense, couple_blocks, decouple_blocks, hamiltonian_features\n",
    "from torch_hamiltonian_utils.torch_builder import TensorBuilder\n",
    "\n",
    "import equistore\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "from equistore_utils.librascal import  RascalSphericalExpansion, RascalPairExpansion\n",
    "from equistore_utils.acdc_mini import acdc_standardize_keys, cg_increment, cg_combine\n",
    "from equistore_utils.model_hamiltonian import get_feat_keys, get_feat_keys_from_uncoupled \n",
    "\n",
    "import importlib\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68aeb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.301657Z",
     "start_time": "2023-02-06T19:36:04.294480Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feat_keys_from_uncoupled(block_keys, sigma=None, order_nu=None):\n",
    "    \"\"\"Map UNCOUPLED block keys to corresponding feature key. take as extra input the sigma, nu value if required.\n",
    "    sigma=0 returns all possible sigma values at given 'nu'\"\"\"\n",
    "    blocktype, species1, n1, l1, species2, n2, l2 = block_keys\n",
    "    feat_blocktype = blocktype\n",
    "    keys_L=[]\n",
    "    for L in range(abs(l1-l2), l1+l2+1):\n",
    "        if sigma is None:\n",
    "            z = (l1+l2+L)%2\n",
    "            inv_sigma = 1 - 2*z\n",
    "        elif abs(sigma)==1:\n",
    "            inv_sigma = sigma\n",
    "        else: \n",
    "            raise(\"Please check sigma value, it should be +1 or -1\")\n",
    "        \n",
    "        if blocktype == 1 and n1 == n2 and l1 == l2:\n",
    "            feat_blocktype = inv_sigma\n",
    "                 \n",
    "        if inv_sigma == -1 and blocktype == 0 and n1 == n2 and l1 == l2:\n",
    "            continue     \n",
    "        \n",
    "        keys_L.append([(order_nu, inv_sigma, L, species1, species2, feat_blocktype)])\n",
    "    #     feat= (blocktype, L,sigma,species1, species2)\n",
    "    feat = Labels([\"order_nu\", \"inversion_sigma\", \"spherical_harmonics_l\", \"species_center\", \"species_neighbor\", \"block_type\"], np.asarray(keys_L, dtype=np.int32).reshape(-1,6))\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10faaf55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.360121Z",
     "start_time": "2023-02-06T19:36:04.334928Z"
    }
   },
   "outputs": [],
   "source": [
    "def lowdin_orthogonalize(fock, s):\n",
    "    \"\"\"\n",
    "    lowdin orthogonalization of a fock matrix computing the square root of the overlap matrix\n",
    "    \"\"\"\n",
    "    eva, eve = np.linalg.eigh(s)\n",
    "    sm12 = eve @ np.diag(1.0/np.sqrt(eva)) @ eve.T\n",
    "    return sm12 @ fock @ sm12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b86aa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.644913Z",
     "start_time": "2023-02-06T19:36:04.361638Z"
    }
   },
   "outputs": [],
   "source": [
    "frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\":1\")\n",
    "# frames2 = ase.io.read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "frames = frames1 #+ frames2\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "#jorbs = json.loads(json.load(open('data/water-hamiltonian/orbs_def2_water.json', \"r\")))\n",
    "# jorbs = json.load(open('data/ethanol-hamiltonian/orbs_saph_ethanol.json', \"r\"))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1, \"C\":6}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]\n",
    "# \n",
    "# focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[:len(frames1)]\n",
    "# overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[:len(frames1)]\n",
    "\n",
    "# orthogonal = []\n",
    "# for i in range(len(focks)): \n",
    "#     focks[i] = fix_pyscf_l1(focks[i],frames[i], orbs)\n",
    "#     overlap[i] = fix_pyscf_l1(overlap[i],frames[i], orbs) \n",
    "#     orthogonal.append(lowdin_orthogonalize(focks[i], overlap[i]))\n",
    "#focks = np.asarray(orthogonal, dtype=np.float64)\n",
    "focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames1)]\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "focks = focks1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc18008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.359435Z",
     "start_time": "2023-02-06T19:36:04.651072Z"
    }
   },
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad712",
   "metadata": {},
   "source": [
    "## Manipulate Hamiltonian into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461e38a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.464335Z",
     "start_time": "2023-02-06T19:36:05.362511Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66014af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.484055Z",
     "start_time": "2023-02-06T19:36:05.465869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 8 blocks\n",
       "keys: ['block_type' 'a_i' 'n_i' 'l_i' 'a_j' 'n_j' 'l_j' 'L']\n",
       "            0        8     2     0     8     2     0    0\n",
       "            0        8     2     0     8     2     1    1\n",
       "            0        8     2     1     8     2     1    0\n",
       "            0        8     2     1     8     2     1    2\n",
       "            2        1     1     0     8     2     0    0\n",
       "            2        1     1     0     8     2     1    1\n",
       "            0        1     1     0     1     1     0    0\n",
       "            1        1     1     0     1     1     0    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8cfe3",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13064b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.561138Z",
     "start_time": "2023-02-06T19:36:05.487376Z"
    }
   },
   "outputs": [],
   "source": [
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 4.0,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 4,\n",
    "    \"gaussian_sigma_constant\" : 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\":  False,\n",
    "}\n",
    "\n",
    "spex = RascalSphericalExpansion(rascal_hypers)\n",
    "rhoi = spex.compute(frames)\n",
    "\n",
    "lmax = rascal_hypers[\"max_angular\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc39e55f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.672601Z",
     "start_time": "2023-02-06T19:36:05.562322Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = RascalPairExpansion(rascal_hypers)\n",
    "gij = pairs.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf74c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.714916Z",
     "start_time": "2023-02-06T19:36:05.674433Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1i = acdc_standardize_keys(rhoi)\n",
    "rho1i = rho1i.keys_to_properties(['species_neighbor'])\n",
    "gij =  acdc_standardize_keys(gij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a666711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:06.296929Z",
     "start_time": "2023-02-06T19:36:05.716462Z"
    }
   },
   "outputs": [],
   "source": [
    "rho2i = cg_increment(rho1i, rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4565f04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:06.946140Z",
     "start_time": "2023-02-06T19:36:06.942526Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho3i = cg_increment(rho2i, rho1i, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdcb6035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.120033Z",
     "start_time": "2023-02-06T19:36:07.441462Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1ij = cg_increment(rho1i, gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c5d9d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.124429Z",
     "start_time": "2023-02-06T19:36:08.121919Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho2ij = cg_increment(rho2i, gij, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b29eb700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.286694Z",
     "start_time": "2023-02-06T19:36:08.126318Z"
    }
   },
   "outputs": [],
   "source": [
    "features = hamiltonian_features(rho2i, rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7bf0c77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.566117Z",
     "start_time": "2023-02-06T19:36:08.482335Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import save\n",
    "save(\"feature.npz\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c7da8",
   "metadata": {},
   "source": [
    "## Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08174c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.576264Z",
     "start_time": "2023-02-06T19:36:09.568334Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_feats(feat, all_blocks=True): \n",
    "    all_norm = 0\n",
    "    normalized_blocks=[]\n",
    "    for block_idx, block in feat: \n",
    "        block_norm = np.linalg.norm(block.values)\n",
    "#         print(block_idx, block_norm)\n",
    "        all_norm = block_norm**2 * len(block.samples) \n",
    "    \n",
    "        newblock = TensorBlock(\n",
    "                        values=block.values/np.sqrt(all_norm ),\n",
    "                        samples=block.samples,\n",
    "                        components=block.components,\n",
    "                        properties= block.properties)                    \n",
    "        normalized_blocks.append(newblock) \n",
    "        \n",
    "    norm_feat = TensorMap(feat.keys, normalized_blocks)\n",
    "    raise Exception (\"Dont do it!\")\n",
    "    return norm_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "279a28b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.625063Z",
     "start_time": "2023-02-06T19:36:09.577966Z"
    }
   },
   "outputs": [],
   "source": [
    "#norm_feat = normalize_feats(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e55b753b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.674273Z",
     "start_time": "2023-02-06T19:36:09.627414Z"
    }
   },
   "outputs": [],
   "source": [
    "# from equistore.io import save\n",
    "# save(\"./norm_feat.npz\", norm_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27106a59",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4ffec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:47:24.977413Z",
     "start_time": "2023-02-07T10:47:24.965586Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import _labels_from_npz\n",
    "import equistore.operations as operations\n",
    "\n",
    "class HamiltonianDataset(torch.utils.data.Dataset):\n",
    "    #Dataset class\n",
    "    def __init__(self, feature_path, target, fock, frames, feature_nu = 2):\n",
    "        #\n",
    "        self.features = np.load(feature_path, mmap_mode = 'r')\n",
    "        #self.target = np.load(target_path, mmap_mode = 'r') \n",
    "        self.target = target #Uncoupled hamiltonian \n",
    "        #self.target = np.load(fock_path, mmap_mode = 'r')\n",
    "        self.fock = torch.tensor(fock) #fock matrix\n",
    "        self.keys_features = equistore.io._labels_from_npz(self.features[\"keys\"])\n",
    "        self.currentkey = self.target.keys[0]\n",
    "        self.feature_nu = feature_nu\n",
    "        self.frames = frames\n",
    "        \n",
    "        self.allfeatkey = []\n",
    "        for t_key in self.target.keys:\n",
    "            feature_key = self.get_feature_keys(t_key)\n",
    "            self.allfeatkey.append(feature_key)\n",
    "        #Remove Duplicates\n",
    "        nodupes = set()\n",
    "        for x in self.allfeatkey:\n",
    "            if len(x) > 1:\n",
    "                for z in x:\n",
    "                    nodupes.add(tuple(z))\n",
    "            else:\n",
    "                nodupes.add(tuple(x[0]))\n",
    "        \n",
    "        nodupes = np.array(list(nodupes), np.int32)\n",
    "        \n",
    "        self.allfeatkey = Labels(names = self.allfeatkey[0].dtype.names, values = nodupes)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, structure_idx):\n",
    "        feature_block, feature_key = self.generate_feature_block(self.features, structure_idx)        \n",
    "        #samples_filter, target_block_samples = self.get_index_from_idx(self.target.block(self.currentkey).samples, structure_idx)\n",
    "\n",
    "        if self.currentkey is None:\n",
    "            target_block = self.fock[sorted(structure_idx)]\n",
    "        else:\n",
    "            target_block = operations.slice_block(self.target.block(self.currentkey), \n",
    "                                                  samples = Labels(names = ['structure'], \n",
    "                                                    values = (np.array(structure_idx)+1).reshape(-1,1)) )\n",
    "        structure = [self.frames[i] for i in sorted(structure_idx)]\n",
    "        #Modify feature_block to tensormap\n",
    "        feature_map = TensorMap(feature_key, feature_block)\n",
    "        return feature_map, target_block, structure\n",
    "\n",
    "\n",
    "    def get_feature_keys(self,uncoupled_key):\n",
    "        return get_feat_keys_from_uncoupled(uncoupled_key, order_nu = self.feature_nu)\n",
    "    \n",
    "    def generate_feature_block(self, memmap, structure_idx):\n",
    "        #Generate the block from npz file\n",
    "        output = []\n",
    "        if self.currentkey is None:\n",
    "            feature_key = self.allfeatkey\n",
    "                \n",
    "        else:\n",
    "            feature_key = self.get_feature_keys(self.currentkey)\n",
    "            \n",
    "        for key in feature_key:\n",
    "            block_index = list(self.keys_features).index(key)\n",
    "            prefix = f\"blocks/{block_index}/values\"        \n",
    "            block_samples = equistore.io._labels_from_npz(memmap[f\"{prefix}/samples\"])\n",
    "            block_components = []\n",
    "            for i in range(1):\n",
    "                block_components.append(equistore.io._labels_from_npz(memmap[f\"{prefix}/components/{i}\"]))\n",
    "            block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "             \n",
    "\n",
    "            samples_filter, block_samples = self.get_index_from_idx(block_samples, structure_idx)\n",
    "            \n",
    "\n",
    "            block_data = memmap[f\"{prefix}/data\"][samples_filter]\n",
    "            block = TensorBlock(block_data, block_samples, block_components, block_properties)\n",
    "            output.append(block)\n",
    "        return output, feature_key\n",
    "    \n",
    "    def get_n_properties(self, memmap, key):\n",
    "        block_index = list(self.keys_features).index(key)\n",
    "        prefix = f\"blocks/{block_index}/values\"  \n",
    "        block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "        \n",
    "        return len(block_properties)\n",
    "    \n",
    "    def get_index_from_idx(self, block_samples, structure_idx):\n",
    "        #Get samples label from IDX\n",
    "        samples = Labels(names = ['structure'], values = np.array(structure_idx).reshape(-1,1))\n",
    "        \n",
    "        all_samples = block_samples[['structure']].tolist()\n",
    "        set_samples_to_slice = set(samples.tolist())\n",
    "        samples_filter = np.array(\n",
    "            [sample in set_samples_to_slice for sample in all_samples]\n",
    "        )\n",
    "        new_samples = block_samples[samples_filter]\n",
    "\n",
    "        return samples_filter, new_samples\n",
    "    \n",
    "    def collate_output_values(blocks):\n",
    "        feature_out = []\n",
    "        target_out = []\n",
    "        for sample_output in blocks:\n",
    "            feature_block, target_block, structure = sample_output\n",
    "            for z in feature_block:\n",
    "                feature_out.append(torch.tensor(z.values))\n",
    "            target_out.append(torch.tensor(target_block.values))\n",
    "\n",
    "        return feature_out, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ac1d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:47:25.034221Z",
     "start_time": "2023-02-07T10:47:24.979379Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_path = \"./feature.npz\"\n",
    "train_dataset = HamiltonianDataset(feature_path, blocks, focks, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ffe2cd",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c336070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:01:55.381711Z",
     "start_time": "2023-02-07T11:01:55.378061Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_blocks(block_tuple):\n",
    "    feature_tensor_map, target_block, structure_array = block_tuple[0]\n",
    "    \n",
    "    return feature_tensor_map, target_block, structure_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c4dd925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T12:31:03.632080Z",
     "start_time": "2023-02-07T12:31:03.621532Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "#Sampler = torch.utils.data.sampler.RandomSampler(test)\n",
    "Sampler = torch.utils.data.sampler.SequentialSampler(train_dataset)\n",
    "BSampler = torch.utils.data.sampler.BatchSampler(Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, sampler = BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704bdf2",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79125947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:14:38.877275Z",
     "start_time": "2023-02-07T11:14:38.874272Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def get_block_samples(t_key, feature_map):\n",
    "    f_key = get_feat_keys_from_uncoupled(t_key, None , 2)\n",
    "    ss = feature_map.block(f_key[0]).samples.copy()\n",
    "    ss[\"structure\"] = rankdata(np.abs(ss[\"structure\"]), method='dense')\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee997771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:58:14.016060Z",
     "start_time": "2023-02-07T10:58:13.979757Z"
    }
   },
   "outputs": [],
   "source": [
    "class HamModel(torch.nn.Module):\n",
    "    #Handles prediction of entire hamiltonian and derived results\n",
    "    def __init__(self, Hamiltonian_Dataset, device, regularization=None, seed=None, layer_size=None):\n",
    "        super().__init__()\n",
    "#         self.features = features \n",
    "#         self.target = target\n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        self.loss_history={}\n",
    "        self.device = device\n",
    "        self.target_keys = Hamiltonian_Dataset.target.keys\n",
    "        self.block_samples = {}\n",
    "        self.block_components = {}\n",
    "        for key in Hamiltonian_Dataset.target.keys:\n",
    "#             _block_type, _a_i, _n_i, _l_i, _a_j, _n_j, _l_j = key\n",
    "#             target_keys = Hamiltonian_Dataset.target.keys[Hamiltonian_Dataset.target.blocks_matching(\n",
    "#                 block_type = _block_type, a_i = _a_i, n_i = _n_i, l_i = _l_i, a_j = _a_j,\n",
    "#                 n_j = _n_j, l_j = _l_j)]\n",
    "            \n",
    "            #self.block_samples[str(key)] = Hamiltonian_Dataset.target.block(key).samples\n",
    "            self.block_components[str(key)] = Hamiltonian_Dataset.target.block(key).components\n",
    "        \n",
    "    \n",
    "            n_inputs = []\n",
    "            model_keys = []\n",
    "\n",
    "            feature_keys = Hamiltonian_Dataset.get_feature_keys(key)\n",
    "            for f_key in feature_keys: \n",
    "                n_features = Hamiltonian_Dataset.get_n_properties(Hamiltonian_Dataset.features, f_key)\n",
    "                n_inputs.append(n_features)\n",
    "                model_keys.append(f_key)\n",
    "                \n",
    "            n_outputs = np.ones_like(n_inputs)\n",
    "                \n",
    "            self.models[str(key)] = BlockModel(cg.decouple,n_inputs, n_outputs, device, model_keys, key, seed = seed, hiddenSize = layer_size)\n",
    "        self.to(device)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #Ham model uses target keys\n",
    "        pred_blocks = []\n",
    "        for t_key in self.target_keys:\n",
    "            \n",
    "            pred = self.models[str(t_key)].forward(x) #feature_tensormap must correspond to the correct features, model returns block\n",
    "            \n",
    "            #try:\n",
    "#             print (pred.shape)\n",
    "#             print ((2 * t_key['l_i'])+1)\n",
    "#             print ((2 * t_key['l_j']) + 1)\n",
    "            pred_block = TensorBlock(\n",
    "                    values=pred.reshape((-1, (2 * t_key['l_i'])+1, (2 * t_key['l_j']) + 1, 1)), #?\n",
    "                    samples = get_block_samples(t_key, x),\n",
    "                    components = self.block_components[str(t_key)] ,\n",
    "                    properties= Labels([\"dummy\"], np.asarray([[0]], dtype=np.int32))\n",
    "                )\n",
    "#             except:\n",
    "#                 print (t_key)\n",
    "#                 print (pred)\n",
    "#                 print (self.block_samples[str(t_key)])\n",
    "#                 print (self.block_components[str(t_key)])\n",
    "                \n",
    "            pred_blocks.append(pred_block)\n",
    "        pred_hamiltonian = TensorMap(self.target_keys, pred_blocks)\n",
    "        return(pred_hamiltonian)\n",
    "    \n",
    "    #write/fix forward function for train_indiv\n",
    "    \n",
    "    def train_individual(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #Iterates through the keys of self.model, then for each key we will fit self.model[key] with data[key]\n",
    "        total = len(self.models)\n",
    "        for index, t_key in enumerate(self.target_keys):\n",
    "            print (\"Now training on Block {} of {}\".format(index, total))\n",
    "            train_dataloader.dataset.currentkey = t_key\n",
    "            \n",
    "            loss_history_key = self.models[str(t_key)].fit(train_dataloader, loss_function, optimizer_type, lr, regularization_dict[str(t_key)], n_epochs)\n",
    "\n",
    "            self.loss_history[str(t_key)] = loss_history_key\n",
    "    \n",
    "    def train_collective(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #for every loop through target keys, we predict the corresponding block and assemble the final hamiltonian\n",
    "        optimizer_dict = {}\n",
    "        if optimizer_type == \"Adam\":\n",
    "            for key in train_dataloader.dataset.target.keys:\n",
    "                optimizer_dict[0] = torch.optim.Adam(self.models[str(key)].parameters(), lr = lr, weight_decay = regularization_dict[str(key)])\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 200\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "#             for key in train_dataloader.dataset.target.keys:\n",
    "#                 optimizer_dict[str(key)] = torch.optim.LBFGS(self.models[str(key)].parameters(), lr = lr)\n",
    "            optimizer_dict[0] = torch.optim.LBFGS(self.models.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 30\n",
    "            tol = 0\n",
    "            history_step = 10                \n",
    "        scheduler_dict = {}\n",
    "        scheduler_dict[0] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_dict[0], factor = 0.1, patience = scheduler_threshold)\n",
    "#         for key in train_dataloader.dataset.target.keys:\n",
    "#             scheduler_dict[str(key)] = torch.optim.lr_scheduler.StepLR(optimizer_dict[str(key)], scheduler_threshold, gamma = 0.5)\n",
    "\n",
    "        reg_weights = torch.tensor(list(regularization_dict.values()))\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            train_dataloader.dataset.currentkey = None\n",
    "            \n",
    "            for x_data, y_data, structure in train_dataloader: \n",
    "                self.collective_zg(optimizer_dict)\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.collective_zg(optimizer_dict)\n",
    "                        _pred = self.forward(x_data)\n",
    "                        _pred_loss = loss_function(_pred, y_data, structure, orbs)       \n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                          \n",
    "                        _reg_loss = self.get_regression_values(reg_weights) #Only works for 1 layer #Need to change!!\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    for value in optimizer_dict.values():\n",
    "                        value.step(closure)\n",
    "#                     for param in self.parameters():\n",
    "#                         print (param.grad)\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data, structure, orbs)  \n",
    "#                     reg_loss = torch.sum(torch.pow(self.nn.weight,2))#Only works for 1 layer\n",
    "                    new_loss = pred_loss \n",
    "                    new_loss.backward()\n",
    "                    self.collective_step(optimizer_dict)\n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in train_dataloader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data, structure, orbs)   #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg_weights)#Only works for 1 layer\n",
    "                new_loss = pred_loss + reg_loss\n",
    "                for scheduler in scheduler_dict.values():\n",
    "                    scheduler.step(new_loss)\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = regularization_dict[str(key)])\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training             \n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "        \n",
    "    def collective_step(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.step()\n",
    "            \n",
    "    def collective_zg(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.zero_grad()\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f59845d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:30:42.198252Z",
     "start_time": "2023-02-07T11:30:42.184392Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlockModel(torch.nn.Module): #Currently only 1 model per block\n",
    "    def __init__(self, reconstruction_function, inputSize, outputSize, device, keys, target_key, seed = None, hiddenSize = None):\n",
    "        super().__init__()\n",
    "        self.reconstruction_function = reconstruction_function\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.device = device\n",
    "        self.keys = keys\n",
    "        self.target_key = target_key\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.initialize_model(seed)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        for index, key in enumerate(self.keys):\n",
    "            if key['spherical_harmonics_l'] == 0:\n",
    "#                 self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = False)\n",
    "\n",
    "                self.models[str(key)] = InvariantNonLinearModel(self.inputSize[index], self.hiddenSize, self.outputSize[index])\n",
    "            else: \n",
    "                self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = False)\n",
    "        \n",
    "    def forward(self, feature_tensormap):\n",
    "        #Block model uses feature keys\n",
    "        pred_values = {}\n",
    "        for key in self.keys:\n",
    "            #print(key)\n",
    "            feature_values = feature_tensormap.block(key).values\n",
    "            d1, d2, d3 = feature_values.shape\n",
    "            L = int((d2 -1)/2)\n",
    "            \n",
    "            pred = self.models[str(key)](torch.tensor(feature_values.reshape(d1 * d2, d3)))\n",
    "            pred = pred.reshape(d1,d2)\n",
    "            pred_values[L] = pred\n",
    "        pred_block_values = self.reconstruction_function({(self.target_key['l_i'],self.target_key['l_j']) : pred_values})\n",
    "        \n",
    "        #DOES NOT WORK FOR BATCHES\n",
    "\n",
    "        #pred = torch.hstack(pred_values)\n",
    "        #pred = self.reconstruction_function(pred_values)\n",
    "        return pred_block_values \n",
    "        #return pred_values\n",
    "\n",
    "    \n",
    "    def fit(self,traindata_loader, loss_function, optimizer_type, lr, reg, n_epochs):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 50\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "            optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 10\n",
    "            tol = 0\n",
    "            history_step = 10\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = scheduler_threshold)      \n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            \n",
    "            for x_data, y_data, structure in traindata_loader: \n",
    "                optimizer.zero_grad()\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        _pred = self.forward(x_data)                                        \n",
    "                        _pred_loss = loss_function(_pred, y_data.values)\n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                 \n",
    "                        _reg_loss = self.get_regression_values(reg.item()) #Only works for 1 layer\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    optimizer.step(closure)\n",
    "\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data.values)\n",
    "                    #reg_loss = self.get_regression_values(reg.item())#Only works for 1 layer\n",
    "                    new_loss = pred_loss #+ reg_loss\n",
    "                    new_loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in traindata_loader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data.values) #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg.item()) \n",
    "                new_loss = pred_loss + reg_loss\n",
    "                scheduler.step(new_loss)\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84bf30ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NonLinearModel(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_size: int,\n",
    "#         hidden_size: int,\n",
    "#         last_hidden_size: int,\n",
    "#         output_size: int,\n",
    "#     ):\n",
    "#         \"\"\" \"\"\"\n",
    "#         super().__init__()\n",
    "#         self.input_layer = torch.nn.Linear(input_size, last_hidden_size)\n",
    "        \n",
    "#         self.invariant_nn = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(input_size, hidden_size),\n",
    "#             torch.nn.Tanh(),\n",
    "#             torch.nn.Linear(hidden_size, hidden_size),\n",
    "#             torch.nn.Tanh(),\n",
    "#             torch.nn.Linear(hidden_size, last_hidden_size),\n",
    "#         )\n",
    "#         # Define the output layer that makes the prediction\n",
    "#         self.output_layer = torch.nn.Linear(last_hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, input: torch.Tensor, invariant: torch.Tensor):\n",
    "#         # H-stack the invariant along the components dimension so that there are\n",
    "#         # (2 \\lambda + 1) copies and the dimensions match the equivariant\n",
    "#         invariant = torch.hstack([invariant] * input.values.shape[1])\n",
    "\n",
    "#         # Pass the invariant tensor through the NN to create a nonlinear\n",
    "#         # multiplier. Also pass the equivariant through a linear input layer.\n",
    "#         nonlinear_multiplier = self.invariant_nn(invariant)\n",
    "#         linear_input = self.input_layer(input)\n",
    "\n",
    "#         # Perform element-wise (Hadamard) multiplication of the transformed\n",
    "#         # input with the nonlinear multiplier, which now have the same dimensions\n",
    "#         nonlinear_input = torch.mul(linear_input, nonlinear_multiplier)\n",
    "\n",
    "#         # Finally pass through the output layer and return\n",
    "#         return self.output_layer(nonlinear_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ba82c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantNonLinearModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "    ):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "                \n",
    "        self.invariant_nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.Softmax(),\n",
    "            #torch.nn.Linear(hidden_size, hidden_size),\n",
    "            #torch.nn.Softmax(),\n",
    "            #torch.nn.Linear(hidden_size, hidden_size),\n",
    "            #torch.nn.Tanh(),\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        # Define the output layer that makes the prediction\n",
    "        #self.output_layer = torch.nn.Linear(last_hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        result = self.invariant_nn(input)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a88f0b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3049caf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T12:31:58.604640Z",
     "start_time": "2023-02-07T12:31:58.590585Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1]) \n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return MSE*10**7\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    #fock = torch.tensor(focks)\n",
    "    #print (mse_full_blockwise(pred_blocks, blocks, frame, orbs))\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)/torch.numel(fock[i])\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2#, mse_loss\n",
    "\n",
    "def mse_full_blockwise(pred_blocks, block_tensormap, frame, orbs):\n",
    "    indiv_mse = torch.zeros(1)\n",
    "    for key,block in pred_blocks:\n",
    "        #MSE = ((torch.linalg.norm(block_tensormap.block(key).values-block.values))**2)\n",
    "        print (key)\n",
    "        print (((torch.linalg.norm(block_tensormap.block(key).values- block.values))**2)/ torch.numel(block_tensormap.block(key).values))\n",
    "        print (torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values))\n",
    "        MSE = torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values)\n",
    "        indiv_mse += MSE\n",
    "    \n",
    "    return (indiv_mse/len(frame))*(Hartree)**2, indiv_mse\n",
    "    \n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f938144",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HamModel(train_dataset, \"cpu\", None, None, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b0ca74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS(\n",
    "        model.parameters(),\n",
    "        lr=1,  line_search_fn=\"strong_wolfe\",\n",
    "        history_size=256, tolerance_grad=1e-12, tolerance_change=1e-12\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5263af4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 55.5356490575776\n",
      "1 2.2535158907136936\n",
      "2 1.9193001349494911\n",
      "3 1.1616664058699668\n",
      "4 0.7913797397891894\n",
      "5 0.7776851541994158\n",
      "6 0.7637541226173614\n",
      "7 0.7425411385161687\n",
      "8 0.6916919411258343\n",
      "9 0.628322635896238\n",
      "10 0.5202032879056664\n",
      "11 0.42138410016019495\n",
      "12 0.3519357216236843\n",
      "13 0.3180832763038013\n",
      "14 0.31802370550769854\n",
      "15 0.3177493243829828\n",
      "16 0.3175384599524493\n",
      "17 0.3170098493519389\n",
      "18 0.3160637574946855\n",
      "19 0.3150284089473091\n",
      "20 0.31413487075004054\n",
      "21 0.31182756506280335\n",
      "22 0.3111409548983539\n",
      "23 0.3102014282052689\n",
      "24 0.3081065934813937\n",
      "25 0.3073852822820888\n",
      "26 0.3068535985152003\n",
      "27 0.3055172578309105\n",
      "28 0.3036760690984036\n",
      "29 0.3028633443125968\n",
      "30 0.30246516731134043\n",
      "31 0.3017674525434194\n",
      "32 0.3002563203483206\n",
      "33 0.2971525960387195\n",
      "34 0.2937440038294316\n",
      "35 0.29053717042166954\n",
      "36 0.286113593118678\n",
      "37 0.2818007871302119\n",
      "38 0.278130040955328\n",
      "39 0.27364210065105843\n",
      "40 0.26928656583657434\n",
      "41 0.2652128418047148\n",
      "42 0.2609029454593501\n",
      "43 0.25682269429345783\n",
      "44 0.2530241625955775\n",
      "45 0.24845105803174727\n",
      "46 0.24407821369479565\n",
      "47 0.240092943861649\n",
      "48 0.23520358649825887\n",
      "49 0.23009443447896752\n",
      "50 0.22498672748063112\n",
      "51 0.22008391141099867\n",
      "52 0.21513760956189268\n",
      "53 0.20966576300429338\n",
      "54 0.20350692790584637\n",
      "55 0.1980069496814323\n",
      "56 0.19130180658177806\n",
      "57 0.18499473983028572\n",
      "58 0.17966998277722027\n",
      "59 0.1717648154886191\n",
      "60 0.16522574624411052\n",
      "61 0.158142089721629\n",
      "62 0.15203430994934375\n",
      "63 0.14711467382585294\n",
      "64 0.14228941073737705\n",
      "65 0.13749936947046487\n",
      "66 0.13437863412789136\n",
      "67 0.13086436554627243\n",
      "68 0.12908476698113197\n",
      "69 0.12699525498517628\n",
      "70 0.12562516135707685\n",
      "71 0.12479269597191857\n",
      "72 0.12433410438644303\n",
      "73 0.12425454944849613\n",
      "74 0.12424099711764273\n",
      "75 0.12424032249041794\n",
      "76 0.12423914816005237\n",
      "77 0.12423768228047333\n",
      "78 0.12423610412538642\n",
      "79 0.1242352206417632\n",
      "80 0.12423452397393418\n",
      "81 0.12423341759532318\n",
      "82 0.12423201077308767\n",
      "83 0.12423068623455795\n",
      "84 0.12422912861427575\n",
      "85 0.12422799416447086\n",
      "86 0.12422644838245299\n",
      "87 0.12422448099082148\n",
      "88 0.12422255823265752\n",
      "89 0.1242204813526603\n",
      "90 0.12421769519963435\n",
      "91 0.12421485217991356\n",
      "92 0.124211712771393\n",
      "93 0.12420894530156995\n",
      "94 0.12420575833171638\n",
      "95 0.12420277767162152\n",
      "96 0.12419988443157924\n",
      "97 0.12419678899159832\n",
      "98 0.12419356893026688\n",
      "99 0.12419028786206934\n",
      "100 0.12418779262842647\n",
      "101 0.12418531357828286\n",
      "102 0.12418277609564672\n",
      "103 0.12418025789289813\n",
      "104 0.12417791528328552\n",
      "105 0.1241761153840953\n",
      "106 0.12417420311794347\n",
      "107 0.12417240197082531\n",
      "108 0.12417013944731527\n",
      "109 0.1241675194109597\n",
      "110 0.12416554438069068\n",
      "111 0.12416369836495951\n",
      "112 0.12416137766630338\n",
      "113 0.12415908440835127\n",
      "114 0.1241571430565825\n",
      "115 0.12415532209072967\n",
      "116 0.12415296484340971\n",
      "117 0.12414991454858139\n",
      "118 0.12414696317850475\n",
      "119 0.12414388668904781\n",
      "120 0.12414149305002473\n",
      "121 0.1241391850163065\n",
      "122 0.12413584244880638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m all_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/lbfgs.py:389\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old):\n\u001b[1;32m    388\u001b[0m         be_i \u001b[38;5;241m=\u001b[39m old_dirs[i]\u001b[38;5;241m.\u001b[39mdot(r) \u001b[38;5;241m*\u001b[39m ro[i]\n\u001b[0;32m--> 389\u001b[0m         \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_stps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mal\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbe_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_flat_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     prev_flat_grad \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#doesn't go to zero even for a single structure\n",
    "#problem with the model probably\n",
    "all_losses = []\n",
    "for epoch in range(300):\n",
    "    dataloader.dataset.currentkey = None\n",
    "    for x_data, y_data, structure in dataloader:    \n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_data)\n",
    "            loss = mse_full(pred, y_data, structure, orbs)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(single_step)\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31ec5b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "step() missing 1 required positional argument: 'closure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_full(pred, y_data, structure, orbs)\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 9\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#scheduler.step()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m all_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: step() missing 1 required positional argument: 'closure'"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "for epoch in range(5000):\n",
    "    dataloader.dataset.currentkey = None\n",
    "    for x_data, y_data, structure in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_data)\n",
    "        loss = mse_full(pred, y_data, structure, orbs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch, loss.item()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5122692",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb10f943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:53.043094Z",
     "start_time": "2022-12-29T10:15:52.986380Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load test set\n",
    "test_frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\"1:2\")\n",
    "# frames2 = ase.io.read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "test_frames = test_frames1 #+ frames2\n",
    "for f in test_frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "\n",
    "# test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:60]\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "\n",
    "test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:80]\n",
    "test_focks = test_focks1\n",
    "# test_focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[50:80]\n",
    "# test_overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[50:80]\n",
    "\n",
    "# test_orthogonal = []\n",
    "# for i in range(len(test_focks)): \n",
    "#     test_focks[i] = fix_pyscf_l1(test_focks[i],test_frames[i], orbs)\n",
    "#     test_overlap[i] = fix_pyscf_l1(test_overlap[i],test_frames[i], orbs)\n",
    "#     test_orthogonal.append(lowdin_orthogonalize(test_focks[i], test_overlap[i]))\n",
    "# test_focks = np.asarray(test_orthogonal, dtype=np.float64)\n",
    "    \n",
    "test_blocks = dense_to_blocks(test_focks, test_frames, orbs)\n",
    "test_fock_bc = couple_blocks(test_blocks, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "515d2062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:54.725559Z",
     "start_time": "2022-12-29T10:15:53.932535Z"
    }
   },
   "outputs": [],
   "source": [
    "test_rhoi = spex.compute(test_frames)\n",
    "test_gij = pairs.compute(test_frames)\n",
    "test_rho1i = acdc_standardize_keys(test_rhoi)\n",
    "test_rho1i = test_rho1i.keys_to_properties(['species_neighbor'])\n",
    "test_gij =  acdc_standardize_keys(test_gij)\n",
    "test_rho2i = cg_increment(test_rho1i, test_rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "test_rho1ij = cg_increment(test_rho1i, test_gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "test_features = hamiltonian_features(test_rho2i, test_rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "695e248b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.390807Z",
     "start_time": "2022-12-29T10:15:54.727705Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import save\n",
    "save(\"test_feature.npz\", test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "514168bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.398022Z",
     "start_time": "2022-12-29T10:15:55.394196Z"
    }
   },
   "outputs": [],
   "source": [
    "# norm_test_feat = normalize_feats(test_features)\n",
    "# from equistore.io import save\n",
    "# save(\"./norm_testfeat.npz\", norm_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d4e81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T16:26:03.738575Z",
     "start_time": "2022-12-28T16:26:03.164926Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5ee3635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.442916Z",
     "start_time": "2022-12-29T10:15:55.402080Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_target_path = \"./test_fock.npz\"\n",
    "test_feature_path = \"./test_feature.npz\"\n",
    "testing = HamiltonianDataset(test_feature_path, test_blocks, test_focks, test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e979a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.726557Z",
     "start_time": "2022-12-29T10:15:55.722442Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "test_Sampler = torch.utils.data.sampler.RandomSampler(testing)\n",
    "test_BSampler = torch.utils.data.sampler.BatchSampler(test_Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "test_dataloader = DataLoader(testing, sampler = test_BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "1091e288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:25:09.307646Z",
     "start_time": "2022-12-29T10:25:09.295845Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1])\n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return MSE\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    #fock = torch.tensor(focks)\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)#/torch.numel(fock[i])\n",
    "\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2\n",
    "\n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    #fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31e5f4b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:29:34.344107Z",
     "start_time": "2022-12-29T10:29:34.230337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5713, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testing.currentkey = None\n",
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in test_dataloader:\n",
    "    t_pred = model(x_data)\n",
    "    print (mse_full(t_pred, torch.tensor(test_focks), structures, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45786c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "786px",
    "left": "27px",
    "top": "111.133px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
