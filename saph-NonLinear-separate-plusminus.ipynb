{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5c7c0",
   "metadata": {
    "hide_input": true,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Manipulate-Hamiltonian-into-blocks\" data-toc-modified-id=\"Manipulate-Hamiltonian-into-blocks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Manipulate Hamiltonian into blocks</a></span></li><li><span><a href=\"#Feature-computation\" data-toc-modified-id=\"Feature-computation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature computation</a></span></li><li><span><a href=\"#Feature-Preprocessing\" data-toc-modified-id=\"Feature-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Preprocessing</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#DataLoader\" data-toc-modified-id=\"DataLoader-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DataLoader</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Tests</a></span><ul class=\"toc-item\"><li><span><a href=\"#set-up-wigner-d-rotations-matrices\" data-toc-modified-id=\"set-up-wigner-d-rotations-matrices-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>set up wigner-d rotations matrices</a></span></li><li><span><a href=\"#block-wise-rotations\" data-toc-modified-id=\"block-wise-rotations-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>block wise rotations</a></span></li><li><span><a href=\"#Rotate-matrix\" data-toc-modified-id=\"Rotate-matrix-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Rotate matrix</a></span></li><li><span><a href=\"#Eigenvalue-tests\" data-toc-modified-id=\"Eigenvalue-tests-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Eigenvalue tests</a></span></li><li><span><a href=\"#check-decoupling-of-blocks\" data-toc-modified-id=\"check-decoupling-of-blocks-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>check decoupling of blocks</a></span></li><li><span><a href=\"#check-feature-rotations\" data-toc-modified-id=\"check-feature-rotations-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>check feature rotations</a></span></li></ul></li><li><span><a href=\"#need-to-modify-samples-for-features-to-start-from-1\" data-toc-modified-id=\"need-to-modify-samples-for-features-to-start-from-1-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>need to modify samples for features to start from 1</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3096d21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:32:48.888548Z",
     "start_time": "2023-02-06T17:32:48.885877Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2bb576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.292787Z",
     "start_time": "2023-02-06T19:35:54.812257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ase.io\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "# from rascal.representations import SphericalExpansion\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from ase.units import Hartree\n",
    "\n",
    "from torch_hamiltonian_utils.torch_cg import ClebschGordanReal\n",
    "from torch_hamiltonian_utils.torch_ham import fix_pyscf_l1, dense_to_blocks, blocks_to_dense, couple_blocks, decouple_blocks, hamiltonian_features\n",
    "from torch_hamiltonian_utils.torch_builder import TensorBuilder\n",
    "\n",
    "import equistore\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "from equistore_utils.librascal import  RascalSphericalExpansion, RascalPairExpansion\n",
    "from equistore_utils.acdc_mini import acdc_standardize_keys, cg_increment, cg_combine\n",
    "from equistore_utils.model_hamiltonian import get_feat_keys, get_feat_keys_from_uncoupled \n",
    "\n",
    "import importlib\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68aeb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.301657Z",
     "start_time": "2023-02-06T19:36:04.294480Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feat_keys_from_uncoupled(block_keys, sigma=None, order_nu=None):\n",
    "    \"\"\"Map UNCOUPLED block keys to corresponding feature key. take as extra input the sigma, nu value if required.\n",
    "    sigma=0 returns all possible sigma values at given 'nu'\"\"\"\n",
    "    blocktype, species1, n1, l1, species2, n2, l2 = block_keys\n",
    "    feat_blocktype = blocktype\n",
    "    keys_L=[]\n",
    "    for L in range(abs(l1-l2), l1+l2+1):\n",
    "        if sigma is None:\n",
    "            z = (l1+l2+L)%2\n",
    "            inv_sigma = 1 - 2*z\n",
    "        elif abs(sigma)==1:\n",
    "            inv_sigma = sigma\n",
    "        else: \n",
    "            raise(\"Please check sigma value, it should be +1 or -1\")\n",
    "        \n",
    "        if blocktype == 1 and n1 == n2 and l1 == l2:\n",
    "            feat_blocktype = inv_sigma\n",
    "                 \n",
    "        if inv_sigma == -1 and blocktype == 0 and n1 == n2 and l1 == l2:\n",
    "            continue     \n",
    "        \n",
    "        keys_L.append([(order_nu, inv_sigma, L, species1, species2, feat_blocktype)])\n",
    "    #     feat= (blocktype, L,sigma,species1, species2)\n",
    "    feat = Labels([\"order_nu\", \"inversion_sigma\", \"spherical_harmonics_l\", \"species_center\", \"species_neighbor\", \"block_type\"], np.asarray(keys_L, dtype=np.int32).reshape(-1,6))\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3361a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.331902Z",
     "start_time": "2023-02-06T19:36:04.303851Z"
    }
   },
   "outputs": [],
   "source": [
    "# frames = ase.io.read(\"data/water_rotated/water_rotated_3.xyz\",\":\")\n",
    "# for f in frames:\n",
    "#     f.cell = [100,100,100]\n",
    "#     f.positions += 50\n",
    "    \n",
    "# jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "# orbs = {}\n",
    "# zdic = {\"O\" : 8, \"H\":1}\n",
    "# for k in jorbs:\n",
    "#     orbs[zdic[k]] = jorbs[k]\n",
    "# focks = np.load(\"data/water_rotated/water_rotated_saph_3.npy\", allow_pickle=True)[:len(frames)]\n",
    "# rotations = np.load(\"data/water_rotated/rotations_3.npy\", allow_pickle = True)\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10faaf55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.360121Z",
     "start_time": "2023-02-06T19:36:04.334928Z"
    }
   },
   "outputs": [],
   "source": [
    "def lowdin_orthogonalize(fock, s):\n",
    "    \"\"\"\n",
    "    lowdin orthogonalization of a fock matrix computing the square root of the overlap matrix\n",
    "    \"\"\"\n",
    "    eva, eve = np.linalg.eigh(s)\n",
    "    sm12 = eve @ np.diag(1.0/np.sqrt(eva)) @ eve.T\n",
    "    return sm12 @ fock @ sm12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b86aa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.644913Z",
     "start_time": "2023-02-06T19:36:04.361638Z"
    }
   },
   "outputs": [],
   "source": [
    "frames = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\":50\")\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1, \"C\":6}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]\n",
    "\n",
    "focks = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames)]\n",
    "focks = focks.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246e69de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.649076Z",
     "start_time": "2023-02-06T19:36:04.646607Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# #generating jagged matrix\n",
    "# f=[]\n",
    "# for x in focks:\n",
    "#     f.append(x)\n",
    "# for x in focks2:\n",
    "#     f.append(x)\n",
    "    \n",
    "# jagged = np.asanyarray(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc18008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.359435Z",
     "start_time": "2023-02-06T19:36:04.651072Z"
    }
   },
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad712",
   "metadata": {},
   "source": [
    "## Manipulate Hamiltonian into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461e38a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.464335Z",
     "start_time": "2023-02-06T19:36:05.362511Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8cfe3",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13064b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.561138Z",
     "start_time": "2023-02-06T19:36:05.487376Z"
    }
   },
   "outputs": [],
   "source": [
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 4.0,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 4,\n",
    "    \"gaussian_sigma_constant\" : 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\":  False,\n",
    "}\n",
    "\n",
    "spex = RascalSphericalExpansion(rascal_hypers)\n",
    "rhoi = spex.compute(frames)\n",
    "\n",
    "lmax = rascal_hypers[\"max_angular\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc39e55f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.672601Z",
     "start_time": "2023-02-06T19:36:05.562322Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = RascalPairExpansion(rascal_hypers)\n",
    "gij = pairs.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf74c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.714916Z",
     "start_time": "2023-02-06T19:36:05.674433Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1i = acdc_standardize_keys(rhoi)\n",
    "rho1i = rho1i.keys_to_properties(['species_neighbor'])\n",
    "gij =  acdc_standardize_keys(gij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a666711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:06.296929Z",
     "start_time": "2023-02-06T19:36:05.716462Z"
    }
   },
   "outputs": [],
   "source": [
    "rho2i = cg_increment(rho1i, rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4565f04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:06.946140Z",
     "start_time": "2023-02-06T19:36:06.942526Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho3i = cg_increment(rho2i, rho1i, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdcb6035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.120033Z",
     "start_time": "2023-02-06T19:36:07.441462Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1ij = cg_increment(rho1i, gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c5d9d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.124429Z",
     "start_time": "2023-02-06T19:36:08.121919Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho2ij = cg_increment(rho2i, gij, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b29eb700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.286694Z",
     "start_time": "2023-02-06T19:36:08.126318Z"
    }
   },
   "outputs": [],
   "source": [
    "features = hamiltonian_features(rho2i, rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eeb7fa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.293007Z",
     "start_time": "2023-02-06T19:36:08.289110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 45 blocks\n",
       "keys: ['order_nu' 'inversion_sigma' 'spherical_harmonics_l' 'species_center' 'species_neighbor' 'block_type']\n",
       "           2             1                    0                   1                1              0\n",
       "           2             1                    1                   1                1              0\n",
       "           2             1                    2                   1                1              0\n",
       "        ...\n",
       "           2            -1                    4                   1                1              1\n",
       "           2            -1                    4                   1                1             -1\n",
       "           2            -1                    4                   1                8              2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7bf0c77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.566117Z",
     "start_time": "2023-02-06T19:36:08.482335Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import save\n",
    "save(\"feature.npz\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c7da8",
   "metadata": {},
   "source": [
    "## Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08174c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.576264Z",
     "start_time": "2023-02-06T19:36:09.568334Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_feats(feat, all_blocks=True): \n",
    "    all_norm = 0\n",
    "    normalized_blocks=[]\n",
    "    for block_idx, block in feat: \n",
    "        block_norm = np.linalg.norm(block.values)\n",
    "#         print(block_idx, block_norm)\n",
    "        all_norm = block_norm**2 * len(block.samples) \n",
    "    \n",
    "        newblock = TensorBlock(\n",
    "                        values=block.values/np.sqrt(all_norm ),\n",
    "                        samples=block.samples,\n",
    "                        components=block.components,\n",
    "                        properties= block.properties)                    \n",
    "        normalized_blocks.append(newblock) \n",
    "        \n",
    "    norm_feat = TensorMap(feat.keys, normalized_blocks)\n",
    "    raise Exception (\"Dont do it!\")\n",
    "    return norm_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "279a28b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.625063Z",
     "start_time": "2023-02-06T19:36:09.577966Z"
    }
   },
   "outputs": [],
   "source": [
    "#norm_feat = normalize_feats(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e55b753b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.674273Z",
     "start_time": "2023-02-06T19:36:09.627414Z"
    }
   },
   "outputs": [],
   "source": [
    "# from equistore.io import save\n",
    "# save(\"./norm_feat.npz\", norm_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27106a59",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4ffec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:47:24.977413Z",
     "start_time": "2023-02-07T10:47:24.965586Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import _labels_from_npz\n",
    "import equistore.operations as operations\n",
    "\n",
    "class HamiltonianDataset(torch.utils.data.Dataset):\n",
    "    #Dataset class\n",
    "    def __init__(self, feature_path, target, fock, frames, feature_nu = 2):\n",
    "        #\n",
    "        self.features = np.load(feature_path, mmap_mode = 'r')\n",
    "        #self.target = np.load(target_path, mmap_mode = 'r') \n",
    "        self.target = target #Uncoupled hamiltonian \n",
    "        #self.target = np.load(fock_path, mmap_mode = 'r')\n",
    "        self.fock = torch.tensor(fock) #fock matrix\n",
    "        self.keys_features = equistore.io._labels_from_npz(self.features[\"keys\"])\n",
    "        self.currentkey = self.target.keys[0]\n",
    "        self.feature_nu = feature_nu\n",
    "        self.frames = frames\n",
    "        \n",
    "        self.allfeatkey = []\n",
    "        for t_key in self.target.keys:\n",
    "            feature_key = self.get_feature_keys(t_key)\n",
    "            self.allfeatkey.append(feature_key)\n",
    "        #Remove Duplicates\n",
    "        nodupes = set()\n",
    "        for x in self.allfeatkey:\n",
    "            if len(x) > 1:\n",
    "                for z in x:\n",
    "                    nodupes.add(tuple(z))\n",
    "            else:\n",
    "                nodupes.add(tuple(x[0]))\n",
    "        \n",
    "        nodupes = np.array(list(nodupes), np.int32)\n",
    "        \n",
    "        self.allfeatkey = Labels(names = self.allfeatkey[0].dtype.names, values = nodupes)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, structure_idx):\n",
    "        feature_block, feature_key = self.generate_feature_block(self.features, structure_idx)        \n",
    "        #samples_filter, target_block_samples = self.get_index_from_idx(self.target.block(self.currentkey).samples, structure_idx)\n",
    "\n",
    "        if self.currentkey is None:\n",
    "            target_block = self.fock[sorted(structure_idx)]\n",
    "        else:\n",
    "            target_block = operations.slice_block(self.target.block(self.currentkey), \n",
    "                                                  samples = Labels(names = ['structure'], \n",
    "                                                    values = (np.array(structure_idx)+1).reshape(-1,1)) )\n",
    "        structure = [self.frames[i] for i in sorted(structure_idx)]\n",
    "        #Modify feature_block to tensormap\n",
    "        feature_map = TensorMap(feature_key, feature_block)\n",
    "        return feature_map, target_block, structure\n",
    "\n",
    "\n",
    "    def get_feature_keys(self,uncoupled_key):\n",
    "        return get_feat_keys_from_uncoupled(uncoupled_key, order_nu = self.feature_nu)\n",
    "    \n",
    "    def generate_feature_block(self, memmap, structure_idx):\n",
    "        #Generate the block from npz file\n",
    "        output = []\n",
    "        if self.currentkey is None:\n",
    "            feature_key = self.allfeatkey\n",
    "                \n",
    "        else:\n",
    "            feature_key = self.get_feature_keys(self.currentkey)\n",
    "            \n",
    "        for key in feature_key:\n",
    "            block_index = list(self.keys_features).index(key)\n",
    "            prefix = f\"blocks/{block_index}/values\"        \n",
    "            block_samples = equistore.io._labels_from_npz(memmap[f\"{prefix}/samples\"])\n",
    "            block_components = []\n",
    "            for i in range(1):\n",
    "                block_components.append(equistore.io._labels_from_npz(memmap[f\"{prefix}/components/{i}\"]))\n",
    "            block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "             \n",
    "\n",
    "            samples_filter, block_samples = self.get_index_from_idx(block_samples, structure_idx)\n",
    "            \n",
    "\n",
    "            block_data = memmap[f\"{prefix}/data\"][samples_filter]\n",
    "            block = TensorBlock(block_data, block_samples, block_components, block_properties)\n",
    "            output.append(block)\n",
    "        return output, feature_key\n",
    "    \n",
    "    def get_n_properties(self, memmap, key):\n",
    "        block_index = list(self.keys_features).index(key)\n",
    "        prefix = f\"blocks/{block_index}/values\"  \n",
    "        block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "        \n",
    "        return len(block_properties)\n",
    "    \n",
    "    def get_index_from_idx(self, block_samples, structure_idx):\n",
    "        #Get samples label from IDX\n",
    "        samples = Labels(names = ['structure'], values = np.array(structure_idx).reshape(-1,1))\n",
    "        \n",
    "        all_samples = block_samples[['structure']].tolist()\n",
    "        set_samples_to_slice = set(samples.tolist())\n",
    "        samples_filter = np.array(\n",
    "            [sample in set_samples_to_slice for sample in all_samples]\n",
    "        )\n",
    "        new_samples = block_samples[samples_filter]\n",
    "\n",
    "        return samples_filter, new_samples\n",
    "    \n",
    "    def collate_output_values(blocks):\n",
    "        feature_out = []\n",
    "        target_out = []\n",
    "        for sample_output in blocks:\n",
    "            feature_block, target_block, structure = sample_output\n",
    "            for z in feature_block:\n",
    "                feature_out.append(torch.tensor(z.values))\n",
    "            target_out.append(torch.tensor(target_block.values))\n",
    "\n",
    "        return feature_out, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ac1d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:47:25.034221Z",
     "start_time": "2023-02-07T10:47:24.979379Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_target_path = \"./test_fock.npz\"\n",
    "test_feature_path = \"./feature.npz\"\n",
    "test = HamiltonianDataset(test_feature_path, blocks, focks, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ffe2cd",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c336070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:01:55.381711Z",
     "start_time": "2023-02-07T11:01:55.378061Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_blocks(block_tuple):\n",
    "    feature_tensor_map, target_block, structure_array = block_tuple[0]\n",
    "    \n",
    "    return feature_tensor_map, target_block, structure_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c4dd925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T12:31:03.632080Z",
     "start_time": "2023-02-07T12:31:03.621532Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "Sampler = torch.utils.data.sampler.RandomSampler(test)\n",
    "#Sampler = torch.utils.data.sampler.SequentialSampler(test)\n",
    "BSampler = torch.utils.data.sampler.BatchSampler(Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "dataloader = DataLoader(test, sampler = BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704bdf2",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79125947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:14:38.877275Z",
     "start_time": "2023-02-07T11:14:38.874272Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def get_block_samples(t_key, feature_map):\n",
    "    f_key = get_feat_keys_from_uncoupled(t_key, None , 2)\n",
    "    ss = feature_map.block(f_key[0]).samples.copy()\n",
    "    #ss[\"structure\"] = rankdata(np.abs(ss[\"structure\"]), method='dense')\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee997771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:58:14.016060Z",
     "start_time": "2023-02-07T10:58:13.979757Z"
    }
   },
   "outputs": [],
   "source": [
    "class HamModel(torch.nn.Module):\n",
    "    #Handles prediction of entire hamiltonian and derived results\n",
    "    def __init__(self, Hamiltonian_Dataset, device, regularization=None, seed=None, layer_size=None):\n",
    "        super().__init__()\n",
    "#         self.features = features \n",
    "#         self.target = target\n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        self.loss_history={}\n",
    "        self.device = device\n",
    "        self.target_keys = Hamiltonian_Dataset.target.keys\n",
    "        self.block_samples = {}\n",
    "        self.block_components = {}\n",
    "        for key in Hamiltonian_Dataset.target.keys:\n",
    "#             _block_type, _a_i, _n_i, _l_i, _a_j, _n_j, _l_j = key\n",
    "#             target_keys = Hamiltonian_Dataset.target.keys[Hamiltonian_Dataset.target.blocks_matching(\n",
    "#                 block_type = _block_type, a_i = _a_i, n_i = _n_i, l_i = _l_i, a_j = _a_j,\n",
    "#                 n_j = _n_j, l_j = _l_j)]\n",
    "            \n",
    "            #self.block_samples[str(key)] = Hamiltonian_Dataset.target.block(key).samples\n",
    "            self.block_components[str(key)] = Hamiltonian_Dataset.target.block(key).components\n",
    "        \n",
    "    \n",
    "            n_inputs = []\n",
    "            model_keys = []\n",
    "\n",
    "            feature_keys = Hamiltonian_Dataset.get_feature_keys(key)\n",
    "            for f_key in feature_keys: \n",
    "                n_features = Hamiltonian_Dataset.get_n_properties(Hamiltonian_Dataset.features, f_key)\n",
    "                n_inputs.append(n_features)\n",
    "                model_keys.append(f_key)\n",
    "                \n",
    "                \n",
    "            n_outputs = np.ones_like(n_inputs)\n",
    "                \n",
    "            self.models[str(key)] = BlockModel(cg.decouple,n_inputs, n_outputs, device, model_keys, key, seed = seed, hiddenSize = layer_size)\n",
    "        self.to(device)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #Ham model uses target keys\n",
    "        pred_blocks = []\n",
    "        for t_key in self.target_keys:\n",
    "            \n",
    "            pred = self.models[str(t_key)].forward(x) #feature_tensormap must correspond to the correct features, model returns block\n",
    "            \n",
    "            #try:\n",
    "#             print (pred.shape)\n",
    "#             print ((2 * t_key['l_i'])+1)\n",
    "#             print ((2 * t_key['l_j']) + 1)\n",
    "            pred_block = TensorBlock(\n",
    "                    values=pred.reshape((-1, (2 * t_key['l_i'])+1, (2 * t_key['l_j']) + 1, 1)), #?\n",
    "                    samples = get_block_samples(t_key, x),\n",
    "#                     samples = x.block(t_key).samples,\n",
    "                    components = self.block_components[str(t_key)] ,\n",
    "                    properties= Labels([\"dummy\"], np.asarray([[0]], dtype=np.int32))\n",
    "                )\n",
    "#             except:\n",
    "#                 print (t_key)\n",
    "#                 print (pred)\n",
    "#                 print (self.block_samples[str(t_key)])\n",
    "#                 print (self.block_components[str(t_key)])\n",
    "                \n",
    "            pred_blocks.append(pred_block)\n",
    "        pred_hamiltonian = TensorMap(self.target_keys, pred_blocks)\n",
    "        return(pred_hamiltonian)\n",
    "    \n",
    "    #write/fix forward function for train_indiv\n",
    "    \n",
    "    def train_individual(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #Iterates through the keys of self.model, then for each key we will fit self.model[key] with data[key]\n",
    "        total = len(self.models)\n",
    "        for index, t_key in enumerate(self.target_keys):\n",
    "            print (\"Now training on Block {} of {}\".format(index, total))\n",
    "            train_dataloader.dataset.currentkey = t_key\n",
    "            \n",
    "            loss_history_key = self.models[str(t_key)].fit(train_dataloader, loss_function, optimizer_type, lr, regularization_dict[str(t_key)], n_epochs)\n",
    "\n",
    "            self.loss_history[str(t_key)] = loss_history_key\n",
    "    \n",
    "    def train_collective(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #for every loop through target keys, we predict the corresponding block and assemble the final hamiltonian\n",
    "        optimizer_dict = {}\n",
    "        if optimizer_type == \"Adam\":\n",
    "            for key in train_dataloader.dataset.target.keys:\n",
    "                optimizer_dict[str(key)] = torch.optim.Adam(self.models[str(key)].parameters(), lr = lr, weight_decay = regularization_dict[str(key)])\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 200\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "#             for key in train_dataloader.dataset.target.keys:\n",
    "#                 optimizer_dict[str(key)] = torch.optim.LBFGS(self.models[str(key)].parameters(), lr = lr)\n",
    "            optimizer_dict[0] = torch.optim.LBFGS(self.models.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 30\n",
    "            tol = 0\n",
    "            history_step = 10                \n",
    "        scheduler_dict = {}\n",
    "        scheduler_dict[0] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_dict[0], factor = 0.1, patience = scheduler_threshold)\n",
    "#         for key in train_dataloader.dataset.target.keys:\n",
    "#             scheduler_dict[str(key)] = torch.optim.lr_scheduler.StepLR(optimizer_dict[str(key)], scheduler_threshold, gamma = 0.5)\n",
    "\n",
    "        reg_weights = torch.tensor(list(regularization_dict.values()))\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            train_dataloader.dataset.currentkey = None\n",
    "            \n",
    "            for x_data, y_data, structure in train_dataloader: \n",
    "                self.collective_zg(optimizer_dict)\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.collective_zg(optimizer_dict)\n",
    "                        _pred = self.forward(x_data)\n",
    "                        _pred_loss = loss_function(_pred, y_data, structure, orbs)       \n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                          \n",
    "                        _reg_loss = self.get_regression_values(reg_weights) #Only works for 1 layer #Need to change!!\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    for value in optimizer_dict.values():\n",
    "                        value.step(closure)\n",
    "#                     for param in self.parameters():\n",
    "#                         print (param.grad)\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data, structure, orbs)  \n",
    "#                     reg_loss = torch.sum(torch.pow(self.nn.weight,2))#Only works for 1 layer\n",
    "                    new_loss = pred_loss \n",
    "                    new_loss.backward()\n",
    "                    self.collective_step(optimizer_dict)\n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in train_dataloader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data, structure, orbs)   #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg_weights)#Only works for 1 layer\n",
    "                new_loss = pred_loss + reg_loss\n",
    "                for scheduler in scheduler_dict.values():\n",
    "                    scheduler.step(new_loss)\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training             \n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "        \n",
    "    def collective_step(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.step()\n",
    "            \n",
    "    def collective_zg(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.zero_grad()\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f59845d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:30:42.198252Z",
     "start_time": "2023-02-07T11:30:42.184392Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlockModel(torch.nn.Module): #Currently only 1 model per block\n",
    "    def __init__(self, reconstruction_function, inputSize, outputSize, device, keys, target_key, seed = None, hiddenSize = None):\n",
    "        super().__init__()\n",
    "        self.reconstruction_function = reconstruction_function\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.device = device\n",
    "        self.keys = keys\n",
    "        self.target_key = target_key\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.initialize_model(seed)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        for index, key in enumerate(self.keys):\n",
    "             if key['spherical_harmonics_l'] == 0:\n",
    "                self.models[str(key)] = InvariantNonLinearModel(self.inputSize[index], self.hiddenSize, self.outputSize[index])\n",
    "#                 self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = True)\n",
    "             else:\n",
    "                self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = False)\n",
    "        \n",
    "    def forward(self, feature_tensormap):\n",
    "        #Block model uses feature keys\n",
    "        pred_values = {}\n",
    "        for key in self.keys:\n",
    "            feature_values = feature_tensormap.block(key).values\n",
    "            d1, d2, d3 = feature_values.shape\n",
    "            L = int((d2 -1)/2)\n",
    "            pred = self.models[str(key)](torch.tensor(feature_values.reshape(d1 * d2, d3)))\n",
    "            pred = pred.reshape(d1,d2)\n",
    "            pred_values[L] = pred\n",
    "        \n",
    "        pred_block_values = self.reconstruction_function({(self.target_key['l_i'],self.target_key['l_j']) : pred_values})\n",
    "        \n",
    "        #DOES NOT WORK FOR BATCHES\n",
    "\n",
    "        #pred = torch.hstack(pred_values)\n",
    "        #pred = self.reconstruction_function(pred_values)\n",
    "        return pred_block_values \n",
    "\n",
    "    \n",
    "    def fit(self,traindata_loader, loss_function, optimizer_type, lr, reg, n_epochs):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 50\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "            optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 10\n",
    "            tol = 0\n",
    "            history_step = 10\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = scheduler_threshold)      \n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            \n",
    "            for x_data, y_data, structure in traindata_loader: \n",
    "                optimizer.zero_grad()\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        _pred = self.forward(x_data)                                        \n",
    "                        _pred_loss = loss_function(_pred, y_data.values)\n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                 \n",
    "                        _reg_loss = self.get_regression_values(reg.item()) #Only works for 1 layer\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    optimizer.step(closure)\n",
    "\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data.values)\n",
    "                    #reg_loss = self.get_regression_values(reg.item())#Only works for 1 layer\n",
    "                    new_loss = pred_loss #+ reg_loss\n",
    "                    new_loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in traindata_loader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data.values) #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg.item()) \n",
    "                new_loss = pred_loss + reg_loss\n",
    "                scheduler.step(new_loss)\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8d4ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantNonLinearModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "    ):\n",
    "        \"\"\" \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.invariant_nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size, bias=False),\n",
    "            torch.nn.ELU(),\n",
    "#             torch.nn.Linear(hidden_size, hidden_size),\n",
    "#             torch.nn.Sigmoid(),\n",
    "#             torch.nn.Linear(hidden_size, hidden_size),\n",
    "#             torch.nn.GroupNorm(8,hidden_size),\n",
    "#             torch.nn.Softplus(),\n",
    "#             torch.nn.Linear(hidden_size, hidden_size),\n",
    "#             torch.nn.GroupNorm(8,hidden_size),\n",
    "#             torch.nn.Softplus(),\n",
    "#             torch.nn.GroupNorm(4, hidden_size),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(hidden_size, hidden_size),\n",
    "#             torch.nn.SiLU(),\n",
    "            torch.nn.Linear(hidden_size, output_size, bias=False)\n",
    "            )\n",
    "        \n",
    "        # Define the output layer that makes the prediction\n",
    "        #self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        result = self.invariant_nn(input)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a88f0b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3049caf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T12:31:58.604640Z",
     "start_time": "2023-02-07T12:31:58.590585Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1]) \n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return torch.mean(MSE)*(Hartree)**2\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    #fock = torch.tensor(focks)\n",
    "    #print (mse_full_blockwise(pred_blocks, blocks, frame, orbs))\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)/torch.numel(fock[i])\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2#, mse_loss\n",
    "\n",
    "def mse_full_blockwise(pred_blocks, block_tensormap, frame, orbs):\n",
    "    indiv_mse = torch.zeros(1)\n",
    "    for key,block in pred_blocks:\n",
    "        #MSE = ((torch.linalg.norm(block_tensormap.block(key).values-block.values))**2)\n",
    "        print (key)\n",
    "        print (((torch.linalg.norm(block_tensormap.block(key).values- block.values))**2)/ torch.numel(block_tensormap.block(key).values))\n",
    "        print (torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values))\n",
    "        MSE = torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values)\n",
    "        indiv_mse += MSE\n",
    "    \n",
    "    return (indiv_mse/len(frame))*(Hartree)**2, indiv_mse\n",
    "    \n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36b78810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HamModel(test, \"cpu\", None, 1234, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99b67050",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structure in dataloader:    \n",
    "    pred = model(x_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd7e146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS(\n",
    "        model.parameters(),\n",
    "        lr=1,  line_search_fn=\"strong_wolfe\",\n",
    "        history_size=256, tolerance_grad=1e-20, tolerance_change=1e-20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e818c7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 58.67585650463337\n",
      "1 13.443867385284962\n",
      "2 9.66026337100206\n",
      "3 2.207289800478552\n",
      "4 0.6386726824355378\n",
      "5 0.5771023252054059\n",
      "6 0.5546570063039103\n",
      "7 0.5403328815604065\n",
      "8 0.5382171113499423\n",
      "9 0.5314785468258695\n",
      "10 0.515656323287001\n",
      "11 0.4628547625269237\n",
      "12 0.4032072159371927\n",
      "13 0.3911112154973252\n",
      "14 0.38704083441600556\n",
      "15 0.3858585167760984\n",
      "16 0.3853846501987214\n",
      "17 0.3852594766617301\n",
      "18 0.3834002000823119\n",
      "19 0.38276962609824117\n",
      "20 0.37927780336304623\n",
      "21 0.37485208396134634\n",
      "22 0.37165727887928324\n",
      "23 0.369393051222528\n",
      "24 0.3591685470856261\n",
      "25 0.35298171476284135\n",
      "26 0.3456019134182031\n",
      "27 0.33475077584967206\n",
      "28 0.3262996874625082\n",
      "29 0.3256704817964604\n",
      "30 0.31879998781586455\n",
      "31 0.30726576700653124\n",
      "32 0.3023906128821411\n",
      "33 0.29579935493477666\n",
      "34 0.2937452805359731\n",
      "35 0.28766336555970284\n",
      "36 0.28328320633282916\n",
      "37 0.271393692236306\n",
      "38 0.2615850555269568\n",
      "39 0.21958249171107325\n",
      "40 0.21748207595249305\n",
      "41 0.191006707475752\n",
      "42 0.19071255128145667\n",
      "43 0.1694221470062465\n",
      "44 0.1648841072637875\n",
      "45 0.14721674307912117\n",
      "46 0.13477715113679595\n",
      "47 0.13017120075572616\n",
      "48 0.12379493319273685\n",
      "49 0.11884611369239921\n",
      "50 0.11725976340964023\n",
      "51 0.11602108503008399\n",
      "52 0.11472411878971217\n",
      "53 0.11466637027341195\n",
      "54 0.11386287302982755\n",
      "55 0.11271688536019196\n",
      "56 0.11268995762393172\n",
      "57 0.11164053955178611\n",
      "58 0.11122079504680696\n",
      "59 0.11113026528104321\n",
      "60 0.10733869988617338\n",
      "61 0.10720886924924065\n",
      "62 0.10666659056208046\n",
      "63 0.1058175624929915\n",
      "64 0.10490222471123087\n",
      "65 0.10410921222252231\n",
      "66 0.1037234042331901\n",
      "67 0.10282757868879257\n",
      "68 0.09940120397597749\n",
      "69 0.09712279914927122\n",
      "70 0.09620216366333562\n",
      "71 0.09251654117859072\n",
      "72 0.0919207210564341\n",
      "73 0.085227877423111\n",
      "74 0.08185553275883714\n",
      "75 0.08021934880376036\n",
      "76 0.07907869339149423\n",
      "77 0.07835217581219539\n",
      "78 0.07395764713046332\n",
      "79 0.07358619648830407\n",
      "80 0.07336771832370444\n",
      "81 0.07216770022479622\n",
      "82 0.07104516427768456\n",
      "83 0.0703234518615733\n",
      "84 0.07026983439651521\n",
      "85 0.06760366314993661\n",
      "86 0.06751784149731903\n",
      "87 0.06703964037058464\n",
      "88 0.06701187117014676\n",
      "89 0.06548616019952853\n",
      "90 0.06546410239388638\n",
      "91 0.0647280439719867\n",
      "92 0.06471047765611619\n",
      "93 0.06428679759689171\n",
      "94 0.06388354872788049\n",
      "95 0.06378652661715187\n",
      "96 0.06376999567698506\n",
      "97 0.06275794434890053\n",
      "98 0.0627219116011534\n",
      "99 0.06271136437878072\n",
      "100 0.06131293114097407\n",
      "101 0.0610187889361356\n",
      "102 0.06083994859534638\n",
      "103 0.060150207172154725\n",
      "104 0.05919559472968428\n",
      "105 0.05865946009701749\n",
      "106 0.058352800616989456\n",
      "107 0.05768330948980563\n",
      "108 0.05647292904581425\n",
      "109 0.056366204049802485\n",
      "110 0.05615722226543793\n",
      "111 0.05602389020315483\n",
      "112 0.054581101119631256\n",
      "113 0.05186246200215924\n",
      "114 0.05143270939684481\n",
      "115 0.0504504571060402\n",
      "116 0.05042964163944993\n",
      "117 0.049684467500248464\n",
      "118 0.04911002279641015\n",
      "119 0.049056924584074034\n",
      "120 0.0489071532338564\n",
      "121 0.04468701775032882\n",
      "122 0.04446933577296171\n",
      "123 0.04446342865920426\n",
      "124 0.04176950029423834\n",
      "125 0.040762207252418914\n",
      "126 0.04074268251445241\n",
      "127 0.04071128350779582\n",
      "128 0.03673904534467152\n",
      "129 0.03609783625279631\n",
      "130 0.03609609321619015\n",
      "131 0.03487425408542188\n",
      "132 0.033799191548650936\n",
      "133 0.033726155160335454\n",
      "134 0.033720459789535445\n",
      "135 0.032708544749889074\n",
      "136 0.032697363917178736\n",
      "137 0.031774171729435544\n",
      "138 0.03154298200394234\n",
      "139 0.03123852599630561\n",
      "140 0.03061494482984336\n",
      "141 0.030612060174236233\n",
      "142 0.03058365905289041\n",
      "143 0.029190267078031656\n",
      "144 0.02915411584519143\n",
      "145 0.029152615401697418\n",
      "146 0.027267261787453416\n",
      "147 0.027137902326111716\n",
      "148 0.027137004632266967\n",
      "149 0.025892009839261418\n",
      "150 0.024977542179454495\n",
      "151 0.02423914508094337\n",
      "152 0.024167958305528932\n",
      "153 0.024142383369341512\n",
      "154 0.02413471462809874\n",
      "155 0.024133671262312393\n",
      "156 0.023246212335568756\n",
      "157 0.02088210136488571\n",
      "158 0.02040866181785484\n",
      "159 0.020198177881413152\n",
      "160 0.020182760060077756\n",
      "161 0.020179384512720742\n",
      "162 0.02017730269831454\n",
      "163 0.020153727277700845\n",
      "164 0.018241607708373252\n",
      "165 0.016931046295181707\n",
      "166 0.01678236807505221\n",
      "167 0.016538249684195438\n",
      "168 0.016472356753515206\n",
      "169 0.01643898903889024\n",
      "170 0.016435456085008696\n",
      "171 0.01643495336781241\n",
      "172 0.016434158536690988\n",
      "173 0.01643328634709913\n",
      "174 0.016368046116939775\n",
      "175 0.015808775094795235\n",
      "176 0.015345556146251273\n",
      "177 0.0150900739499807\n",
      "178 0.015025646676052403\n",
      "179 0.015021421194585407\n",
      "180 0.015019697819179294\n",
      "181 0.015019251171159695\n",
      "182 0.015018350753487935\n",
      "183 0.014981928462487958\n",
      "184 0.014958937225375862\n",
      "185 0.014950953797535698\n",
      "186 0.014948058268832732\n",
      "187 0.01493742922847098\n",
      "188 0.01489203120045721\n",
      "189 0.014886929252409727\n",
      "190 0.014871580273505864\n",
      "191 0.014842570371269195\n",
      "192 0.014818978701993363\n",
      "193 0.014798101497799088\n",
      "194 0.014777013585929187\n",
      "195 0.014727092653605015\n",
      "196 0.01468341285584122\n",
      "197 0.014661888384306812\n",
      "198 0.01464770880135581\n",
      "199 0.014622886149997214\n",
      "200 0.01460335429739147\n",
      "201 0.014598728668764369\n",
      "202 0.014595150238889198\n",
      "203 0.014590421838363636\n",
      "204 0.014581243414524542\n",
      "205 0.014541243037552177\n",
      "206 0.014432021874573101\n",
      "207 0.014386164816348142\n",
      "208 0.014318096604634655\n",
      "209 0.014263874724005669\n",
      "210 0.014228389001779031\n",
      "211 0.014206575805665703\n",
      "212 0.01418232988152194\n",
      "213 0.014171774324635306\n",
      "214 0.014161235913703434\n",
      "215 0.014153836677554084\n",
      "216 0.014148586661491113\n",
      "217 0.014146065777857092\n",
      "218 0.014141029327170426\n",
      "219 0.014130716660428902\n",
      "220 0.014125612318144989\n",
      "221 0.014123336829902725\n",
      "222 0.01412155833923675\n",
      "223 0.014113351941701237\n",
      "224 0.014095381388720197\n",
      "225 0.014077149665955815\n",
      "226 0.01404831471455273\n",
      "227 0.014031563320408069\n",
      "228 0.014006824128705872\n",
      "229 0.01399436602374476\n",
      "230 0.01398355619642407\n",
      "231 0.013975588798228376\n",
      "232 0.013967855716942415\n",
      "233 0.013961244270306141\n",
      "234 0.013957076605179669\n",
      "235 0.013945778414123849\n",
      "236 0.01393601184902135\n",
      "237 0.013931671956572385\n",
      "238 0.013927687275251883\n",
      "239 0.013925774744874922\n",
      "240 0.01391896785308569\n",
      "241 0.013899667010347264\n",
      "242 0.013860128527963214\n",
      "243 0.013840869015048968\n",
      "244 0.013835487535196023\n",
      "245 0.013813422884287163\n",
      "246 0.01378929116867\n",
      "247 0.013722186174602092\n",
      "248 0.01369687468499194\n",
      "249 0.013692451953173802\n",
      "250 0.013662670748911336\n",
      "251 0.013641714284158436\n",
      "252 0.013625716472575539\n",
      "253 0.013578946607919367\n",
      "254 0.013550258099172444\n",
      "255 0.013529431394142108\n",
      "256 0.01349297174004095\n",
      "257 0.013478884665655947\n",
      "258 0.013465366958395666\n",
      "259 0.013462348267049676\n",
      "260 0.013461488736097645\n",
      "261 0.013459469189618916\n",
      "262 0.0134591687487595\n",
      "263 0.013457397513627572\n",
      "264 0.013451740385934078\n",
      "265 0.013446816894997729\n",
      "266 0.013445641057294993\n",
      "267 0.0134406947818146\n",
      "268 0.013436433982883595\n",
      "269 0.013433027236597556\n",
      "270 0.01342721714748034\n",
      "271 0.013406891984407497\n",
      "272 0.013376670661114393\n",
      "273 0.013362358695801916\n",
      "274 0.013353601118395172\n",
      "275 0.01334535750709559\n",
      "276 0.013340823789160922\n",
      "277 0.013311836081118987\n",
      "278 0.013279241289347566\n",
      "279 0.013274823514444608\n",
      "280 0.013271296658940633\n",
      "281 0.013255811969932807\n",
      "282 0.01323529345828367\n",
      "283 0.013215199230585133\n",
      "284 0.013197920855285985\n",
      "285 0.013193833757565557\n",
      "286 0.013177719119357167\n",
      "287 0.013153250230845265\n",
      "288 0.013146711471911762\n",
      "289 0.013128415196571196\n",
      "290 0.013102278212206796\n",
      "291 0.013085451967314036\n",
      "292 0.013064437562178136\n",
      "293 0.013044397891275997\n",
      "294 0.013028475782736871\n",
      "295 0.013005071760148123\n",
      "296 0.012992047418465184\n",
      "297 0.012986864123843951\n",
      "298 0.012984755302902058\n",
      "299 0.012981793017867737\n",
      "300 0.01297773968223345\n",
      "301 0.012940549531608187\n",
      "302 0.012883976491465466\n",
      "303 0.012835884534204063\n",
      "304 0.012769372730976405\n",
      "305 0.012700857587576614\n",
      "306 0.01266042414169548\n",
      "307 0.012643057266919642\n",
      "308 0.012598934913477496\n",
      "309 0.012557468697326898\n",
      "310 0.012544733431416478\n",
      "311 0.01252904903519036\n",
      "312 0.01246448553305577\n",
      "313 0.012393490843984124\n",
      "314 0.012320321750405809\n",
      "315 0.012270498222291986\n",
      "316 0.01222112660269808\n",
      "317 0.012198022630239655\n",
      "318 0.012190597434688744\n",
      "319 0.012184402371253051\n",
      "320 0.01217613173527969\n",
      "321 0.012168414763559768\n",
      "322 0.012166437690331922\n",
      "323 0.012164544217348954\n",
      "324 0.012160541125242123\n",
      "325 0.012101383281568272\n",
      "326 0.012058312885217042\n",
      "327 0.011992876284112516\n",
      "328 0.011981307377899708\n",
      "329 0.011976593575413277\n",
      "330 0.011974298687850257\n",
      "331 0.011970810216495023\n",
      "332 0.011969094339094797\n",
      "333 0.011963882924833861\n",
      "334 0.011920608989216352\n",
      "335 0.011771233603953133\n",
      "336 0.011614814071599647\n",
      "337 0.011589656242650067\n",
      "338 0.011587824006417342\n",
      "339 0.011570871970813532\n",
      "340 0.011485971845218905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 0.011445780881190462\n",
      "342 0.011400769964564636\n",
      "343 0.011367709652484738\n",
      "344 0.011363203007494632\n",
      "345 0.011357465863491048\n",
      "346 0.01135316112227329\n",
      "347 0.011351870296419611\n",
      "348 0.011349864196348366\n",
      "349 0.011348185212805272\n",
      "350 0.011335684564027216\n",
      "351 0.01131736840054238\n",
      "352 0.011296477799345978\n",
      "353 0.011281244819571434\n",
      "354 0.0112660839534125\n",
      "355 0.011257066039514122\n",
      "356 0.011250355816324827\n",
      "357 0.011247537357312482\n",
      "358 0.011245310213346998\n",
      "359 0.011244038055507215\n",
      "360 0.011242497261113534\n",
      "361 0.01123334319955743\n",
      "362 0.011188601951690382\n",
      "363 0.01106183265956858\n",
      "364 0.010922105666562145\n",
      "365 0.01080129544568439\n",
      "366 0.010765239967741355\n",
      "367 0.010743338920964603\n",
      "368 0.01070704817174191\n",
      "369 0.010592745771066359\n",
      "370 0.010463595101376656\n",
      "371 0.01041059420855859\n",
      "372 0.010347044461318828\n",
      "373 0.010317822575070986\n",
      "374 0.010295275263948384\n",
      "375 0.01026206951844443\n",
      "376 0.01020924978187072\n",
      "377 0.010122172386437001\n",
      "378 0.010013067988247859\n",
      "379 0.009990724792555264\n",
      "380 0.009916702181234738\n",
      "381 0.009908563506186494\n",
      "382 0.009904499389290423\n",
      "383 0.009896988473917694\n",
      "384 0.009850335103892107\n",
      "385 0.009839490501293258\n",
      "386 0.009832587405804569\n",
      "387 0.009832305474624435\n",
      "388 0.009830516285888243\n",
      "389 0.009828488706703983\n",
      "390 0.009826843712532578\n",
      "391 0.009824714637095928\n",
      "392 0.009820570741080411\n",
      "393 0.009813714384009879\n",
      "394 0.0098025362639061\n",
      "395 0.009787276916814554\n",
      "396 0.009767915958349964\n",
      "397 0.009750900208047404\n",
      "398 0.009732872721132652\n",
      "399 0.00971663333386649\n",
      "400 0.009708601117050776\n",
      "401 0.009704390766144102\n",
      "402 0.009702456989621959\n",
      "403 0.009701738855737302\n",
      "404 0.00970113289286102\n",
      "405 0.009700407693195249\n",
      "406 0.009700071938438668\n",
      "407 0.009698930897493106\n",
      "408 0.009696004377173245\n",
      "409 0.009692818124597484\n",
      "410 0.009689845219383557\n",
      "411 0.00968579654001378\n",
      "412 0.009677269333293919\n",
      "413 0.009675700430945785\n",
      "414 0.00967453047675266\n",
      "415 0.009670258780269206\n",
      "416 0.009664210291957898\n",
      "417 0.009657468996376444\n",
      "418 0.009647225085460384\n",
      "419 0.009635007603655535\n",
      "420 0.00961915132785509\n",
      "421 0.009599366644343505\n",
      "422 0.009588620015860083\n",
      "423 0.009576402001442807\n",
      "424 0.009545458563269996\n",
      "425 0.009523633991297664\n",
      "426 0.009501083660221963\n",
      "427 0.00947396646176637\n",
      "428 0.009451123135815637\n",
      "429 0.009440964334917552\n",
      "430 0.009421350950862665\n",
      "431 0.009391260361577208\n",
      "432 0.009379860056335633\n",
      "433 0.009357330926727848\n",
      "434 0.009338877132632449\n",
      "435 0.009325560291905947\n",
      "436 0.009304861272484848\n",
      "437 0.009272653183813115\n",
      "438 0.009224620821766977\n",
      "439 0.00921629108443213\n",
      "440 0.009210833734722403\n",
      "441 0.009209474464399877\n",
      "442 0.009208688839540772\n",
      "443 0.009207930559219979\n",
      "444 0.009206602768332987\n",
      "445 0.009206164217184714\n",
      "446 0.009204798677570911\n",
      "447 0.00920129316252835\n",
      "448 0.00919819316555506\n",
      "449 0.009188539695226906\n",
      "450 0.009168165486311655\n",
      "451 0.00914154739551117\n",
      "452 0.009114210035747755\n",
      "453 0.009093668547293194\n",
      "454 0.009078468295029686\n",
      "455 0.009073013118236379\n",
      "456 0.009070316711055421\n",
      "457 0.009069221995769476\n",
      "458 0.009067514904789952\n",
      "459 0.009066343152687008\n",
      "460 0.009064878342671804\n",
      "461 0.009064268167302847\n",
      "462 0.009063839236118091\n",
      "463 0.009060178437666913\n",
      "464 0.009054283877089604\n",
      "465 0.009048609168345677\n",
      "466 0.009043502420271205\n",
      "467 0.009036198715222503\n",
      "468 0.009025487487659413\n",
      "469 0.009010384671608189\n",
      "470 0.009001327684448229\n",
      "471 0.008998482417453229\n",
      "472 0.00899622347733232\n",
      "473 0.008985613824791915\n",
      "474 0.008980562409652754\n",
      "475 0.008972900443827085\n",
      "476 0.008962093405110819\n",
      "477 0.00894653621116048\n",
      "478 0.008930388513112246\n",
      "479 0.008922242695084136\n",
      "480 0.008918533821310614\n",
      "481 0.008913916819230799\n",
      "482 0.00891106263214553\n",
      "483 0.008910026854617692\n",
      "484 0.008909801638944902\n",
      "485 0.008909632738475183\n",
      "486 0.008909425937571842\n",
      "487 0.008907991267656266\n",
      "488 0.008904687339877005\n",
      "489 0.008901255266486616\n",
      "490 0.008892761689684957\n",
      "491 0.008881700637234868\n",
      "492 0.00887093480432364\n",
      "493 0.008814271123117957\n",
      "494 0.008797056257403443\n",
      "495 0.008757527164591182\n",
      "496 0.008735475879979884\n",
      "497 0.008729614305149723\n",
      "498 0.008728984167380828\n",
      "499 0.00872870282735348\n",
      "500 0.008727620270940846\n",
      "501 0.008726561218893071\n",
      "502 0.008723663200148557\n",
      "503 0.008666858043839988\n",
      "504 0.008616651839099897\n",
      "505 0.008613921545373581\n",
      "506 0.008608173071541651\n",
      "507 0.008598810926111962\n",
      "508 0.008583311264646256\n",
      "509 0.008564638969185733\n",
      "510 0.008550360341164038\n",
      "511 0.008548322470013352\n",
      "512 0.008547878221622082\n",
      "513 0.00854777847273139\n",
      "514 0.008547607241463145\n",
      "515 0.008540818623358901\n",
      "516 0.008507164118786716\n",
      "517 0.00844870264251224\n",
      "518 0.008363610018851615\n",
      "519 0.00831899235880206\n",
      "520 0.008291602401046703\n",
      "521 0.008273984952150617\n",
      "522 0.008268019182914433\n",
      "523 0.008266818670921177\n",
      "524 0.008266532932998168\n",
      "525 0.008266154568881806\n",
      "526 0.008264085413394628\n",
      "527 0.008261569887795304\n",
      "528 0.00825863420779158\n",
      "529 0.008254351913283862\n",
      "530 0.008250006864685008\n",
      "531 0.008244391002432008\n",
      "532 0.008238409316164692\n",
      "533 0.00822777722261626\n",
      "534 0.008214722161720324\n",
      "535 0.00820536782381729\n",
      "536 0.008198343373276105\n",
      "537 0.008194041651753135\n",
      "538 0.00818680169190468\n",
      "539 0.008177399062647167\n",
      "540 0.008171900219111511\n",
      "541 0.00816969036760158\n",
      "542 0.008167858680192657\n",
      "543 0.008166373913363032\n",
      "544 0.008165649286919326\n",
      "545 0.008165322665139656\n",
      "546 0.00816417948612733\n",
      "547 0.008161429098015326\n",
      "548 0.00815731105284252\n",
      "549 0.008114732308049318\n",
      "550 0.007946285192799777\n",
      "551 0.007690402335399343\n",
      "552 0.007542222918679844\n",
      "553 0.007507935124130005\n",
      "554 0.007491094015357967\n",
      "555 0.007428387945344994\n",
      "556 0.007411607441249325\n",
      "557 0.007402837175920724\n",
      "558 0.007385565569645837\n",
      "559 0.007362559680461403\n",
      "560 0.0073589122008924975\n",
      "561 0.007352714340525673\n",
      "562 0.007349189223492955\n",
      "563 0.007343081156823089\n",
      "564 0.007327308529314787\n",
      "565 0.007320661094925081\n",
      "566 0.007316388719731215\n",
      "567 0.007310514859662548\n",
      "568 0.007301907354322276\n",
      "569 0.007298916935454051\n",
      "570 0.007297747410312496\n",
      "571 0.007294840783513298\n",
      "572 0.0072910677695666015\n",
      "573 0.007283588871870299\n",
      "574 0.007278202248883171\n",
      "575 0.007276020643953178\n",
      "576 0.007274745606920973\n",
      "577 0.007273797813218772\n",
      "578 0.007273331760430378\n",
      "579 0.007273249809432613\n",
      "580 0.007273017479162196\n",
      "581 0.007272251417905531\n",
      "582 0.007271109438359692\n",
      "583 0.007270223335211511\n",
      "584 0.0072693738409616605\n",
      "585 0.007268209777432478\n",
      "586 0.00726678645919094\n",
      "587 0.007265303930521683\n",
      "588 0.007262326077742559\n",
      "589 0.0072597466856016216\n",
      "590 0.007256492885151651\n",
      "591 0.0072528126349667545\n",
      "592 0.0072491766750244655\n",
      "593 0.007245357274062868\n",
      "594 0.007240065650454259\n",
      "595 0.0072340944821833566\n",
      "596 0.007227602569153585\n",
      "597 0.007215377421773278\n",
      "598 0.007202570498577628\n",
      "599 0.007181450601878221\n",
      "600 0.007156754109729009\n",
      "601 0.007139864737833735\n",
      "602 0.007129588582091192\n",
      "603 0.007116761118138552\n",
      "604 0.007099751232349913\n",
      "605 0.0070835158672646345\n",
      "606 0.007055656529230889\n",
      "607 0.007033755063055074\n",
      "608 0.007005411906641913\n",
      "609 0.006982884492625914\n",
      "610 0.0069657179302671166\n",
      "611 0.006950007326741164\n",
      "612 0.006940593095134049\n",
      "613 0.006933449726726464\n",
      "614 0.006917128784414777\n",
      "615 0.006903571631097117\n",
      "616 0.006895100694457104\n",
      "617 0.00688586583527237\n",
      "618 0.0068744468452345405\n",
      "619 0.006864060115321516\n",
      "620 0.006857287251223016\n",
      "621 0.006856159308304013\n",
      "622 0.006855691208807389\n",
      "623 0.006855200617385978\n",
      "624 0.006850727795140747\n",
      "625 0.006845778314228443\n",
      "626 0.006836386486645222\n",
      "627 0.006829784404021239\n",
      "628 0.006826665381583779\n",
      "629 0.006821944094343885\n",
      "630 0.006817629349230182\n",
      "631 0.006816948870155481\n",
      "632 0.006814326005693709\n",
      "633 0.006802187180603601\n",
      "634 0.006780964531222282\n",
      "635 0.006778102087314701\n",
      "636 0.00677655779661819\n",
      "637 0.006757263934517266\n",
      "638 0.006743638677181148\n",
      "639 0.006737129546275981\n",
      "640 0.006734835732391729\n",
      "641 0.006734386354637332\n",
      "642 0.006733899564479591\n",
      "643 0.0067313846392941185\n",
      "644 0.00672807225778961\n",
      "645 0.00672326843903183\n",
      "646 0.006715466441427638\n",
      "647 0.006707917879755363\n",
      "648 0.006703087812726117\n",
      "649 0.006701717441386165\n",
      "650 0.006700985451499722\n",
      "651 0.006699423790624961\n",
      "652 0.006694501418215693\n",
      "653 0.006674752847127611\n",
      "654 0.006663366390204789\n",
      "655 0.00663865053203482\n",
      "656 0.006594054973271338\n",
      "657 0.006555309430810661\n",
      "658 0.006470614413708506\n",
      "659 0.00640868847238307\n",
      "660 0.006328372250915728\n",
      "661 0.0063119134264178\n",
      "662 0.006283045428436359\n",
      "663 0.00623182392751483\n",
      "664 0.006198735846943605\n",
      "665 0.006164948554904313\n",
      "666 0.006113392193482754\n",
      "667 0.006018276570265377\n",
      "668 0.005954333750811938\n",
      "669 0.005919771767143194\n",
      "670 0.005898651746701676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671 0.00588268135656672\n",
      "672 0.005854888264037702\n",
      "673 0.005819901960784967\n",
      "674 0.00580352211763758\n",
      "675 0.005791899642516214\n",
      "676 0.005784033969388189\n",
      "677 0.005776753526112263\n",
      "678 0.005763766425369292\n",
      "679 0.00575920014341426\n",
      "680 0.005755148402398861\n",
      "681 0.005747757727636947\n",
      "682 0.005738307302845292\n",
      "683 0.00573168664769892\n",
      "684 0.005729672859584604\n",
      "685 0.0057289362962552794\n",
      "686 0.005727223197326721\n",
      "687 0.005726354139361823\n",
      "688 0.0057241211386402525\n",
      "689 0.005723000465542735\n",
      "690 0.005722030845056811\n",
      "691 0.005721603197968286\n",
      "692 0.005721349677103128\n",
      "693 0.005721068533361963\n",
      "694 0.005720556825015966\n",
      "695 0.005719647661978679\n",
      "696 0.00571575186151741\n",
      "697 0.005713061789741055\n",
      "698 0.00570822270676217\n",
      "699 0.005705155522419263\n",
      "700 0.005703028643395191\n",
      "701 0.0057023228298726586\n",
      "702 0.005702130108271676\n",
      "703 0.005701898088080323\n",
      "704 0.005700090939872897\n",
      "705 0.005694128751477855\n",
      "706 0.005689964178235063\n",
      "707 0.005686859850523146\n",
      "708 0.005683804965872811\n",
      "709 0.005678570107000056\n",
      "710 0.0056668988362057054\n",
      "711 0.005654589835817386\n",
      "712 0.005641141649693055\n",
      "713 0.005622789399865314\n",
      "714 0.005600132637259494\n",
      "715 0.0055749765536462995\n",
      "716 0.005545387821798217\n",
      "717 0.005521218665236648\n",
      "718 0.00550261845762922\n",
      "719 0.005478368239060147\n",
      "720 0.005461952842721435\n",
      "721 0.005439231984628602\n",
      "722 0.0054095545938211385\n",
      "723 0.005384332886212346\n",
      "724 0.005343094738853021\n",
      "725 0.005312165757066364\n",
      "726 0.00527824858954165\n",
      "727 0.005213398738914775\n",
      "728 0.0051737320391603645\n",
      "729 0.005118006554211758\n",
      "730 0.005069786429778812\n",
      "731 0.005022046519203375\n",
      "732 0.0049943785590175985\n",
      "733 0.004970284640025462\n",
      "734 0.00495951172171472\n",
      "735 0.004952125581370461\n",
      "736 0.004945541898328631\n",
      "737 0.004940334715814929\n",
      "738 0.004935101491294886\n",
      "739 0.004930018263875288\n",
      "740 0.004921048279031975\n",
      "741 0.004916837917222132\n",
      "742 0.004908516832773455\n",
      "743 0.004896069362767209\n",
      "744 0.004886960003815692\n",
      "745 0.004866515041622189\n",
      "746 0.004857762183981801\n",
      "747 0.004842343120551361\n",
      "748 0.00483947499906835\n",
      "749 0.004835998409442202\n",
      "750 0.004834768642852841\n",
      "751 0.004833346654968893\n",
      "752 0.004832261295121408\n",
      "753 0.004832007238895631\n",
      "754 0.004831462071096321\n",
      "755 0.0048294006796733655\n",
      "756 0.004828399821579254\n",
      "757 0.0048283041013382115\n",
      "758 0.004828251555993746\n",
      "759 0.004827977935826551\n",
      "760 0.004825872364007631\n",
      "761 0.004819279200129537\n",
      "762 0.004811268235706219\n",
      "763 0.004801584106189498\n",
      "764 0.004781235634207074\n",
      "765 0.00475764705941189\n",
      "766 0.004726986022099455\n",
      "767 0.0047015995692433424\n",
      "768 0.004680476860313842\n",
      "769 0.004669659292798569\n",
      "770 0.004660166359216554\n",
      "771 0.00465354403058563\n",
      "772 0.004648489390380011\n",
      "773 0.004645592500667791\n",
      "774 0.004634034290371563\n",
      "775 0.004620393798400757\n",
      "776 0.004609306072180223\n",
      "777 0.004597935334011597\n",
      "778 0.004591670568800925\n",
      "779 0.004583044249557761\n",
      "780 0.004571501282116476\n",
      "781 0.0045552259419106905\n",
      "782 0.004538042216171187\n",
      "783 0.004528110332465277\n",
      "784 0.0045239505410056375\n",
      "785 0.004521102136083418\n",
      "786 0.004519010655906693\n",
      "787 0.00451779660696617\n",
      "788 0.004516734079505316\n",
      "789 0.0045149278908661016\n",
      "790 0.004512323789138441\n",
      "791 0.004509586757984829\n",
      "792 0.004507697907284836\n",
      "793 0.00450312604814753\n",
      "794 0.004498420115946079\n",
      "795 0.00449547430495035\n",
      "796 0.00449371050468679\n",
      "797 0.004492123143031869\n",
      "798 0.004491643217288146\n",
      "799 0.004491496348137619\n",
      "800 0.004491171645963174\n",
      "801 0.004489896572242488\n",
      "802 0.004488782898802789\n",
      "803 0.004484810694064414\n",
      "804 0.00448217426059837\n",
      "805 0.004478785518304554\n",
      "806 0.004476462926978861\n",
      "807 0.004472813247399604\n",
      "808 0.004467065197504723\n",
      "809 0.004464283649411416\n",
      "810 0.004462146198426628\n",
      "811 0.004457979969146326\n",
      "812 0.004455016568658519\n",
      "813 0.0044528710726237\n",
      "814 0.004450789110141167\n",
      "815 0.004448600045556396\n",
      "816 0.0044468991699499975\n",
      "817 0.004444462308891095\n",
      "818 0.004443070711942394\n",
      "819 0.004438360754064263\n",
      "820 0.004431301163353757\n",
      "821 0.004425801508514017\n",
      "822 0.004417695115946527\n",
      "823 0.004409742184283009\n",
      "824 0.004393627532600647\n",
      "825 0.004381717853149427\n",
      "826 0.0043702798482397835\n",
      "827 0.004361496779859517\n",
      "828 0.004350569214850393\n",
      "829 0.004343970892225445\n",
      "830 0.00434080975682777\n",
      "831 0.004339786763741362\n",
      "832 0.00433945395276755\n",
      "833 0.00433916746819178\n",
      "834 0.004338356001150399\n",
      "835 0.0043372950084909255\n",
      "836 0.004336760562022704\n",
      "837 0.0043367121875310145\n",
      "838 0.004336638205416303\n",
      "839 0.004336334746239636\n",
      "840 0.004336134499203892\n",
      "841 0.004336016007842932\n",
      "842 0.004335486308441779\n",
      "843 0.004334868446446346\n",
      "844 0.004333966124944626\n",
      "845 0.0043331292563031695\n",
      "846 0.004332103465572478\n",
      "847 0.004331092716164168\n",
      "848 0.004329790366911548\n",
      "849 0.004328472943859108\n",
      "850 0.004327030183717497\n",
      "851 0.004324841825128014\n",
      "852 0.004322848283565003\n",
      "853 0.00431879973298525\n",
      "854 0.004315973644837344\n",
      "855 0.004311549162775754\n",
      "856 0.00430497883013417\n",
      "857 0.004299701407536904\n",
      "858 0.0042945820881036606\n",
      "859 0.004287000307425422\n",
      "860 0.004275994336424265\n",
      "861 0.004266020724331123\n",
      "862 0.0042527226016034255\n",
      "863 0.004240509010026284\n",
      "864 0.004227289733892668\n",
      "865 0.004214484211370202\n",
      "866 0.00420139765657079\n",
      "867 0.004194897832595828\n",
      "868 0.004189199822985211\n",
      "869 0.004186182943631793\n",
      "870 0.004183960971698231\n",
      "871 0.004182127789847\n",
      "872 0.004181520462257798\n",
      "873 0.004180968553608314\n",
      "874 0.004180478960697131\n",
      "875 0.0041795971790264935\n",
      "876 0.0041792703827189595\n",
      "877 0.0041790357479639635\n",
      "878 0.004178898405421977\n",
      "879 0.004178713435531486\n",
      "880 0.004178663109486188\n",
      "881 0.004178565490125814\n",
      "882 0.004178429140958984\n",
      "883 0.00417821215679797\n",
      "884 0.004178168294061121\n",
      "885 0.00417798426831623\n",
      "886 0.004177928675503912\n",
      "887 0.004177874399823379\n",
      "888 0.004177681982718108\n",
      "889 0.004177569022319583\n",
      "890 0.004177455075520418\n",
      "891 0.004177235212467308\n",
      "892 0.004177049880843527\n",
      "893 0.0041769865122754405\n",
      "894 0.004176803643736442\n",
      "895 0.004176288899958726\n",
      "896 0.004175805338024515\n",
      "897 0.004175526253547495\n",
      "898 0.0041750247502926295\n",
      "899 0.004174250023771927\n",
      "900 0.004173450042599969\n",
      "901 0.004172202791226646\n",
      "902 0.0041708748114440905\n",
      "903 0.004169601688325737\n",
      "904 0.0041677314861943695\n",
      "905 0.00416590944073966\n",
      "906 0.004164532304361201\n",
      "907 0.0041627531952150515\n",
      "908 0.004160467627843709\n",
      "909 0.004155214184432517\n",
      "910 0.004152356813324424\n",
      "911 0.004145177858240473\n",
      "912 0.004135240419077721\n",
      "913 0.004126576928971741\n",
      "914 0.004105091818578711\n",
      "915 0.004091572205936926\n",
      "916 0.004078876196917006\n",
      "917 0.004066442458903087\n",
      "918 0.004044260380447819\n",
      "919 0.004025978755830949\n",
      "920 0.0040227756256077185\n",
      "921 0.004018694745128463\n",
      "922 0.0040169574882693996\n",
      "923 0.004015787043697819\n",
      "924 0.004014774704161453\n",
      "925 0.0040136955084461145\n",
      "926 0.004012930933778691\n",
      "927 0.004012524865778715\n",
      "928 0.004012123055834475\n",
      "929 0.004011928080692353\n",
      "930 0.004011816684271223\n",
      "931 0.004011767809791128\n",
      "932 0.0040116016188858645\n",
      "933 0.004011515574631918\n",
      "934 0.004011466864101182\n",
      "935 0.004011386655255282\n",
      "936 0.004011232379469368\n",
      "937 0.004011038142233962\n",
      "938 0.004010745462972749\n",
      "939 0.0040105884927114435\n",
      "940 0.00401038024215818\n",
      "941 0.004010250303741049\n",
      "942 0.004009616398052131\n",
      "943 0.004008746475649962\n",
      "944 0.004008536187319056\n",
      "945 0.00400832664409521\n",
      "946 0.004007817095401755\n",
      "947 0.004007261986722074\n",
      "948 0.004006786672711497\n",
      "949 0.00400609651473156\n",
      "950 0.004004657705654102\n",
      "951 0.004002591584308574\n",
      "952 0.004000275765179268\n",
      "953 0.003998679377357408\n",
      "954 0.003996575679922\n",
      "955 0.003993799688085716\n",
      "956 0.003990382650094138\n",
      "957 0.003988105159311987\n",
      "958 0.00398484742604327\n",
      "959 0.003981778148466031\n",
      "960 0.003977357643153426\n",
      "961 0.0039680058716302135\n",
      "962 0.003955581194824943\n",
      "963 0.0039427163215486555\n",
      "964 0.00392549787710105\n",
      "965 0.003909596914510747\n",
      "966 0.0038926480853715296\n",
      "967 0.0038786460571553732\n",
      "968 0.0038649508272980545\n",
      "969 0.0038484429826648463\n",
      "970 0.003841101506954299\n",
      "971 0.003837698480359992\n",
      "972 0.003833118471447513\n",
      "973 0.003830807097937001\n",
      "974 0.0038298694772506614\n",
      "975 0.0038289355339464334\n",
      "976 0.0038278585514917984\n",
      "977 0.003827090821917056\n",
      "978 0.0038268040402882455\n",
      "979 0.0038261641864750172\n",
      "980 0.0038259253598883513\n",
      "981 0.0038256502911357577\n",
      "982 0.0038253737875854925\n",
      "983 0.003825078679463629\n",
      "984 0.0038249428620204264\n",
      "985 0.003824681215134433\n",
      "986 0.003824551382642325\n",
      "987 0.003824451987389634\n",
      "988 0.003824403745617357\n",
      "989 0.0038243552834695783\n",
      "990 0.0038243478076304266\n",
      "991 0.0038243201820660913\n",
      "992 0.003824200195487035\n",
      "993 0.0038241326065125455\n",
      "994 0.0038238570696449018\n",
      "995 0.003823455658775165\n",
      "996 0.0038230907077732895\n",
      "997 0.003822790311807338\n",
      "998 0.003822281094438954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 0.0038220127004691628\n"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "for epoch in range(1000):\n",
    "    dataloader.dataset.currentkey = None\n",
    "    for x_data, y_data, structure in dataloader:    \n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_data)\n",
    "            loss = mse_full(pred, y_data, structure, orbs)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(single_step)\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03cb59bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0012464972788176667\n",
      "1 0.001246497261380447\n",
      "2 0.001246497243966604\n",
      "3 0.0012464972265763773\n",
      "4 0.001246497209210777\n",
      "5 0.0012464971918664859\n",
      "6 0.0012464971745468868\n",
      "7 0.0012464971572494008\n",
      "8 0.0012464971399747554\n",
      "9 0.0012464971227227823\n",
      "10 0.0012464971054942937\n",
      "11 0.0012464970901062589\n",
      "12 0.001246497075165657\n",
      "13 0.001246497060249157\n",
      "14 0.0012464970453594266\n",
      "15 0.0012464970304943783\n",
      "16 0.00124649701565079\n",
      "17 0.0012464970008271672\n",
      "18 0.0012464969860254327\n",
      "19 0.001246496971243852\n",
      "20 0.0012464969564919628\n",
      "21 0.0012464969417559527\n",
      "22 0.0012464969270477506\n",
      "23 0.0012464969123584773\n",
      "24 0.0012464968976892345\n",
      "25 0.0012464968830454978\n",
      "26 0.0012464968684161948\n",
      "27 0.0012464968538118174\n",
      "28 0.0012464968392307562\n",
      "29 0.001246496824673962\n",
      "30 0.0012464968101291599\n",
      "31 0.0012464967956094652\n",
      "32 0.0012464967811091087\n",
      "33 0.0012464967666273087\n",
      "34 0.0012464967521646842\n",
      "35 0.0012464967377236737\n",
      "36 0.0012464967233024266\n",
      "37 0.0012464967088970489\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m all_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/lbfgs.py:425\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 425\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    428\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/lbfgs.py:49\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     51\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/lbfgs.py:423\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/lbfgs.py:277\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 277\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    278\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36msingle_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(x_data)\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_full(pred, y_data, structure, orbs)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    dataloader.dataset.currentkey = None\n",
    "    for x_data, y_data, structure in dataloader:    \n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_data)\n",
    "            loss = mse_full(pred, y_data, structure, orbs)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(single_step)\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f627a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddbf7eb580>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5122692",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb10f943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:53.043094Z",
     "start_time": "2022-12-29T10:15:52.986380Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load test set\n",
    "test_frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\"50:80\")\n",
    "# frames2 = ase.io.read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "test_frames = test_frames1 #+ frames2\n",
    "for f in test_frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "\n",
    "# test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:60]\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "\n",
    "test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:80]\n",
    "test_focks = test_focks1.astype(np.float64)\n",
    "# test_focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[50:80]\n",
    "# test_overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[50:80]\n",
    "\n",
    "# test_orthogonal = []\n",
    "# for i in range(len(test_focks)): \n",
    "#     test_focks[i] = fix_pyscf_l1(test_focks[i],test_frames[i], orbs)\n",
    "#     test_overlap[i] = fix_pyscf_l1(test_overlap[i],test_frames[i], orbs)\n",
    "#     test_orthogonal.append(lowdin_orthogonalize(test_focks[i], test_overlap[i]))\n",
    "# test_focks = np.asarray(test_orthogonal, dtype=np.float64)\n",
    "    \n",
    "test_blocks = dense_to_blocks(test_focks, test_frames, orbs)\n",
    "test_fock_bc = couple_blocks(test_blocks, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "515d2062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:54.725559Z",
     "start_time": "2022-12-29T10:15:53.932535Z"
    }
   },
   "outputs": [],
   "source": [
    "test_rhoi = spex.compute(test_frames)\n",
    "test_gij = pairs.compute(test_frames)\n",
    "test_rho1i = acdc_standardize_keys(test_rhoi)\n",
    "test_rho1i = test_rho1i.keys_to_properties(['species_neighbor'])\n",
    "test_gij =  acdc_standardize_keys(test_gij)\n",
    "test_rho2i = cg_increment(test_rho1i, test_rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "test_rho1ij = cg_increment(test_rho1i, test_gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "test_features = hamiltonian_features(test_rho2i, test_rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "695e248b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.390807Z",
     "start_time": "2022-12-29T10:15:54.727705Z"
    }
   },
   "outputs": [],
   "source": [
    "#from equistore.io import save\n",
    "save(\"test_feature.npz\", test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5ee3635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.442916Z",
     "start_time": "2022-12-29T10:15:55.402080Z"
    }
   },
   "outputs": [],
   "source": [
    "test_feature_path = \"./test_feature.npz\"\n",
    "testing = HamiltonianDataset(test_feature_path, test_blocks, test_focks, test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e979a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.726557Z",
     "start_time": "2022-12-29T10:15:55.722442Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "test_Sampler = torch.utils.data.sampler.RandomSampler(testing)\n",
    "test_BSampler = torch.utils.data.sampler.BatchSampler(test_Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "test_dataloader = DataLoader(testing, sampler = test_BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d07f4e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0035, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testing.currentkey = None\n",
    "test_dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in test_dataloader:\n",
    "    t_pred = model(x_data)\n",
    "    print (mse_full(t_pred, torch.tensor(test_focks), structures, orbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc669ae9",
   "metadata": {},
   "source": [
    "### Test for equivariance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "309cb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_frames = frames.copy()\n",
    "for i, f in enumerate(rot_frames):\n",
    "    f.positions = 50+(50-frames[i].positions[:,[2,0,1]])\n",
    "    f.positions[:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4072bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_rhoi = spex.compute(rot_frames)\n",
    "rot_gij = pairs.compute(rot_frames)\n",
    "rot_rho1i = acdc_standardize_keys(rot_rhoi)\n",
    "rot_rho1i = rot_rho1i.keys_to_properties(['species_neighbor'])\n",
    "rot_gij =  acdc_standardize_keys(rot_gij)\n",
    "rot_rho2i = cg_increment(rot_rho1i, rot_rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "rot_rho1ij = cg_increment(rot_rho1i, rot_gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b91e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_features = hamiltonian_features(rot_rho2i, rot_rho1ij)\n",
    "\n",
    "#from equistore.io import save\n",
    "save(\"rot_feature.npz\", rot_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "51bc0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = \"./rot_feature.npz\"\n",
    "rot_dataset = HamiltonianDataset(feature_path, blocks, focks, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "34d22f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3112, -0.6972, -0.5651, -0.4966,  0.1740,  0.2535],\n",
       "       grad_fn=<LinalgEighBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot_pred = model(rot_features)\n",
    "rot_pred_fock = blocks_to_dense(rot_pred, rot_frames, orbs)\n",
    "torch.linalg.eigvalsh(rot_pred_fock[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49acc490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3112, -0.6972, -0.5651, -0.4966,  0.1740,  0.2535],\n",
       "       grad_fn=<LinalgEighBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(features)\n",
    "pred_fock = blocks_to_dense(pred, frames, orbs)\n",
    "torch.linalg.eigvalsh(pred_fock[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c72c6f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure()\n",
    "axes = figure.add_subplot(111)\n",
    " \n",
    "caxes = axes.matshow(rot_pred_fock[0].cpu().detach().numpy()- pred_fock[0].cpu().detach().numpy(), interpolation ='nearest')\n",
    "figure.colorbar(caxes)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6cec20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "786px",
    "left": "27px",
    "top": "111.133px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
