{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5c7c0",
   "metadata": {
    "hide_input": true,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Manipulate-Hamiltonian-into-blocks\" data-toc-modified-id=\"Manipulate-Hamiltonian-into-blocks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Manipulate Hamiltonian into blocks</a></span></li><li><span><a href=\"#Feature-computation\" data-toc-modified-id=\"Feature-computation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature computation</a></span></li><li><span><a href=\"#Feature-Preprocessing\" data-toc-modified-id=\"Feature-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Preprocessing</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#DataLoader\" data-toc-modified-id=\"DataLoader-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DataLoader</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Tests</a></span><ul class=\"toc-item\"><li><span><a href=\"#set-up-wigner-d-rotations-matrices\" data-toc-modified-id=\"set-up-wigner-d-rotations-matrices-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>set up wigner-d rotations matrices</a></span></li><li><span><a href=\"#block-wise-rotations\" data-toc-modified-id=\"block-wise-rotations-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>block wise rotations</a></span></li><li><span><a href=\"#Rotate-matrix\" data-toc-modified-id=\"Rotate-matrix-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Rotate matrix</a></span></li><li><span><a href=\"#Eigenvalue-tests\" data-toc-modified-id=\"Eigenvalue-tests-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Eigenvalue tests</a></span></li><li><span><a href=\"#check-decoupling-of-blocks\" data-toc-modified-id=\"check-decoupling-of-blocks-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>check decoupling of blocks</a></span></li><li><span><a href=\"#check-feature-rotations\" data-toc-modified-id=\"check-feature-rotations-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>check feature rotations</a></span></li></ul></li><li><span><a href=\"#need-to-modify-samples-for-features-to-start-from-1\" data-toc-modified-id=\"need-to-modify-samples-for-features-to-start-from-1-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>need to modify samples for features to start from 1</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3096d21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T17:32:48.888548Z",
     "start_time": "2023-02-06T17:32:48.885877Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2bb576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.292787Z",
     "start_time": "2023-02-06T19:35:54.812257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ase.io\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "# from rascal.representations import SphericalExpansion\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from ase.units import Hartree\n",
    "\n",
    "from torch_hamiltonian_utils.torch_cg import ClebschGordanReal\n",
    "from torch_hamiltonian_utils.torch_ham import fix_pyscf_l1, dense_to_blocks, blocks_to_dense, couple_blocks, decouple_blocks, hamiltonian_features\n",
    "from torch_hamiltonian_utils.torch_builder import TensorBuilder\n",
    "\n",
    "import equistore\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "from equistore_utils.librascal import  RascalSphericalExpansion, RascalPairExpansion\n",
    "from equistore_utils.acdc_mini import acdc_standardize_keys, cg_increment, cg_combine\n",
    "from equistore_utils.model_hamiltonian import get_feat_keys, get_feat_keys_from_uncoupled \n",
    "\n",
    "import importlib\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68aeb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.301657Z",
     "start_time": "2023-02-06T19:36:04.294480Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feat_keys_from_uncoupled(block_keys, sigma=None, order_nu=None):\n",
    "    \"\"\"Map UNCOUPLED block keys to corresponding feature key. take as extra input the sigma, nu value if required.\n",
    "    sigma=0 returns all possible sigma values at given 'nu'\"\"\"\n",
    "    blocktype, species1, n1, l1, species2, n2, l2 = block_keys\n",
    "    feat_blocktype = blocktype\n",
    "    keys_L=[]\n",
    "    for L in range(abs(l1-l2), l1+l2+1):\n",
    "        if sigma is None:\n",
    "            z = (l1+l2+L)%2\n",
    "            inv_sigma = 1 - 2*z\n",
    "        elif abs(sigma)==1:\n",
    "            inv_sigma = sigma\n",
    "        else: \n",
    "            raise(\"Please check sigma value, it should be +1 or -1\")\n",
    "        \n",
    "        if blocktype == 1 and n1 == n2 and l1 == l2:\n",
    "            feat_blocktype = inv_sigma\n",
    "                 \n",
    "        if inv_sigma == -1 and blocktype == 0 and n1 == n2 and l1 == l2:\n",
    "            continue     \n",
    "        \n",
    "        keys_L.append([(order_nu, inv_sigma, L, species1, species2, feat_blocktype)])\n",
    "    #     feat= (blocktype, L,sigma,species1, species2)\n",
    "    feat = Labels([\"order_nu\", \"inversion_sigma\", \"spherical_harmonics_l\", \"species_center\", \"species_neighbor\", \"block_type\"], np.asarray(keys_L, dtype=np.int32).reshape(-1,6))\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3361a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.331902Z",
     "start_time": "2023-02-06T19:36:04.303851Z"
    }
   },
   "outputs": [],
   "source": [
    "# frames = ase.io.read(\"data/water_rotated/water_rotated_3.xyz\",\":\")\n",
    "# for f in frames:\n",
    "#     f.cell = [100,100,100]\n",
    "#     f.positions += 50\n",
    "    \n",
    "# jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "# orbs = {}\n",
    "# zdic = {\"O\" : 8, \"H\":1}\n",
    "# for k in jorbs:\n",
    "#     orbs[zdic[k]] = jorbs[k]\n",
    "# focks = np.load(\"data/water_rotated/water_rotated_saph_3.npy\", allow_pickle=True)[:len(frames)]\n",
    "# rotations = np.load(\"data/water_rotated/rotations_3.npy\", allow_pickle = True)\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10faaf55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.360121Z",
     "start_time": "2023-02-06T19:36:04.334928Z"
    }
   },
   "outputs": [],
   "source": [
    "def lowdin_orthogonalize(fock, s):\n",
    "    \"\"\"\n",
    "    lowdin orthogonalization of a fock matrix computing the square root of the overlap matrix\n",
    "    \"\"\"\n",
    "    eva, eve = np.linalg.eigh(s)\n",
    "    sm12 = eve @ np.diag(1.0/np.sqrt(eva)) @ eve.T\n",
    "    return sm12 @ fock @ sm12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b86aa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.644913Z",
     "start_time": "2023-02-06T19:36:04.361638Z"
    }
   },
   "outputs": [],
   "source": [
    "frames = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\":50\")\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1, \"C\":6}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]\n",
    "\n",
    "focks = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames)]\n",
    "focks = focks.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246e69de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:04.649076Z",
     "start_time": "2023-02-06T19:36:04.646607Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# #generating jagged matrix\n",
    "# f=[]\n",
    "# for x in focks:\n",
    "#     f.append(x)\n",
    "# for x in focks2:\n",
    "#     f.append(x)\n",
    "    \n",
    "# jagged = np.asanyarray(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc18008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.359435Z",
     "start_time": "2023-02-06T19:36:04.651072Z"
    }
   },
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad712",
   "metadata": {},
   "source": [
    "## Manipulate Hamiltonian into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461e38a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.464335Z",
     "start_time": "2023-02-06T19:36:05.362511Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8cfe3",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13064b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.561138Z",
     "start_time": "2023-02-06T19:36:05.487376Z"
    }
   },
   "outputs": [],
   "source": [
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 4.0,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 4,\n",
    "    \"gaussian_sigma_constant\" : 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\":  False,\n",
    "}\n",
    "\n",
    "spex = RascalSphericalExpansion(rascal_hypers)\n",
    "rhoi = spex.compute(frames)\n",
    "\n",
    "lmax = rascal_hypers[\"max_angular\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc39e55f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.672601Z",
     "start_time": "2023-02-06T19:36:05.562322Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = RascalPairExpansion(rascal_hypers)\n",
    "gij = pairs.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf74c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:05.714916Z",
     "start_time": "2023-02-06T19:36:05.674433Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1i = acdc_standardize_keys(rhoi)\n",
    "rho1i = rho1i.keys_to_properties(['species_neighbor'])\n",
    "gij =  acdc_standardize_keys(gij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a666711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:06.296929Z",
     "start_time": "2023-02-06T19:36:05.716462Z"
    }
   },
   "outputs": [],
   "source": [
    "rho2i = cg_increment(rho1i, rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4565f04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:06.946140Z",
     "start_time": "2023-02-06T19:36:06.942526Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho3i = cg_increment(rho2i, rho1i, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdcb6035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.120033Z",
     "start_time": "2023-02-06T19:36:07.441462Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1ij = cg_increment(rho1i, gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c5d9d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.124429Z",
     "start_time": "2023-02-06T19:36:08.121919Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho2ij = cg_increment(rho2i, gij, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b29eb700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.286694Z",
     "start_time": "2023-02-06T19:36:08.126318Z"
    }
   },
   "outputs": [],
   "source": [
    "features = hamiltonian_features(rho2i, rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eeb7fa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:08.293007Z",
     "start_time": "2023-02-06T19:36:08.289110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 45 blocks\n",
       "keys: ['order_nu' 'inversion_sigma' 'spherical_harmonics_l' 'species_center' 'species_neighbor' 'block_type']\n",
       "           2             1                    0                   1                1              0\n",
       "           2             1                    1                   1                1              0\n",
       "           2             1                    2                   1                1              0\n",
       "        ...\n",
       "           2            -1                    4                   1                1              1\n",
       "           2            -1                    4                   1                1             -1\n",
       "           2            -1                    4                   1                8              2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7bf0c77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.566117Z",
     "start_time": "2023-02-06T19:36:08.482335Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import save\n",
    "save(\"feature.npz\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c7da8",
   "metadata": {},
   "source": [
    "## Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08174c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.576264Z",
     "start_time": "2023-02-06T19:36:09.568334Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_feats(feat, all_blocks=True): \n",
    "    all_norm = 0\n",
    "    normalized_blocks=[]\n",
    "    for block_idx, block in feat: \n",
    "        block_norm = np.linalg.norm(block.values)\n",
    "#         print(block_idx, block_norm)\n",
    "        all_norm = block_norm**2 * len(block.samples) \n",
    "    \n",
    "        newblock = TensorBlock(\n",
    "                        values=block.values/np.sqrt(all_norm ),\n",
    "                        samples=block.samples,\n",
    "                        components=block.components,\n",
    "                        properties= block.properties)                    \n",
    "        normalized_blocks.append(newblock) \n",
    "        \n",
    "    norm_feat = TensorMap(feat.keys, normalized_blocks)\n",
    "    raise Exception (\"Dont do it!\")\n",
    "    return norm_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279a28b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.625063Z",
     "start_time": "2023-02-06T19:36:09.577966Z"
    }
   },
   "outputs": [],
   "source": [
    "#norm_feat = normalize_feats(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e55b753b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-06T19:36:09.674273Z",
     "start_time": "2023-02-06T19:36:09.627414Z"
    }
   },
   "outputs": [],
   "source": [
    "# from equistore.io import save\n",
    "# save(\"./norm_feat.npz\", norm_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27106a59",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4ffec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:47:24.977413Z",
     "start_time": "2023-02-07T10:47:24.965586Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import _labels_from_npz\n",
    "import equistore.operations as operations\n",
    "\n",
    "class HamiltonianDataset(torch.utils.data.Dataset):\n",
    "    #Dataset class\n",
    "    def __init__(self, feature_path, target, fock, frames, feature_nu = 2):\n",
    "        #\n",
    "        self.features = np.load(feature_path, mmap_mode = 'r')\n",
    "        #self.target = np.load(target_path, mmap_mode = 'r') \n",
    "        self.target = target #Uncoupled hamiltonian \n",
    "        #self.target = np.load(fock_path, mmap_mode = 'r')\n",
    "        self.fock = torch.tensor(fock) #fock matrix\n",
    "        self.keys_features = equistore.io._labels_from_npz(self.features[\"keys\"])\n",
    "        self.currentkey = self.target.keys[0]\n",
    "        self.feature_nu = feature_nu\n",
    "        self.frames = frames\n",
    "        \n",
    "        self.allfeatkey = []\n",
    "        for t_key in self.target.keys:\n",
    "            feature_key = self.get_feature_keys(t_key)\n",
    "            self.allfeatkey.append(feature_key)\n",
    "        #Remove Duplicates\n",
    "        nodupes = set()\n",
    "        for x in self.allfeatkey:\n",
    "            if len(x) > 1:\n",
    "                for z in x:\n",
    "                    nodupes.add(tuple(z))\n",
    "            else:\n",
    "                nodupes.add(tuple(x[0]))\n",
    "        \n",
    "        nodupes = np.array(list(nodupes), np.int32)\n",
    "        \n",
    "        self.allfeatkey = Labels(names = self.allfeatkey[0].dtype.names, values = nodupes)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, structure_idx):\n",
    "        feature_block, feature_key = self.generate_feature_block(self.features, structure_idx)        \n",
    "        #samples_filter, target_block_samples = self.get_index_from_idx(self.target.block(self.currentkey).samples, structure_idx)\n",
    "\n",
    "        if self.currentkey is None:\n",
    "            target_block = self.fock[sorted(structure_idx)]\n",
    "        else:\n",
    "            target_block = operations.slice_block(self.target.block(self.currentkey), \n",
    "                                                  samples = Labels(names = ['structure'], \n",
    "                                                    values = (np.array(structure_idx)+1).reshape(-1,1)) )\n",
    "        structure = [self.frames[i] for i in sorted(structure_idx)]\n",
    "        #Modify feature_block to tensormap\n",
    "        feature_map = TensorMap(feature_key, feature_block)\n",
    "        return feature_map, target_block, structure\n",
    "\n",
    "\n",
    "    def get_feature_keys(self,uncoupled_key):\n",
    "        return get_feat_keys_from_uncoupled(uncoupled_key, order_nu = self.feature_nu)\n",
    "    \n",
    "    def generate_feature_block(self, memmap, structure_idx):\n",
    "        #Generate the block from npz file\n",
    "        output = []\n",
    "        if self.currentkey is None:\n",
    "            feature_key = self.allfeatkey\n",
    "                \n",
    "        else:\n",
    "            feature_key = self.get_feature_keys(self.currentkey)\n",
    "            \n",
    "        for key in feature_key:\n",
    "            block_index = list(self.keys_features).index(key)\n",
    "            prefix = f\"blocks/{block_index}/values\"        \n",
    "            block_samples = equistore.io._labels_from_npz(memmap[f\"{prefix}/samples\"])\n",
    "            block_components = []\n",
    "            for i in range(1):\n",
    "                block_components.append(equistore.io._labels_from_npz(memmap[f\"{prefix}/components/{i}\"]))\n",
    "            block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "             \n",
    "\n",
    "            samples_filter, block_samples = self.get_index_from_idx(block_samples, structure_idx)\n",
    "            \n",
    "\n",
    "            block_data = memmap[f\"{prefix}/data\"][samples_filter]\n",
    "            block = TensorBlock(block_data, block_samples, block_components, block_properties)\n",
    "            output.append(block)\n",
    "        return output, feature_key\n",
    "    \n",
    "    def get_n_properties(self, memmap, key):\n",
    "        block_index = list(self.keys_features).index(key)\n",
    "        prefix = f\"blocks/{block_index}/values\"  \n",
    "        block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "        \n",
    "        return len(block_properties)\n",
    "    \n",
    "    def get_index_from_idx(self, block_samples, structure_idx):\n",
    "        #Get samples label from IDX\n",
    "        samples = Labels(names = ['structure'], values = np.array(structure_idx).reshape(-1,1))\n",
    "        \n",
    "        all_samples = block_samples[['structure']].tolist()\n",
    "        set_samples_to_slice = set(samples.tolist())\n",
    "        samples_filter = np.array(\n",
    "            [sample in set_samples_to_slice for sample in all_samples]\n",
    "        )\n",
    "        new_samples = block_samples[samples_filter]\n",
    "\n",
    "        return samples_filter, new_samples\n",
    "    \n",
    "    def collate_output_values(blocks):\n",
    "        feature_out = []\n",
    "        target_out = []\n",
    "        for sample_output in blocks:\n",
    "            feature_block, target_block, structure = sample_output\n",
    "            for z in feature_block:\n",
    "                feature_out.append(torch.tensor(z.values))\n",
    "            target_out.append(torch.tensor(target_block.values))\n",
    "\n",
    "        return feature_out, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ac1d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:47:25.034221Z",
     "start_time": "2023-02-07T10:47:24.979379Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_target_path = \"./test_fock.npz\"\n",
    "test_feature_path = \"./feature.npz\"\n",
    "test = HamiltonianDataset(test_feature_path, blocks, focks, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ffe2cd",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c336070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:01:55.381711Z",
     "start_time": "2023-02-07T11:01:55.378061Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_blocks(block_tuple):\n",
    "    feature_tensor_map, target_block, structure_array = block_tuple[0]\n",
    "    \n",
    "    return feature_tensor_map, target_block, structure_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c4dd925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T12:31:03.632080Z",
     "start_time": "2023-02-07T12:31:03.621532Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "Sampler = torch.utils.data.sampler.RandomSampler(test)\n",
    "#Sampler = torch.utils.data.sampler.SequentialSampler(test)\n",
    "BSampler = torch.utils.data.sampler.BatchSampler(Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "dataloader = DataLoader(test, sampler = BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704bdf2",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79125947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:14:38.877275Z",
     "start_time": "2023-02-07T11:14:38.874272Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "def get_block_samples(t_key, feature_map):\n",
    "    f_key = get_feat_keys_from_uncoupled(t_key, None , 2)\n",
    "    ss = feature_map.block(f_key[0]).samples.copy()\n",
    "    #ss[\"structure\"] = rankdata(np.abs(ss[\"structure\"]), method='dense')\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee997771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T10:58:14.016060Z",
     "start_time": "2023-02-07T10:58:13.979757Z"
    }
   },
   "outputs": [],
   "source": [
    "class HamModel(torch.nn.Module):\n",
    "    #Handles prediction of entire hamiltonian and derived results\n",
    "    def __init__(self, Hamiltonian_Dataset, device, regularization=None, seed=None, layer_size=None):\n",
    "        super().__init__()\n",
    "#         self.features = features \n",
    "#         self.target = target\n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        self.loss_history={}\n",
    "        self.device = device\n",
    "        self.target_keys = Hamiltonian_Dataset.target.keys\n",
    "        self.block_samples = {}\n",
    "        self.block_components = {}\n",
    "        for key in Hamiltonian_Dataset.target.keys:\n",
    "#             _block_type, _a_i, _n_i, _l_i, _a_j, _n_j, _l_j = key\n",
    "#             target_keys = Hamiltonian_Dataset.target.keys[Hamiltonian_Dataset.target.blocks_matching(\n",
    "#                 block_type = _block_type, a_i = _a_i, n_i = _n_i, l_i = _l_i, a_j = _a_j,\n",
    "#                 n_j = _n_j, l_j = _l_j)]\n",
    "            \n",
    "            #self.block_samples[str(key)] = Hamiltonian_Dataset.target.block(key).samples\n",
    "            self.block_components[str(key)] = Hamiltonian_Dataset.target.block(key).components\n",
    "        \n",
    "    \n",
    "            n_inputs = []\n",
    "            model_keys = []\n",
    "\n",
    "            feature_keys = Hamiltonian_Dataset.get_feature_keys(key)\n",
    "            for f_key in feature_keys: \n",
    "                n_features = Hamiltonian_Dataset.get_n_properties(Hamiltonian_Dataset.features, f_key)\n",
    "                n_inputs.append(n_features)\n",
    "                model_keys.append(f_key)\n",
    "                \n",
    "                \n",
    "            n_outputs = np.ones_like(n_inputs)\n",
    "                \n",
    "            self.models[str(key)] = BlockModel(cg.decouple,n_inputs, n_outputs, device, model_keys, key, seed = seed, hidden_layers = layer_size)\n",
    "        self.to(device)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #Ham model uses target keys\n",
    "        pred_blocks = []\n",
    "        for t_key in self.target_keys:\n",
    "            \n",
    "            pred = self.models[str(t_key)].forward(x) #feature_tensormap must correspond to the correct features, model returns block\n",
    "            \n",
    "            #try:\n",
    "#             print (pred.shape)\n",
    "#             print ((2 * t_key['l_i'])+1)\n",
    "#             print ((2 * t_key['l_j']) + 1)\n",
    "            pred_block = TensorBlock(\n",
    "                    values=pred.reshape((-1, (2 * t_key['l_i'])+1, (2 * t_key['l_j']) + 1, 1)), #?\n",
    "                    samples = get_block_samples(t_key, x),\n",
    "#                     samples = x.block(t_key).samples,\n",
    "                    components = self.block_components[str(t_key)] ,\n",
    "                    properties= Labels([\"dummy\"], np.asarray([[0]], dtype=np.int32))\n",
    "                )\n",
    "#             except:\n",
    "#                 print (t_key)\n",
    "#                 print (pred)\n",
    "#                 print (self.block_samples[str(t_key)])\n",
    "#                 print (self.block_components[str(t_key)])\n",
    "                \n",
    "            pred_blocks.append(pred_block)\n",
    "        pred_hamiltonian = TensorMap(self.target_keys, pred_blocks)\n",
    "        return(pred_hamiltonian)\n",
    "    \n",
    "    #write/fix forward function for train_indiv\n",
    "    \n",
    "    def train_individual(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #Iterates through the keys of self.model, then for each key we will fit self.model[key] with data[key]\n",
    "        total = len(self.models)\n",
    "        for index, t_key in enumerate(self.target_keys):\n",
    "            print (\"Now training on Block {} of {}\".format(index, total))\n",
    "            train_dataloader.dataset.currentkey = t_key\n",
    "            \n",
    "            loss_history_key = self.models[str(t_key)].fit(train_dataloader, loss_function, optimizer_type, lr, regularization_dict[str(t_key)], n_epochs)\n",
    "\n",
    "            self.loss_history[str(t_key)] = loss_history_key\n",
    "    \n",
    "    def train_collective(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #for every loop through target keys, we predict the corresponding block and assemble the final hamiltonian\n",
    "        optimizer_dict = {}\n",
    "        if optimizer_type == \"Adam\":\n",
    "            for key in train_dataloader.dataset.target.keys:\n",
    "                optimizer_dict[str(key)] = torch.optim.Adam(self.models[str(key)].parameters(), lr = lr, weight_decay = regularization_dict[str(key)])\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 200\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "#             for key in train_dataloader.dataset.target.keys:\n",
    "#                 optimizer_dict[str(key)] = torch.optim.LBFGS(self.models[str(key)].parameters(), lr = lr)\n",
    "            optimizer_dict[0] = torch.optim.LBFGS(self.models.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 30\n",
    "            tol = 0\n",
    "            history_step = 10                \n",
    "        scheduler_dict = {}\n",
    "        scheduler_dict[0] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_dict[0], factor = 0.1, patience = scheduler_threshold)\n",
    "#         for key in train_dataloader.dataset.target.keys:\n",
    "#             scheduler_dict[str(key)] = torch.optim.lr_scheduler.StepLR(optimizer_dict[str(key)], scheduler_threshold, gamma = 0.5)\n",
    "\n",
    "        reg_weights = torch.tensor(list(regularization_dict.values()))\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            train_dataloader.dataset.currentkey = None\n",
    "            \n",
    "            for x_data, y_data, structure in train_dataloader: \n",
    "                self.collective_zg(optimizer_dict)\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.collective_zg(optimizer_dict)\n",
    "                        _pred = self.forward(x_data)\n",
    "                        _pred_loss = loss_function(_pred, y_data, structure, orbs)       \n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                          \n",
    "                        _reg_loss = self.get_regression_values(reg_weights) #Only works for 1 layer #Need to change!!\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    for value in optimizer_dict.values():\n",
    "                        value.step(closure)\n",
    "#                     for param in self.parameters():\n",
    "#                         print (param.grad)\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data, structure, orbs)  \n",
    "#                     reg_loss = torch.sum(torch.pow(self.nn.weight,2))#Only works for 1 layer\n",
    "                    new_loss = pred_loss \n",
    "                    new_loss.backward()\n",
    "                    self.collective_step(optimizer_dict)\n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in train_dataloader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data, structure, orbs)   #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg_weights)#Only works for 1 layer\n",
    "                new_loss = pred_loss + reg_loss\n",
    "                for scheduler in scheduler_dict.values():\n",
    "                    scheduler.step(new_loss)\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training             \n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "        \n",
    "    def collective_step(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.step()\n",
    "            \n",
    "    def collective_zg(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.zero_grad()\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f59845d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T11:30:42.198252Z",
     "start_time": "2023-02-07T11:30:42.184392Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlockModel(torch.nn.Module): #Currently only 1 model per block\n",
    "    def __init__(self, reconstruction_function, inputSize, outputSize, device, keys, target_key, seed = None, hidden_layers = None):\n",
    "        super().__init__()\n",
    "        self.reconstruction_function = reconstruction_function\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.device = device\n",
    "        self.keys = keys\n",
    "        self.target_key = target_key\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.initialize_model(seed)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        for index, key in enumerate(self.keys):\n",
    "#             if key['spherical_harmonics_l'] == 0:\n",
    "#                 self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = True)\n",
    "#             else:\n",
    "            self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = False)\n",
    "        \n",
    "    def forward(self, feature_tensormap):\n",
    "        #Block model uses feature keys\n",
    "        pred_values = {}\n",
    "        for key in self.keys:\n",
    "            feature_values = feature_tensormap.block(key).values\n",
    "            d1, d2, d3 = feature_values.shape\n",
    "            L = int((d2 -1)/2)\n",
    "            pred = self.models[str(key)](torch.tensor(feature_values.reshape(d1 * d2, d3)))\n",
    "            pred = pred.reshape(d1,d2)\n",
    "            pred_values[L] = pred\n",
    "        \n",
    "        pred_block_values = self.reconstruction_function({(self.target_key['l_i'],self.target_key['l_j']) : pred_values})\n",
    "        \n",
    "        #DOES NOT WORK FOR BATCHES\n",
    "\n",
    "        #pred = torch.hstack(pred_values)\n",
    "        #pred = self.reconstruction_function(pred_values)\n",
    "        return pred_block_values \n",
    "\n",
    "    \n",
    "    def fit(self,traindata_loader, loss_function, optimizer_type, lr, reg, n_epochs):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 50\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "            optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 10\n",
    "            tol = 0\n",
    "            history_step = 10\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = scheduler_threshold)      \n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            \n",
    "            for x_data, y_data, structure in traindata_loader: \n",
    "                optimizer.zero_grad()\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        _pred = self.forward(x_data)                                        \n",
    "                        _pred_loss = loss_function(_pred, y_data.values)\n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                 \n",
    "                        _reg_loss = self.get_regression_values(reg.item()) #Only works for 1 layer\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    optimizer.step(closure)\n",
    "\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data.values)\n",
    "                    #reg_loss = self.get_regression_values(reg.item())#Only works for 1 layer\n",
    "                    new_loss = pred_loss #+ reg_loss\n",
    "                    new_loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in traindata_loader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data.values) #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg.item()) \n",
    "                new_loss = pred_loss + reg_loss\n",
    "                scheduler.step(new_loss)\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a88f0b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3049caf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-07T12:31:58.604640Z",
     "start_time": "2023-02-07T12:31:58.590585Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1]) \n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return torch.mean(MSE)*(Hartree)**2\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    #fock = torch.tensor(focks)\n",
    "    #print (mse_full_blockwise(pred_blocks, blocks, frame, orbs))\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)/torch.numel(fock[i])\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2#, mse_loss\n",
    "\n",
    "def mse_full_blockwise(pred_blocks, block_tensormap, frame, orbs):\n",
    "    indiv_mse = torch.zeros(1)\n",
    "    for key,block in pred_blocks:\n",
    "        #MSE = ((torch.linalg.norm(block_tensormap.block(key).values-block.values))**2)\n",
    "        print (key)\n",
    "        print (((torch.linalg.norm(block_tensormap.block(key).values- block.values))**2)/ torch.numel(block_tensormap.block(key).values))\n",
    "        print (torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values))\n",
    "        MSE = torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values)\n",
    "        indiv_mse += MSE\n",
    "    \n",
    "    return (indiv_mse/len(frame))*(Hartree)**2, indiv_mse\n",
    "    \n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36b78810",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HamModel(test, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e8cb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structure in dataloader:    \n",
    "    pred = model(x_data)\n",
    "    loss = mse_full(pred, y_data, structure, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0dc1fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(58.6756, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd7e146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS(\n",
    "        model.parameters(),\n",
    "        lr=1,  line_search_fn=\"strong_wolfe\",\n",
    "        history_size=256, tolerance_grad=1e-20, tolerance_change=1e-20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e818c7c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 58.67562750148212\n",
      "1 0.08448594716969758\n",
      "2 0.011122983672122626\n",
      "3 0.0017490690556406475\n",
      "4 0.0007707432628983982\n",
      "5 0.00035613772798267633\n",
      "6 0.00019079734697538347\n",
      "7 0.00010463990698828119\n",
      "8 7.967453392746054e-05\n",
      "9 6.340755366744068e-05\n",
      "10 4.783718713066272e-05\n",
      "11 3.771920151927733e-05\n",
      "12 3.101730919731857e-05\n",
      "13 2.6016503014989257e-05\n",
      "14 2.4250601808216494e-05\n",
      "15 2.349687869885814e-05\n",
      "16 2.21911555933651e-05\n",
      "17 2.0571559143536017e-05\n",
      "18 1.9006407754436844e-05\n",
      "19 1.8209760994921776e-05\n",
      "20 1.7474110310831927e-05\n",
      "21 1.6522667309169702e-05\n",
      "22 1.5382336997459258e-05\n",
      "23 1.4641263226995018e-05\n",
      "24 1.3821703786109164e-05\n",
      "25 1.3084876501773705e-05\n",
      "26 1.2639149372239359e-05\n",
      "27 1.2206535001727158e-05\n",
      "28 1.187347460558182e-05\n",
      "29 1.117670298304542e-05\n",
      "30 1.0737817464176798e-05\n",
      "31 1.0482922494394057e-05\n",
      "32 1.0193032517804637e-05\n",
      "33 9.871147006782425e-06\n",
      "34 9.755357024899737e-06\n",
      "35 9.44886573990565e-06\n",
      "36 9.17043529708451e-06\n",
      "37 8.996338578916477e-06\n",
      "38 8.67912392622481e-06\n",
      "39 8.457278019726467e-06\n",
      "40 8.357209915959465e-06\n",
      "41 8.181816423975327e-06\n",
      "42 8.003464851162035e-06\n",
      "43 7.855879485870038e-06\n",
      "44 7.653605530433483e-06\n",
      "45 7.371312623652165e-06\n",
      "46 6.857513048161718e-06\n",
      "47 6.436431449026387e-06\n",
      "48 6.241564053329417e-06\n",
      "49 6.072919578463483e-06\n",
      "50 5.930070064792912e-06\n",
      "51 5.7871475092085746e-06\n",
      "52 5.715450461174616e-06\n",
      "53 5.652260701560924e-06\n",
      "54 5.5554290961313755e-06\n",
      "55 5.538150026146843e-06\n",
      "56 5.5378579141622055e-06\n",
      "57 5.537569556197893e-06\n",
      "58 5.537282564843613e-06\n",
      "59 5.536996544820823e-06\n",
      "60 5.536711409698297e-06\n",
      "61 5.536427135798704e-06\n",
      "62 5.536143713411003e-06\n",
      "63 5.5358611362404365e-06\n",
      "64 5.535579398961363e-06\n",
      "65 5.5352984965797845e-06\n",
      "66 5.535018424246325e-06\n",
      "67 5.534739177191e-06\n",
      "68 5.534460750696443e-06\n",
      "69 5.534183140089202e-06\n",
      "70 5.53390634073356e-06\n",
      "71 5.5336303480253435e-06\n",
      "72 5.5333551573979175e-06\n",
      "73 5.533080764315178e-06\n",
      "74 5.532807164272589e-06\n",
      "75 5.53253435279913e-06\n",
      "76 5.5322623254546585e-06\n",
      "77 5.531991077829047e-06\n",
      "78 5.531720605544744e-06\n",
      "79 5.531450904255452e-06\n",
      "80 5.5311819696441785e-06\n",
      "81 5.530913797424974e-06\n",
      "82 5.530646383341375e-06\n",
      "83 5.530379723168198e-06\n",
      "84 5.530113812707903e-06\n",
      "85 5.5298486477923645e-06\n",
      "86 5.529584224284932e-06\n",
      "87 5.529320538074519e-06\n",
      "88 5.529057585081497e-06\n",
      "89 5.528795361252664e-06\n",
      "90 5.528533862565242e-06\n",
      "91 5.5282730850221674e-06\n",
      "92 5.528013024655419e-06\n",
      "93 5.5277536775244305e-06\n",
      "94 5.527495039714848e-06\n",
      "95 5.5272371073413105e-06\n",
      "96 5.526979876543748e-06\n",
      "97 5.526723343488418e-06\n",
      "98 5.526467504369567e-06\n",
      "99 5.526212355406818e-06\n",
      "100 5.5259578928468325e-06\n",
      "101 5.525704112959921e-06\n",
      "102 5.525451012043214e-06\n",
      "103 5.525198586419705e-06\n",
      "104 5.524946832437917e-06\n",
      "105 5.5246957464707625e-06\n",
      "106 5.524445324914908e-06\n",
      "107 5.524195564193732e-06\n",
      "108 5.523946460754741e-06\n",
      "109 5.523698011068582e-06\n",
      "110 5.5234502116308605e-06\n",
      "111 5.523203058960724e-06\n",
      "112 5.5229565496014715e-06\n",
      "113 5.5227106801193066e-06\n",
      "114 5.5224654471038934e-06\n",
      "115 5.522220847168212e-06\n",
      "116 5.521976876949006e-06\n",
      "117 5.521733533104344e-06\n",
      "118 5.521490812316133e-06\n",
      "119 5.521248711287685e-06\n",
      "120 5.521007226744628e-06\n",
      "121 5.520766355435439e-06\n",
      "122 5.520526094130455e-06\n",
      "123 5.5202864396220194e-06\n",
      "124 5.520047388723194e-06\n",
      "125 5.519808938267876e-06\n",
      "126 5.519571085112267e-06\n",
      "127 5.519333826134047e-06\n",
      "128 5.519097158230325e-06\n",
      "129 5.518861078320216e-06\n",
      "130 5.518625583344308e-06\n",
      "131 5.518390670260759e-06\n",
      "132 5.518156336050561e-06\n",
      "133 5.517922577713463e-06\n",
      "134 5.517689392270354e-06\n",
      "135 5.517456776761746e-06\n",
      "136 5.517224728245866e-06\n",
      "137 5.516993243802561e-06\n",
      "138 5.516762320530456e-06\n",
      "139 5.516531955548161e-06\n",
      "140 5.516302145990677e-06\n",
      "141 5.516072889014602e-06\n",
      "142 5.51584418179335e-06\n",
      "143 5.515616021520975e-06\n",
      "144 5.515388405409327e-06\n",
      "145 5.515161330689609e-06\n",
      "146 5.5149347946084e-06\n",
      "147 5.514708794432708e-06\n",
      "148 5.51448332744726e-06\n",
      "149 5.514258390954435e-06\n",
      "150 5.514033982273362e-06\n",
      "151 5.513810098741508e-06\n",
      "152 5.51358673771239e-06\n",
      "153 5.513363896558396e-06\n",
      "154 5.513141572668023e-06\n",
      "155 5.5129197634491875e-06\n",
      "156 5.512698466323211e-06\n",
      "157 5.512477678729512e-06\n",
      "158 5.512257398125029e-06\n",
      "159 5.5120376219805346e-06\n",
      "160 5.511818347788162e-06\n",
      "161 5.511599573051746e-06\n",
      "162 5.511381295293284e-06\n",
      "163 5.5111635120494155e-06\n",
      "164 5.510946220873931e-06\n",
      "165 5.510729419336396e-06\n",
      "166 5.510513105020232e-06\n",
      "167 5.5102972755267935e-06\n",
      "168 5.510081928471038e-06\n",
      "169 5.509867061485423e-06\n",
      "170 5.509652672214908e-06\n",
      "171 5.5094387583208335e-06\n",
      "172 5.509225317478979e-06\n",
      "173 5.509012347381093e-06\n",
      "174 5.508799845733056e-06\n",
      "175 5.5085878102544865e-06\n",
      "176 5.508376238680267e-06\n",
      "177 5.50816512876022e-06\n",
      "178 5.507954478257584e-06\n",
      "179 5.507744284949451e-06\n",
      "180 5.507534546628249e-06\n",
      "181 5.50732526109953e-06\n",
      "182 5.507116426183296e-06\n",
      "183 5.506908039714024e-06\n",
      "184 5.506700099537312e-06\n",
      "185 5.506492603514451e-06\n",
      "186 5.506285549519086e-06\n",
      "187 5.50607893543851e-06\n",
      "188 5.505872759173481e-06\n",
      "189 5.505667018638156e-06\n",
      "190 5.505461711759436e-06\n",
      "191 5.5052568364765206e-06\n",
      "192 5.50505239074387e-06\n",
      "193 5.50484837252529e-06\n",
      "194 5.504644779800759e-06\n",
      "195 5.504441610561233e-06\n",
      "196 5.504238862810449e-06\n",
      "197 5.504036534562982e-06\n",
      "198 5.503834623847428e-06\n",
      "199 5.503633128703771e-06\n",
      "200 5.503432047185466e-06\n",
      "201 5.503231377355601e-06\n",
      "202 5.503031117291674e-06\n",
      "203 5.502831265081329e-06\n",
      "204 5.502631818826184e-06\n",
      "205 5.502432776637573e-06\n",
      "206 5.502234136639355e-06\n",
      "207 5.5020358969662365e-06\n",
      "208 5.501838055764322e-06\n",
      "209 5.50164061119071e-06\n",
      "210 5.501443561415724e-06\n",
      "211 5.501246904619011e-06\n",
      "212 5.501050638992157e-06\n",
      "213 5.500854762739829e-06\n",
      "214 5.500659274073401e-06\n",
      "215 5.500464171217288e-06\n",
      "216 5.500269452406883e-06\n",
      "217 5.500075115888081e-06\n",
      "218 5.499881159917089e-06\n",
      "219 5.499687582761073e-06\n",
      "220 5.499494382698004e-06\n",
      "221 5.499301558015428e-06\n",
      "222 5.499109107011793e-06\n",
      "223 5.498917027996357e-06\n",
      "224 5.498725319287363e-06\n",
      "225 5.498533979215279e-06\n",
      "226 5.498343006118102e-06\n",
      "227 5.498152398344809e-06\n",
      "228 5.4979621542539204e-06\n",
      "229 5.497772272214445e-06\n",
      "230 5.4975827506055294e-06\n",
      "231 5.497393587814412e-06\n",
      "232 5.497204782239253e-06\n",
      "233 5.497016332288696e-06\n",
      "234 5.496828236378418e-06\n",
      "235 5.496640492933371e-06\n",
      "236 5.496453100391226e-06\n",
      "237 5.4962660571965136e-06\n",
      "238 5.496079361803832e-06\n",
      "239 5.495893012674846e-06\n",
      "240 5.495707008282598e-06\n",
      "241 5.495521347108702e-06\n",
      "242 5.495336027644147e-06\n",
      "243 5.49515104838739e-06\n",
      "244 5.494966407847806e-06\n",
      "245 5.494782104540918e-06\n",
      "246 5.4945981369922465e-06\n",
      "247 5.494414503736192e-06\n",
      "248 5.494231203315659e-06\n",
      "249 5.494048234283077e-06\n",
      "250 5.4938655951972145e-06\n",
      "251 5.493683284627009e-06\n",
      "252 5.493501301149562e-06\n",
      "253 5.493319643348746e-06\n",
      "254 5.493138309818055e-06\n",
      "255 5.492957299159094e-06\n",
      "256 5.49277660998142e-06\n",
      "257 5.492596240902381e-06\n",
      "258 5.492416190547226e-06\n",
      "259 5.4922364575490825e-06\n",
      "260 5.492057040548754e-06\n",
      "261 5.4918779381966126e-06\n",
      "262 5.491699149149589e-06\n",
      "263 5.49152067207152e-06\n",
      "264 5.491342505635291e-06\n",
      "265 5.491164648521011e-06\n",
      "266 5.490987099416947e-06\n",
      "267 5.490809857017619e-06\n",
      "268 5.490632920025825e-06\n",
      "269 5.490456287152383e-06\n",
      "270 5.4902799571127805e-06\n",
      "271 5.490103928632872e-06\n",
      "272 5.489928200444821e-06\n",
      "273 5.48975277128892e-06\n",
      "274 5.489577639910357e-06\n",
      "275 5.489402805063044e-06\n",
      "276 5.489228265506635e-06\n",
      "277 5.489054020009314e-06\n",
      "278 5.488880067345806e-06\n",
      "279 5.488706406298261e-06\n",
      "280 5.488533035653398e-06\n",
      "281 5.488359954208863e-06\n",
      "282 5.488187160764634e-06\n",
      "283 5.488014654131627e-06\n",
      "284 5.4878424331232275e-06\n",
      "285 5.487670496562495e-06\n",
      "286 5.487498843278403e-06\n",
      "287 5.487327472103847e-06\n",
      "288 5.487156381882753e-06\n",
      "289 5.486985571462659e-06\n",
      "290 5.486815039697869e-06\n",
      "291 5.486644785449709e-06\n",
      "292 5.4864748075853714e-06\n",
      "293 5.486305104978386e-06\n",
      "294 5.486135676508876e-06\n",
      "295 5.4859665210629435e-06\n",
      "296 5.4857976375332935e-06\n",
      "297 5.485629024818758e-06\n",
      "298 5.485460681822263e-06\n",
      "299 5.485292607454412e-06\n"
     ]
    }
   ],
   "source": [
    "all_losses = []\n",
    "for epoch in range(300):\n",
    "    dataloader.dataset.currentkey = None\n",
    "    for x_data, y_data, structure in dataloader:    \n",
    "        def single_step():\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x_data)\n",
    "            loss = mse_full(pred, y_data, structure, orbs)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(single_step)\n",
    "\n",
    "        all_losses.append(loss.item())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, loss.item()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5122692",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb10f943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:53.043094Z",
     "start_time": "2022-12-29T10:15:52.986380Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load test set\n",
    "test_frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\"50:80\")\n",
    "# frames2 = ase.io.read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "test_frames = test_frames1 #+ frames2\n",
    "for f in test_frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "\n",
    "# test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:60]\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "\n",
    "test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:80]\n",
    "test_focks = test_focks1.astype(np.float64)\n",
    "# test_focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[50:80]\n",
    "# test_overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[50:80]\n",
    "\n",
    "# test_orthogonal = []\n",
    "# for i in range(len(test_focks)): \n",
    "#     test_focks[i] = fix_pyscf_l1(test_focks[i],test_frames[i], orbs)\n",
    "#     test_overlap[i] = fix_pyscf_l1(test_overlap[i],test_frames[i], orbs)\n",
    "#     test_orthogonal.append(lowdin_orthogonalize(test_focks[i], test_overlap[i]))\n",
    "# test_focks = np.asarray(test_orthogonal, dtype=np.float64)\n",
    "    \n",
    "test_blocks = dense_to_blocks(test_focks, test_frames, orbs)\n",
    "test_fock_bc = couple_blocks(test_blocks, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "515d2062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:54.725559Z",
     "start_time": "2022-12-29T10:15:53.932535Z"
    }
   },
   "outputs": [],
   "source": [
    "test_rhoi = spex.compute(test_frames)\n",
    "test_gij = pairs.compute(test_frames)\n",
    "test_rho1i = acdc_standardize_keys(test_rhoi)\n",
    "test_rho1i = test_rho1i.keys_to_properties(['species_neighbor'])\n",
    "test_gij =  acdc_standardize_keys(test_gij)\n",
    "test_rho2i = cg_increment(test_rho1i, test_rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "test_rho1ij = cg_increment(test_rho1i, test_gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "test_features = hamiltonian_features(test_rho2i, test_rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "695e248b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.390807Z",
     "start_time": "2022-12-29T10:15:54.727705Z"
    }
   },
   "outputs": [],
   "source": [
    "#from equistore.io import save\n",
    "save(\"test_feature.npz\", test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5ee3635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.442916Z",
     "start_time": "2022-12-29T10:15:55.402080Z"
    }
   },
   "outputs": [],
   "source": [
    "test_feature_path = \"./test_feature.npz\"\n",
    "testing = HamiltonianDataset(test_feature_path, test_blocks, test_focks, test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e979a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.726557Z",
     "start_time": "2022-12-29T10:15:55.722442Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "test_Sampler = torch.utils.data.sampler.SequentialSampler(testing)\n",
    "test_BSampler = torch.utils.data.sampler.BatchSampler(test_Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "test_dataloader = DataLoader(testing, sampler = test_BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d07f4e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8114e-05, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testing.currentkey = None\n",
    "test_dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in test_dataloader:\n",
    "    t_pred = model(x_data)\n",
    "    print (mse_full(t_pred, torch.tensor(test_focks), structures, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad601d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599dea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "786px",
    "left": "27px",
    "top": "111.133px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
