{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2052fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T18:06:12.299279Z",
     "start_time": "2022-12-28T18:05:48.101846Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.03it/s, lowest_loss=0.000382, pred_loss=0.000382, trigger=11]\n",
      "Epoch: 19: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:11<00:00,  1.79it/s, lowest_loss=62.1, pred_loss=62.1, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training on Block 0 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 49: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 32.95it/s, lowest_loss=0.655, pred_loss=0.655, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training on Block 1 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31:  62%|████████████████████████████████████████████████████████████████████▊                                          | 31/50 [00:00<00:00, 63.71it/s, lowest_loss=0.00694, pred_loss=0.00694, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.006936603382617929\n",
      "Now training on Block 2 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31:  62%|██████████████████████████████████████████████████████████████████████                                           | 31/50 [00:00<00:00, 40.58it/s, lowest_loss=0.0505, pred_loss=0.0505, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.050496892984710724\n",
      "Now training on Block 3 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31:  62%|███████████████████████████████████████████████████████████████████████▎                                           | 31/50 [00:00<00:00, 84.74it/s, lowest_loss=0.213, pred_loss=0.213, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.2134932704413064\n",
      "Now training on Block 4 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31:  62%|██████████████████████████████████████████████████████████████████████                                           | 31/50 [00:00<00:00, 78.31it/s, lowest_loss=0.0549, pred_loss=0.0549, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.054929677128490416\n",
      "Now training on Block 5 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 49: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 36.30it/s, lowest_loss=0.157, pred_loss=0.157, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training on Block 6 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31:  62%|█████████████████████████████████████████████████████████████████████▍                                          | 31/50 [00:00<00:00, 100.36it/s, lowest_loss=0.0408, pred_loss=0.0408, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.04077760195671071\n",
      "Train error for mse_full is 0.0003818065020378808\n",
      "Test error for mse_full is 0.4781943675013342\n",
      "Train error for mse_eigval is 62.067829012996114\n",
      "Test error for mse_full is 58.326143569702374\n",
      "Train error for mse_indiv is 372.48666252436146\n",
      "Test error for mse_indiv is 350.03224339271117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ase.io\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "# from rascal.representations import SphericalExpansion\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from ase.units import Hartree\n",
    "\n",
    "from torch_hamiltonian_utils.torch_cg import ClebschGordanReal\n",
    "from torch_hamiltonian_utils.torch_hamiltonians import fix_pyscf_l1, lowdin_orthogonalize, dense_to_blocks, blocks_to_dense, couple_blocks, decouple_blocks, hamiltonian_features\n",
    "from torch_hamiltonian_utils.torch_builder import TensorBuilder\n",
    "\n",
    "import equistore\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "from equistore_utils.librascal import  RascalSphericalExpansion, RascalPairExpansion\n",
    "from equistore_utils.acdc_mini import acdc_standardize_keys, cg_increment, cg_combine\n",
    "from equistore_utils.model_hamiltonian import get_feat_keys, get_feat_keys_from_uncoupled \n",
    "\n",
    "import importlib\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "\n",
    "def get_feat_keys_from_uncoupled(block_keys, sigma=None, order_nu=None):\n",
    "    \"\"\"Map UNCOUPLED block keys to corresponding feature key. take as extra input the sigma, nu value if required.\n",
    "    sigma=0 returns all possible sigma values at given 'nu'\"\"\"\n",
    "    blocktype, species1, n1, l1, species2, n2, l2 = block_keys\n",
    "    feat_blocktype = blocktype\n",
    "    keys_L=[]\n",
    "    for L in range(abs(l1-l2), l1+l2+1):\n",
    "        if sigma is None:\n",
    "            z = (l1+l2+L)%2\n",
    "            inv_sigma = 1 - 2*z\n",
    "        elif abs(sigma)==1:\n",
    "            inv_sigma = sigma\n",
    "        else: \n",
    "            raise(\"Please check sigma value, it should be +1 or -1\")\n",
    "        \n",
    "        if blocktype == 1 and n1 == n2 and l1 == l2:\n",
    "            feat_blocktype = inv_sigma\n",
    "                 \n",
    "        if inv_sigma == -1 and blocktype == 0 and n1 == n2 and l1 == l2:\n",
    "            continue     \n",
    "        \n",
    "        keys_L.append([(order_nu, inv_sigma, L, species1, species2, feat_blocktype)])\n",
    "    #     feat= (blocktype, L,sigma,species1, species2)\n",
    "    feat = Labels([\"order_nu\", \"inversion_sigma\", \"spherical_harmonics_l\", \"species_center\", \"species_neighbor\", \"block_type\"], np.asarray(keys_L, dtype=np.int32).reshape(-1,6))\n",
    "    return feat\n",
    "\n",
    "\n",
    "def lowdin_orthogonalize(fock, s):\n",
    "    \"\"\"\n",
    "    lowdin orthogonalization of a fock matrix computing the square root of the overlap matrix\n",
    "    \"\"\"\n",
    "    eva, eve = np.linalg.eigh(s)\n",
    "    sm12 = eve @ np.diag(1.0/np.sqrt(eva)) @ eve.T\n",
    "    return sm12 @ fock @ sm12\n",
    "\n",
    "\n",
    "frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\":5\")\n",
    "frames = frames1 #+ frames2\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "#jorbs = json.loads(json.load(open('data/water-hamiltonian/orbs_def2_water.json', \"r\")))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1, \"C\":6}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]\n",
    "\n",
    "#focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[:len(frames1)]\n",
    "#overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[:len(frames1)]\n",
    "\n",
    "#orthogonal = []\n",
    "#for i in range(len(focks)): \n",
    "#    focks[i] = fix_pyscf_l1(focks[i],frames[i], orbs)\n",
    "#    overlap[i] = fix_pyscf_l1(overlap[i],frames[i], orbs)\n",
    "#    orthogonal.append(lowdin_orthogonalize(focks[i], overlap[i]))\n",
    "\n",
    "focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames1)]\n",
    "focks = focks1\n",
    "\n",
    "\n",
    "cg = ClebschGordanReal(8)\n",
    "\n",
    "blocks = dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)\n",
    "# ## Feature computation\n",
    "\n",
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 4.0,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 4,\n",
    "    \"gaussian_sigma_constant\" : 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\":  False,\n",
    "}\n",
    "\n",
    "spex = RascalSphericalExpansion(rascal_hypers)\n",
    "rhoi = spex.compute(frames)\n",
    "\n",
    "lmax = rascal_hypers[\"max_angular\"]\n",
    "pairs = RascalPairExpansion(rascal_hypers)\n",
    "gij = pairs.compute(frames)\n",
    "rho1i = acdc_standardize_keys(rhoi)\n",
    "rho1i.keys_to_properties(['species_neighbor'])\n",
    "gij =  acdc_standardize_keys(gij)\n",
    "\n",
    "\n",
    "rho2i = cg_increment(rho1i, rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "#rho3i = cg_increment(rho2i, rho1i, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "rho1ij = cg_increment(rho1i, gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "#rho2ij = cg_increment(rho2i, gij, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "features = hamiltonian_features(rho2i, rho1ij)\n",
    "\n",
    "from equistore.io import save\n",
    "save(\"feature.npz\", features)\n",
    "\n",
    "\n",
    "def normalize_feats(feat, all_blocks=True): \n",
    "    all_norm = 0\n",
    "    for block_idx, block in feat: \n",
    "        block_norm = np.linalg.norm(block.values)\n",
    "#         print(block_idx, block_norm)\n",
    "        all_norm += block_norm**2\n",
    "    normalized_blocks=[]\n",
    "    for block_idx, block in feat: \n",
    "        newblock = TensorBlock(\n",
    "                        values=block.values/np.sqrt(all_norm ),\n",
    "                        samples=block.samples,\n",
    "                        components=block.components,\n",
    "                        properties= block.properties)\n",
    "                    \n",
    "        normalized_blocks.append(newblock) \n",
    "        \n",
    "    norm_feat = TensorMap(feat.keys, normalized_blocks)\n",
    "    return norm_feat\n",
    "\n",
    "#norm_feat = normalize_feats(features)\n",
    "\n",
    "#save(\"./norm_feat.npz\", norm_feat)\n",
    "\n",
    "from equistore.io import _labels_from_npz\n",
    "import equistore.operations as operations\n",
    "\n",
    "class HamiltonianDataset(torch.utils.data.Dataset):\n",
    "    #Dataset class\n",
    "    def __init__(self, feature_path, target, frames, feature_nu = 2):\n",
    "        #\n",
    "        self.features = np.load(feature_path, mmap_mode = 'r')\n",
    "        #self.target = np.load(target_path, mmap_mode = 'r') \n",
    "        self.target = target #Uncoupled hamiltonian \n",
    "        self.keys_features = equistore.io._labels_from_npz(self.features[\"keys\"])\n",
    "        self.currentkey = self.target.keys[0]\n",
    "        self.feature_nu = feature_nu\n",
    "        self.frames = frames\n",
    "        \n",
    "        self.allfeatkey = []\n",
    "        for t_key in self.target.keys:\n",
    "            feature_key = self.get_feature_keys(t_key)\n",
    "            self.allfeatkey.append(feature_key)\n",
    "        #Remove Duplicates\n",
    "        nodupes = set()\n",
    "        for x in self.allfeatkey:\n",
    "            if len(x) > 1:\n",
    "                for z in x:\n",
    "                    nodupes.add(tuple(z))\n",
    "            else:\n",
    "                nodupes.add(tuple(x[0]))\n",
    "        \n",
    "        nodupes = np.array(list(nodupes), np.int32)\n",
    "        \n",
    "        self.allfeatkey = Labels(names = self.allfeatkey[0].dtype.names, values = nodupes)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, structure_idx):\n",
    "        feature_block, feature_key = self.generate_feature_block(self.features, structure_idx)        \n",
    "        #samples_filter, target_block_samples = self.get_index_from_idx(self.target.block(self.currentkey).samples, structure_idx)\n",
    "\n",
    "        if self.currentkey is None:\n",
    "            t_blocks = []\n",
    "            for _, block in self.target:            \n",
    "                t_block = operations.slice_block(block, samples = Labels(names = ['structure'], values = (np.array(structure_idx)+1).reshape(-1,1)) )\n",
    "                t_blocks.append(t_block)\n",
    "            target_block = TensorMap(self.target.keys, t_blocks)\n",
    "        else:\n",
    "            target_block = operations.slice_block(self.target.block(self.currentkey), samples = Labels(names = ['structure'], values = (np.array(structure_idx)+1).reshape(-1,1)) )\n",
    "        structure = [self.frames[i] for i in structure_idx]\n",
    "        #Modify feature_block to tensormap\n",
    "        feature_map = TensorMap(feature_key, feature_block)\n",
    "        return feature_map, target_block, structure\n",
    "\n",
    "\n",
    "    def get_feature_keys(self,uncoupled_key):\n",
    "        return get_feat_keys_from_uncoupled(uncoupled_key, order_nu = self.feature_nu)\n",
    "    \n",
    "    def generate_feature_block(self, memmap, structure_idx):\n",
    "        #Generate the block from npz file\n",
    "        output = []\n",
    "        if self.currentkey is None:\n",
    "            feature_key = self.allfeatkey\n",
    "                \n",
    "        else:\n",
    "            feature_key = self.get_feature_keys(self.currentkey)\n",
    "            \n",
    "        for key in feature_key:\n",
    "            block_index = list(self.keys_features).index(key)\n",
    "            prefix = f\"blocks/{block_index}/values\"        \n",
    "            block_samples = equistore.io._labels_from_npz(memmap[f\"{prefix}/samples\"])\n",
    "            block_components = []\n",
    "            for i in range(1):\n",
    "                block_components.append(equistore.io._labels_from_npz(memmap[f\"{prefix}/components/{i}\"]))\n",
    "            block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "             \n",
    "\n",
    "            samples_filter, block_samples = self.get_index_from_idx(block_samples, structure_idx)\n",
    "\n",
    "            block_data = memmap[f\"{prefix}/data\"][samples_filter]\n",
    "            block = TensorBlock(block_data, block_samples, block_components, block_properties)\n",
    "            output.append(block)\n",
    "        return output, feature_key\n",
    "    \n",
    "    def get_n_properties(self, memmap, key):\n",
    "        block_index = list(self.keys_features).index(key)\n",
    "        prefix = f\"blocks/{block_index}/values\"  \n",
    "        block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "        \n",
    "        return len(block_properties)\n",
    "    \n",
    "    def get_index_from_idx(self, block_samples, structure_idx):\n",
    "        #Get samples label from IDX\n",
    "        samples = Labels(names = ['structure'], values = np.array(structure_idx).reshape(-1,1))\n",
    "        \n",
    "        all_samples = block_samples[['structure']].tolist()\n",
    "        set_samples_to_slice = set(samples.tolist())\n",
    "        samples_filter = np.array(\n",
    "            [sample in set_samples_to_slice for sample in all_samples]\n",
    "        )\n",
    "        new_samples = block_samples[samples_filter]\n",
    "        \n",
    "        return samples_filter, new_samples\n",
    "    \n",
    "    def collate_output_values(blocks):\n",
    "        feature_out = []\n",
    "        target_out = []\n",
    "        for sample_output in blocks:\n",
    "            feature_block, target_block, structure = sample_output\n",
    "            for z in feature_block:\n",
    "                feature_out.append(torch.tensor(z.values))\n",
    "            target_out.append(torch.tensor(target_block.values))\n",
    "\n",
    "        return feature_out, target_out\n",
    "\n",
    "\n",
    "test_target_path = \"./test_fock.npz\"\n",
    "test_feature_path = \"./feature.npz\"\n",
    "test = HamiltonianDataset(test_feature_path, blocks, frames)\n",
    "\n",
    "# ## DataLoader\n",
    "\n",
    "def collate_blocks(block_tuple):\n",
    "    feature_tensor_map, target_block, structure_array = block_tuple[0]\n",
    "    \n",
    "    return feature_tensor_map, target_block, structure_array\n",
    "    \n",
    "\n",
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "Sampler = torch.utils.data.sampler.RandomSampler(test)\n",
    "BSampler = torch.utils.data.sampler.BatchSampler(Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "dataloader = DataLoader(test, sampler = BSampler, collate_fn = collate_blocks)\n",
    "\n",
    "\n",
    "# ## Model \n",
    "\n",
    "def get_block_samples(t_key, feature_map):\n",
    "    f_key = get_feat_keys_from_uncoupled(t_key, None , 2)\n",
    "    ss = feature_map.block(f_key[0]).samples.copy()\n",
    "    ss[\"structure\"] = ss[\"structure\"]+1\n",
    "    \n",
    "    return ss\n",
    "\n",
    "\n",
    "class HamModel(torch.nn.Module):\n",
    "    #Handles prediction of entire hamiltonian and derived results\n",
    "    def __init__(self, Hamiltonian_Dataset, device, regularization=None, seed=None, layer_size=None):\n",
    "        super().__init__()\n",
    "#         self.features = features \n",
    "#         self.target = target\n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        self.loss_history={}\n",
    "        self.device = device\n",
    "        self.target_keys = Hamiltonian_Dataset.target.keys\n",
    "        self.block_samples = {}\n",
    "        self.block_components = {}\n",
    "        for key in Hamiltonian_Dataset.target.keys:\n",
    "#             _block_type, _a_i, _n_i, _l_i, _a_j, _n_j, _l_j = key\n",
    "#             target_keys = Hamiltonian_Dataset.target.keys[Hamiltonian_Dataset.target.blocks_matching(\n",
    "#                 block_type = _block_type, a_i = _a_i, n_i = _n_i, l_i = _l_i, a_j = _a_j,\n",
    "#                 n_j = _n_j, l_j = _l_j)]\n",
    "            \n",
    "            #self.block_samples[str(key)] = Hamiltonian_Dataset.target.block(key).samples\n",
    "            self.block_components[str(key)] = Hamiltonian_Dataset.target.block(key).components\n",
    "        \n",
    "    \n",
    "            n_inputs = []\n",
    "            model_keys = []\n",
    "\n",
    "            feature_keys = Hamiltonian_Dataset.get_feature_keys(key)\n",
    "            for f_key in feature_keys: \n",
    "                n_features = Hamiltonian_Dataset.get_n_properties(Hamiltonian_Dataset.features, f_key)\n",
    "                n_inputs.append(n_features)\n",
    "                model_keys.append(f_key)\n",
    "                \n",
    "                \n",
    "            n_outputs = np.ones_like(n_inputs)\n",
    "                \n",
    "            self.models[str(key)] = BlockModel(cg.decouple,n_inputs, n_outputs, device, model_keys, key, seed = seed, hidden_layers = layer_size)\n",
    "        self.to(device)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #Ham model uses target keys\n",
    "        pred_blocks = []\n",
    "        for t_key in self.target_keys:\n",
    "            \n",
    "            pred = self.models[str(t_key)].forward(x) #feature_tensormap must correspond to the correct features, model returns block\n",
    "            \n",
    "            #try:\n",
    "#             print (pred.shape)\n",
    "#             print ((2 * t_key['l_i'])+1)\n",
    "#             print ((2 * t_key['l_j']) + 1)\n",
    "            pred_block = TensorBlock(\n",
    "                    values=pred.reshape((-1, (2 * t_key['l_i'])+1, (2 * t_key['l_j']) + 1, 1)), #?\n",
    "                    samples = get_block_samples(t_key, x),\n",
    "                    components = self.block_components[str(t_key)] ,\n",
    "                    properties= Labels([\"dummy\"], np.asarray([[0]], dtype=np.int32))\n",
    "                )\n",
    "#             except:\n",
    "#                 print (t_key)\n",
    "#                 print (pred)\n",
    "#                 print (self.block_samples[str(t_key)])\n",
    "#                 print (self.block_components[str(t_key)])\n",
    "                \n",
    "            pred_blocks.append(pred_block)\n",
    "        pred_hamiltonian = TensorMap(self.target_keys, pred_blocks)\n",
    "        return(pred_hamiltonian)\n",
    "    \n",
    "    #write/fix forward function for train_indiv\n",
    "    \n",
    "    def train_individual(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #Iterates through the keys of self.model, then for each key we will fit self.model[key] with data[key]\n",
    "        total = len(self.models)\n",
    "        for index, t_key in enumerate(self.target_keys):\n",
    "            print (\"Now training on Block {} of {}\".format(index, total))\n",
    "            train_dataloader.dataset.currentkey = t_key\n",
    "            \n",
    "            loss_history_key = self.models[str(t_key)].fit(train_dataloader, loss_function, optimizer_type, lr, regularization_dict[str(t_key)], n_epochs)\n",
    "\n",
    "            self.loss_history[str(t_key)] = loss_history_key\n",
    "    \n",
    "    def train_collective(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #for every loop through target keys, we predict the corresponding block and assemble the final hamiltonian\n",
    "        optimizer_dict = {}\n",
    "        if optimizer_type == \"Adam\":\n",
    "            for key in train_dataloader.dataset.target.keys:\n",
    "                optimizer_dict[str(key)] = torch.optim.Adam(self.models[str(key)].parameters(), lr = lr, weight_decay = regularization_dict[str(key)])\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 200\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "#             for key in train_dataloader.dataset.target.keys:\n",
    "#                 optimizer_dict[str(key)] = torch.optim.LBFGS(self.models[str(key)].parameters(), lr = lr)\n",
    "            optimizer_dict[0] = torch.optim.LBFGS(self.models.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 30\n",
    "            tol = 0\n",
    "            history_step = 10                \n",
    "    \n",
    "        scheduler_dict = {}\n",
    "        scheduler_dict[0] = torch.optim.lr_scheduler.StepLR(optimizer_dict[0], scheduler_threshold, gamma = 0.5)\n",
    "#         for key in train_dataloader.dataset.target.keys:\n",
    "#             scheduler_dict[str(key)] = torch.optim.lr_scheduler.StepLR(optimizer_dict[str(key)], scheduler_threshold, gamma = 0.5)\n",
    "\n",
    "        reg_weights = torch.tensor(list(regularization_dict.values()))\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            train_dataloader.dataset.currentkey = None\n",
    "            \n",
    "            for x_data, y_data, structure in train_dataloader: \n",
    "                self.collective_zg(optimizer_dict)\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.collective_zg(optimizer_dict)\n",
    "                        _pred = self.forward(x_data)\n",
    "                        _pred_loss = loss_function(_pred, y_data, structure, orbs)       \n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                          \n",
    "                        _reg_loss = self.get_regression_values(reg_weights) #Only works for 1 layer #Need to change!!\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    for value in optimizer_dict.values():\n",
    "                        value.step(closure)\n",
    "#                     for param in self.parameters():\n",
    "#                         print (param.grad)\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data, structure, orbs)  \n",
    "#                     reg_loss = torch.sum(torch.pow(self.nn.weight,2))#Only works for 1 layer\n",
    "                    new_loss = pred_loss \n",
    "                    new_loss.backward()\n",
    "                    self.collective_step(optimizer_dict)\n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in train_dataloader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data, structure, orbs)   #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg_weights)#Only works for 1 layer\n",
    "                new_loss = pred_loss + reg_loss\n",
    "\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training             \n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    self.collective_step(scheduler_dict)\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "        \n",
    "    def collective_step(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.step()\n",
    "            \n",
    "    def collective_zg(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.zero_grad()\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class BlockModel(torch.nn.Module): #Currently only 1 model per block\n",
    "    def __init__(self, reconstruction_function, inputSize, outputSize, device, keys, target_key, seed = None, hidden_layers = None):\n",
    "        super().__init__()\n",
    "        self.reconstruction_function = reconstruction_function\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.device = device\n",
    "        self.keys = keys\n",
    "        self.target_key = target_key\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.initialize_model(seed)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        for index, key in enumerate(self.keys):\n",
    "            self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = False)\n",
    "        \n",
    "    def forward(self, feature_tensormap):\n",
    "        #Block model uses feature keys\n",
    "        pred_values = {}\n",
    "        for key in self.keys:\n",
    "            feature_values = feature_tensormap.block(key).values\n",
    "            d1, d2, d3 = feature_values.shape\n",
    "            L = int((d2 -1)/2)\n",
    "            pred = self.models[str(key)](torch.tensor(feature_values.reshape(d1 * d2, d3)))\n",
    "            pred = pred.reshape(d1,d2)\n",
    "            pred_values[L] = pred\n",
    "        \n",
    "        pred_block_values = self.reconstruction_function({(self.target_key['l_i'],self.target_key['l_j']) : pred_values})\n",
    "        \n",
    "\n",
    "        #pred = torch.hstack(pred_values)\n",
    "        #pred = self.reconstruction_function(pred_values)\n",
    "        return pred_block_values \n",
    "\n",
    "    \n",
    "    def fit(self,traindata_loader, loss_function, optimizer_type, lr, reg, n_epochs):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 50\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "            optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 10\n",
    "            tol = 0\n",
    "            history_step = 10\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_threshold, gamma = 0.5)\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            \n",
    "            for x_data, y_data, structure in traindata_loader: \n",
    "                optimizer.zero_grad()\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        _pred = self.forward(x_data)                                        \n",
    "                        _pred_loss = loss_function(_pred, y_data.values)\n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                 \n",
    "                        _reg_loss = self.get_regression_values(reg.item()) #Only works for 1 layer\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    optimizer.step(closure)\n",
    "\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data.values)\n",
    "                    #reg_loss = self.get_regression_values(reg.item())#Only works for 1 layer\n",
    "                    new_loss = pred_loss #+ reg_loss\n",
    "                    new_loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in traindata_loader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data.values) #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg.item()) \n",
    "                new_loss = pred_loss + reg_loss\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    scheduler.step()\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output\n",
    "\n",
    "\n",
    "# ## Training\n",
    "\n",
    "\n",
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1])\n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return MSE\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    fock = torch.tensor(focks)\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)/len(fock[i])\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2\n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2\n",
    "\n",
    "\n",
    "testham_msefull = HamModel(test, \"cpu\")\n",
    "regularization_dict = {}\n",
    "# for a in range (16):\n",
    "#     regularization_dict[a] = torch.tensor(0)\n",
    "# for value in testham.models.values():\n",
    "#     for key in value.models:\n",
    "\n",
    "#         regularization_dict[str(key)] = torch.tensor(0)\n",
    "# print (len(regularization_dict))\n",
    "\n",
    "for key in blocks.keys:\n",
    "    regularization_dict[str(key)] = torch.tensor(0)\n",
    "#testham.train_individual(dataloader, regularization_dict, \"LBFGS\", 1000, mse_block_values, 1)\n",
    "testham_msefull.train_collective(dataloader, regularization_dict, \"LBFGS\", 20, mse_full, 1)\n",
    "\n",
    "\n",
    "testham_eigval = HamModel(test, \"cpu\")\n",
    "regularization_dict = {}\n",
    "# for a in range (16):\n",
    "#     regularization_dict[a] = torch.tensor(0)\n",
    "# for value in testham.models.values():\n",
    "#     for key in value.models:\n",
    "#         regularization_dict[str(key)] = torch.tensor(0)\n",
    "# print (len(regularization_dict))\n",
    "\n",
    "for key in blocks.keys:\n",
    "    regularization_dict[str(key)] = torch.tensor(0)\n",
    "#testham.train_individual(dataloader, regularization_dict, \"LBFGS\", 1000, mse_block_values, 1)\n",
    "testham_eigval.train_collective(dataloader, regularization_dict, \"LBFGS\", 20, mse_eigvals, 1)\n",
    "\n",
    "regularization_dict = {}\n",
    "for key in blocks.keys:\n",
    "    regularization_dict[str(key)] = torch.tensor(0)\n",
    "testham_indiv = HamModel(test, \"cpu\")\n",
    "testham_indiv.train_individual(dataloader, regularization_dict, \"LBFGS\", 50, mse_block_values, 1)\n",
    "\n",
    "# ## Evaluation\n",
    "\n",
    "#Load test set\n",
    "test_frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\"50:80\")\n",
    "# frames2 = ase.io.read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "test_frames = test_frames1 #+ frames2\n",
    "for f in test_frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "\n",
    "test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:80]\n",
    "test_focks = test_focks1\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "#test_focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[50:80]\n",
    "#test_overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[50:80]\n",
    "\n",
    "#test_orthogonal = []\n",
    "#for i in range(len(test_focks)): \n",
    "#    test_focks[i] = fix_pyscf_l1(test_focks[i],test_frames[i], orbs)\n",
    "#    test_overlap[i] = fix_pyscf_l1(test_overlap[i],test_frames[i], orbs)\n",
    "#    test_orthogonal.append(lowdin_orthogonalize(test_focks[i], test_overlap[i]))\n",
    "#test_focks = np.asarray(test_orthogonal, dtype=np.float64)\n",
    "    \n",
    "test_blocks = dense_to_blocks(test_focks, test_frames, orbs)\n",
    "test_fock_bc = couple_blocks(test_blocks, cg)\n",
    "\n",
    "\n",
    "# In[146]:\n",
    "\n",
    "\n",
    "test_rhoi = spex.compute(test_frames)\n",
    "test_gij = pairs.compute(test_frames)\n",
    "test_rho1i = acdc_standardize_keys(test_rhoi)\n",
    "test_rho1i.keys_to_properties(['species_neighbor'])\n",
    "test_gij =  acdc_standardize_keys(test_gij)\n",
    "test_rho2i = cg_increment(test_rho1i, test_rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "test_rho1ij = cg_increment(test_rho1i, test_gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "test_features = hamiltonian_features(test_rho2i, test_rho1ij)\n",
    "\n",
    "from equistore.io import save\n",
    "save(\"test_feature.npz\", test_features)\n",
    "\n",
    "\n",
    "# In[148]:\n",
    "\n",
    "\n",
    "#norm_test_feat = normalize_feats(test_features)\n",
    "#test_target_path = \"./test_fock.npz\"\n",
    "test_feature_path = \"./test_feature.npz\"\n",
    "testing = HamiltonianDataset(test_feature_path, test_blocks, test_frames)\n",
    "\n",
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "test_Sampler = torch.utils.data.sampler.RandomSampler(testing)\n",
    "test_BSampler = torch.utils.data.sampler.BatchSampler(test_Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "test_dataloader = DataLoader(testing, sampler = test_BSampler, collate_fn = collate_blocks)\n",
    "\n",
    "\n",
    "# In[152]:\n",
    "\n",
    "\n",
    "\n",
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1])\n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return MSE\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    #fock = torch.tensor(focks)\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)/len(fock[i])\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2\n",
    "\n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    #fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2\n",
    "\n",
    "\n",
    "# In[162]:\n",
    "\n",
    "\n",
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in dataloader:\n",
    "    t_pred = testham_msefull(x_data)\n",
    "    print (\"Train error for mse_full is {}\".format(mse_full(t_pred, torch.tensor(focks), structures, orbs)))\n",
    "\n",
    "\n",
    "test_dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in test_dataloader:\n",
    "    pred = testham_msefull(x_data)\n",
    "    print (\"Test error for mse_full is {}\".format(mse_full(pred, torch.tensor(test_focks), structures, orbs)))\n",
    "\n",
    "\n",
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in dataloader:\n",
    "    t_pred = testham_eigval(x_data)\n",
    "    print (\"Train error for mse_eigval is {}\".format(mse_eigvals(t_pred, torch.tensor(focks), structures, orbs)))\n",
    "\n",
    "\n",
    "test_dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in test_dataloader:\n",
    "    pred = testham_eigval(x_data)\n",
    "    print (\"Test error for mse_full is {}\".format(mse_eigvals(pred, torch.tensor(test_focks), structures, orbs)))\n",
    "\n",
    "\n",
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in dataloader:\n",
    "    t_pred = testham_indiv(x_data)\n",
    "    print (\"Train error for mse_indiv is {}\".format(mse_full(t_pred, torch.tensor(focks), structures, orbs)))\n",
    "\n",
    "\n",
    "test_dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in test_dataloader:\n",
    "    pred = testham_indiv(x_data)\n",
    "    print (\"Test error for mse_indiv is {}\".format(mse_full(pred, torch.tensor(test_focks), structures, orbs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134112b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
